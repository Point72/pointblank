Series.qcut(quantiles: Sequence[float] | int, *, labels: Sequence[str] | None = None, left_closed: bool = False, allow_duplicates: bool = False, include_breaks: bool = False) -> Series: Bin continuous values into discrete categories based on quantiles. quantiles: Quantile probabilities or number of bins. labels: Category names. left_closed: Intervals left-closed. allow_duplicates: Drop duplicates in quantiles. include_breaks: Include breakpoint column; output type changes to Struct. Returns: Categorical Series or Struct Series if include_breaks is True. Expr.rolling_sum(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr: Apply a rolling sum over the values. Parameters: window_size (length of the window), weights (multiply elementwise with the window), min_samples (min non-null values to compute result, defaults to window_size), center (labels at the center). Example: df.with_columns(pl.col("A").rolling_sum(window_size=2)). DataFrame.rename(mapping: dict[str, str] | Callable[[str], str], *, strict: bool = True) -> DataFrame: Rename column names. `strict` validates that all column names exist. Expr.dt.truncate(every: str | dt.timedelta | Expr) -> Expr: Divides the date/datetime range into buckets, mapping each value to the start of its bucket. Weekly buckets start on Monday. Parameters: every: Interval start and period length (e.g., "1h", timedelta(hours=1)). Returns: Expr of type Date or Datetime. Note: every argument uses a string language (1ns, 1us, 1ms, 1s, 1m, 1h, 1d, 1w, 1mo, 1q, 1y). These strings can be combined (e.g., 3d12h4m25s). DataFrame.fold(operation: Callable[[Series, Series], Series]) -> Series. Apply a horizontal reduction on a DataFrame. Series.cut(breaks: Sequence[float], *, labels: Sequence[str] | None = None, left_closed: bool = False, include_breaks: bool = False) -> Series. Bins values into categories. Expr.str.slice(offset: int | IntoExprColumn, length: int | IntoExprColumn | None = None) -> Expr: Extract substring. Offset and length in characters (UTF8). Series.str.extract_all(pattern: str | Series) -> Series: Extracts all regex matches as list. Returns Series of List(String). Null input returns Null. polars.Expr.bottom_k(k: int | IntoExprColumn = 5) -> Expr. Return the k smallest elements. Prefers non-null. O(n) complexity. Not guaranteed to be sorted. k is number of elements. See also: top_k, top_k_by, bottom_k_by. polars.concat_str(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr, separator: str = '', ignore_nulls: bool = False) -> Expr: Concatenate columns to string. Params: exprs, more_exprs, separator, ignore_nulls. Returns: Expr. Note: Linear time. Series.str.head(n: int | IntoExprColumn) -> Series: Returns first n characters of each string. n: Length of slice. Returns: Series of type String. If n is negative, returns chars up to nth from end. If string length < n, returns full string. Example: s.str.head(5) Series.list.sample(n: int, fraction: float, with_replacement: bool, shuffle: bool, seed: int) -> Series Sample from list. Use n or fraction. polars.duration(*, weeks: Expr | str | int | None = None, days: Expr | str | int | None = None, hours: Expr | str | int | None = None, minutes: Expr | str | int | None = None, seconds: Expr | str | int | None = None, milliseconds: Expr | str | int | None = None, microseconds: Expr | str | int | None = None, nanoseconds: Expr | str | int | None = None, time_unit: TimeUnit | None = None) -> Expr: Create Duration from time components. Returns: Expr of data type Duration. Use Expr.dt.offset_by() for non-fixed durations. Example: pl.duration(days=1) DataFrame.select(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> DataFrame: Select columns from DataFrame. Parameters: *exprs: Column(s) to select (str or Expr). **named_exprs: Additional columns, renamed to keyword. Example: df.select("foo") Expr.rolling_min(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr. Apply rolling min. Params: window_size, weights, min_samples, center. Returns: Expr. Note: use rolling for multiple aggregations over same window. Series.equals(other: Series, check_dtypes: bool, check_names: bool, null_equal: bool) -> bool Check if Series equals another Series. Series.str.replace_all(pattern: str, value: str, *, literal: bool = False) -> Series: Replace all matching regex/literal substrings. `pattern`: regex pattern. `value`: replacement string. `literal`: treat pattern as literal. Notes: use inline flags like `(?i)` for case-insensitivity. Use $$ for literal $, or set literal=True. Supports capture groups ($1, ${1}, named groups). Example: s.str.replace_all(r"h(.)t", "b${1}d") LazyFrame.pipe(function: Callable, *args, **kwargs) -> T: Apply sequence of UDFs. function: UDF to apply. *args, **kwargs: Arguments to UDF. LazyFrame.with_columns(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> LazyFrame: Add columns to this LazyFrame, replacing existing columns with the same name. *exprs: Column(s) to add (expressions or column names). **named_exprs: Additional columns (renamed to the keyword). Returns new LazyFrame. Example: lf.with_columns((pl.col("a") ** 2).alias("a^2")) LazyFrame.group_by_dynamic(index_column: IntoExpr, *, every: str | timedelta, period: str | timedelta | None = None, offset: str | timedelta | None = None, include_boundaries: bool = False, closed: ClosedInterval = 'left', label: Label = 'left', group_by: IntoExpr | Iterable[IntoExpr] | None = None, start_by: StartBy = 'window') -> LazyGroupBy: Groups based on a time or index value. index_column must be sorted. Parameters: index_column: Column to group by. Must be Date/Datetime or Int32/Int64. every: Interval of the window. period: Length of the window (defaults to 'every'). offset: Offset of the window (defaults to zero). include_boundaries: Add lower/upper bound columns (impacts performance). closed: {'left', 'right', 'both', 'none'} Define which sides of the temporal interval are closed (inclusive). label: {'left', 'right', 'datapoint'} Label for the window. group_by: Also group by this column/these columns. start_by: {'window', 'datapoint', 'monday', ..., 'sunday'} Strategy to determine the start of the first window. Returns: LazyGroupBy object. Result will be sorted by index_column within each group. Note: index_column must be sorted in ascending order. If group_by is passed, then the index column must be sorted in ascending order within each group. polars.from_dict(data: Mapping[str, Sequence[object] | Mapping[str, Sequence[object]] | Series], schema: SchemaDefinition | None = None, *, schema_overrides: SchemaDict | None = None, strict: bool = True) -> DataFrame: Construct DataFrame from dict of sequences. Parameters: data (dict of sequences), schema (list of names, (name, type), or {name:type}; auto-inferred if None), schema_overrides ({str:DataType}), strict (bool, default True, throw error if type mismatch, else cast or set null). Returns: DataFrame. Example: pl.from_dict({"a": [1, 2], "b": [3, 4]}) polars.read_database_uri(query: list[str] | str, uri: str, partition_on: str | None = None, partition_range: tuple[int, int] | None = None, partition_num: int | None = None, protocol: str | None = None, engine: DbReadEngine | None = None, schema_overrides: SchemaDict | None = None, execute_options: dict[str, Any] | None = None) -> DataFrame: Read SQL query results into a DataFrame from a URI. Parameters: query (SQL query), uri (connection URI), partition_on, partition_range, partition_num, protocol, engine ('connectorx', 'adbc'), schema_overrides, execute_options. Requires connectorx>=0.3.2 or pyarrow and ADBC driver. Expr.rolling_mean(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr[source]. Apply rolling mean. Parameters: window_size, weights, min_samples, center. Example: df.with_columns(rolling_mean=pl.col("A").rolling_mean(window_size=2)). Expr.is_between(lower_bound: IntoExpr, upper_bound: IntoExpr, closed: ClosedInterval = 'both') -> Expr Check if this expression is between the given lower and upper bounds. If lower_bound > upper_bound, returns False. LazyFrame.null_count() -> LazyFrame: Aggregate columns as sum of null value counts. Example: lf.null_count().collect() DataFrame.cast(dtypes: Mapping[ColumnNameOrSelector|PolarsDataType, PolarsDataType|PythonDataType]|PolarsDataType, *, strict:bool=True) -> DataFrame: Cast DataFrame column(s). dtypes: column names/selector to dtypes or single dtype. strict: raise if invalid cast. Expr.str.strptime(dtype: PolarsTemporalType, format: str | None = None, *, strict: bool = True, exact: bool = True, cache: bool = True, ambiguous: Ambiguous | Expr = 'raise') -> Expr: Convert string column to Date/Datetime/Time. Parameters: dtype (Date, Datetime, Time), format (strftime, inferred if None), strict (error if fails), exact (require exact format), cache, ambiguous ('raise', 'earliest', 'latest', 'null'). Datetime infers unit from format, defaults to "us". Example: s.str.strptime(pl.Datetime, "%Y-%m-%d %H:%M%#z") Expr.str.zfill(length: int | IntoExprColumn) -> Expr. Pad start of string with zeros to given length. Parameters: length: Pad length. Notes: Intended for numeric strings. Use pad_start for non-ASCII. Expr.arg_sort(*, descending: bool = False, nulls_last: bool = False) -> Expr. Get indices to sort column. descending: Sort descending. nulls_last: Place nulls last. DataFrame.pivot(on, *, index=None, values=None, aggregate_function=None, maintain_order=True, sort_columns=False, separator='_') -> DataFrame: Create pivot table. Params: on (pivot columns), index (index columns), values (value columns), aggregate_function (aggregation function), maintain_order, sort_columns, separator. Eager mode only. Expr.pct_change(n: int | IntoExprColumn = 1) -> Expr: Computes percentage change between values. n: periods to shift. Expr.dt.offset_by(by:str|Expr) -> Expr. Offset date by relative time offset (months, leap years considered). Parameters: by (offset str '1ns', '1d', '1mo', '1y' etc or Expr). Returns: Expr (Date or Datetime). polars.when(*predicates: IntoExprColumn | Iterable[IntoExprColumn] | bool, **constraints: Any) -> When Starts a when-then-otherwise expression (like if-else). Chain with .then(). and optionally .when().then(). .otherwise(). for a final value, else null. All expressions computed in parallel. Predicates combined with &. Expr.struct.field(name: str | list[str], *more_names: str) -> Expr: Retrieves one or multiple Struct field(s). name: Field name. more_names: Additional field names. polars.datetime_ranges(start: datetime | date | IntoExprColumn, end: datetime | date | IntoExprColumn, interval: str | timedelta = '1d', *, closed: ClosedInterval = 'both', time_unit: TimeUnit | None = None, time_zone: str | None = None, eager: bool = False) -> Series | Expr: Create a column of datetime ranges. Returns: Expr or Series of type List(Datetime). Expr.add(other: Any) -> Expr. Add 'other' to the expression. other: numeric or string value, or expression. Example: df.with_columns(pl.col("x").add(2).alias("x+int")) Series.bin.reinterpret(*, dtype: PolarsDataType, endianness: Endianness = 'little') -> Series Interpret a buffer as a numerical polars type. dtype: PolarsDataType. polars.api.register_lazyframe_namespace(name: str) -> Callable[[type[NS]], type[NS]]. Decorator for registering custom functionality with a Polars LazyFrame. Expr.str.split(by: IntoExpr, *, inclusive: bool=False) -> Expr. Split string by substring. Params: by, inclusive. Returns: Expr. If inclusive=True, include separator in results. Series.dt.century() -> Series: Extract century from Date/Datetime columns. Returns: Series of Int32. Example: s.dt.century() Series.str.split_exact(by: IntoExpr, n: int, *, inclusive: bool = False) -> Series: Split string by substring using n splits, result in struct of n+1 fields. If cannot make n splits, remaining fields will be null. Parameters: by (substring to split by), n (number of splits), inclusive (include split character in results). Returns: Series of data type Struct. Example: df["x"].str.split_exact("_", 1).alias("fields"). Expr.replace_strict(old: IntoExpr | Sequence[Any] | Mapping[Any, Any], new: IntoExpr | Sequence[Any], *, default: IntoExpr | NoDefault = , return_dtype: PolarsDataType | None = None) -> Expr. Replaces values. Parameters: old, new, default (error if no default and value not replaced), return_dtype. Raises: InvalidOperationError. Notes: String cache for categorical values. Series.fill_null(value=None, strategy=None, limit=None) -> Series Fill null values. Strategies: forward, backward, min, max, mean, zero, one. s = pl.Series("a", [1, 2, 3, None]); s.fill_null(strategy="forward") polars.read_database(query: str | TextClause | Selectable, connection: ConnectionOrCursor | str, *, iter_batches: bool = False, batch_size: int | None = None, schema_overrides: SchemaDict | None = None, infer_schema_length: int | None = 100, execute_options: dict[str, Any] | None = None) -> DataFrame | Iterator[DataFrame]. Read SQL query into DataFrame. Params: query, connection, iter_batches, batch_size, schema_overrides, infer_schema_length, execute_options. polars.scan_delta(source: str | DeltaTable, *, version: int | str | datetime | None = None, storage_options: dict[str, Any] | None = None, ...) Returns LazyFrame. Lazily read Delta lake table. Parameters: source: path/DeltaTable, version, storage_options, etc. Example: pl.scan_delta(table_path).collect(). Series.list.get(index: int | Series | list[int], null_on_oob: bool = False) -> Series: Get value by index in sublists. null_on_oob=True returns null for out-of-bounds index, False raises error. LazyFrame.with_row_count(name: str = 'row_nr', offset: int = 0) -> LazyFrame. Add row count column at index 0. Deprecated: Use with_row_index(). Warning: Negative effect on query performance. Expr.str.join(delimiter: str = '', *, ignore_nulls: bool = True) -> Expr. Concatenates string values to single string. delimiter: string to insert. ignore_nulls: ignore null values (default), if False output is null if any nulls. Series.str.to_time(format: str | None = None, *, strict: bool = True, cache: bool = True) -> Series: Convert a String column into a Time column. Parameters: format (strptime format), strict (raise error on failure), cache (use conversion cache). Example: s.str.to_time("%H:%M") DataFrame.write_excel(workbook: str | Workbook | IO[bytes] | Path | None = None, worksheet: str | Worksheet | None = None, *, position: tuple[int, int] | str = 'A1', table_style: str | dict[str, Any] | None = None, table_name: str | None = None, column_formats: ColumnFormatDict | None = None, dtype_formats: dict[OneOrMoreDataTypes, str] | None = None, conditional_formats: ConditionalFormatDict | None = None, header_format: dict[str, Any] | None = None, column_totals: ColumnTotalsDefinition | None = None, column_widths: ColumnWidthsDefinition | None = None, row_totals: RowTotalsDefinition | None = None, row_heights: dict[int | tuple[int, ...], int] | int | None = None, sparklines: dict[str, Sequence[str] | dict[str, Any]] | None = None, formulas: dict[str, str | dict[str, str]] | None = None, float_precision: int = 3, include_header: bool = True, autofilter: bool = True, autofit: bool = False, hidden_columns: Sequence[str] | SelectorType | None = None, hide_gridlines: bool = False, sheet_zoom: int | None = None, freeze_panes: str | tuple[int, int] | tuple[str, int, int] | tuple[int, int, int, int] | None = None) -> Workbook Write DataFrame to Excel. Parameters: workbook, worksheet: File/sheet names. position: Table position. table_style, table_name, column_formats, dtype_formats, conditional_formats, header_format, column_totals, column_widths, row_totals, row_heights, sparklines, formulas, float_precision, include_header, autofilter, autofit, hidden_columns, hide_gridlines, sheet_zoom, freeze_panes: Various formatting options. Expr.bin.contains(literal: IntoExpr) -> Expr: Check if binaries contain substring. Returns boolean expression. DataFrame.write_database(table_name:str, connection:ConnectionOrCursor|str, *, if_table_exists:DbWriteMode='fail', engine:DbWriteEngine|None=None, engine_options:dict[str,Any]|None=None) -> int. Write DataFrame to database. Parameters: table_name:Table name, connection:DB connection URI, if_table_exists:{'append', 'replace', 'fail'}, engine:{'sqlalchemy', 'adbc'}, engine_options:Engine options. Returns: int (rows affected or -1). Expr.arr.to_struct(fields: Sequence[str] | Callable[[int], str] | None = None) -> Expr: Convert Array to Struct Series. `fields`: list of field names, a function to generate names, or None (defaults to field_0, field_1...). Example: df.with_columns(struct=pl.col("n").arr.to_struct(fields=lambda idx: f"n{idx}")) polars.linear_space(start: NumericLiteral | TemporalLiteral | IntoExpr, end: NumericLiteral | TemporalLiteral | IntoExpr, num_samples: int | IntoExpr, *, closed: ClosedInterval = 'both', eager: bool = False) -> Expr | Series Create sequence of evenly-spaced points. Returns Expr or Series. DataFrame.to_jax(return_type='array', device=None, label=None, features=None, dtype=None, order='fortran') -> jax.Array | dict[str, jax.Array]. Convert DataFrame to a Jax Array or dict of Jax Arrays. return_type: 'array' or 'dict'. device: jax.Device or str. label, features: str or Expr or Sequence. dtype: PolarsDataType. order: 'c' or 'fortran'. polars.from_epoch(column: str | Expr | Series | Sequence[int], time_unit: EpochTimeUnit = 's') -> Expr | Series. Parse epoch timestamp to Polars Datetime/Date. Parameters: column: Series/Expr, time_unit: {'d', 's', 'ms', 'us', 'ns'}. Returns: pl.Date or pl.Datetime. DataFrame.explode(columns: str | Expr | Sequence[str | Expr], *more_columns: str | Expr) -> DataFrame: Explodes list/array columns to long format. Series.filter(predicate: Series | Iterable[bool]) -> Self. Filter elements by boolean mask. Series.str.count_matches(pattern: str | Series, *, literal: bool = False) -> Series. Count regex matches. pattern: regex or literal string. Returns UInt32 Series, null if original null. DataFrame.top_k(k: int, *, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool] = False) -> DataFrame: Return k largest rows based on 'by' columns. Non-null elements preferred. Result not guaranteed to be sorted. See also: bottom_k. Example: df.top_k(4, by="b") Expr.list.gather_every(n: int | IntoExprColumn, offset: int | IntoExprColumn = 0) -> Expr. Take every n-th value from offset in sublists. DataFrame.write_json(file: IOBase | str | Path | None = None) -> str | None: Serialize to JSON. file: File path or file-like object. If None, returns string. df = pl.DataFrame({"foo": [1, 2, 3],"bar": [6, 7, 8]}); df.write_json() polars.last(*columns: str) -> Expr: Get last column or value. Returns last column if no columns given, else last value of columns. Series.dt.epoch(time_unit: EpochTimeUnit = 'us') -> Series: Get time since Unix epoch. time_unit: {'us', 'ns', 'ms', 's', 'd'} LazyFrame.select(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> LazyFrame: Selects columns. exprs: Columns to select (positional). named_exprs: Columns to select (keyword), renamed to keyword. DataFrame.to_numpy(order='fortran', writable=False, allow_copy=True, structured=False, use_pyarrow=None) -> np.ndarray: Converts DataFrame to NumPy ndarray. order: 'C' or 'fortran'. writable: force data copy to make array writable. allow_copy: fail if zero-copy not possible. structured: return structured array. df = pl.DataFrame({"a": [1, 2, 3]}); df.to_numpy() Series.reshape(dimensions: tuple[int, ...]) -> Series: Reshape this Series to a flat Series or an Array Series. Parameters: dimensions (tuple of dimension sizes, -1 infers). Returns: Series (original type if single dimension, Array if multiple). See also: Series.list.explode. s = pl.Series("foo", [1, 2, 3, 4, 5, 6, 7, 8, 9]); s.reshape((3, 3)) Series.struct.rename_fields(names: Sequence[str]) -> Series: Rename the fields of the struct. names: New names in order of struct's fields. Example: s.struct.rename_fields(["c", "d"]) Series.dt.to_string(format: str | None = None) -> Series. Convert Date/Time/Datetime to String with format. Default ISO. Datetime: ISO, ISO:strict. Duration: ISO, polars. Expr.fill_null(value: Any | Expr | None = None, strategy: FillNullStrategy | None = None, limit: int | None = None) -> Expr: Fills null values. Strategies: forward, backward, min, max, mean, zero, one. polars.concat(items: Iterable[PolarsType], *, how: ConcatMethod = 'vertical', rechunk: bool = False, parallel: bool = True) -> PolarsType: Concatenate DataFrames, LazyFrames, or Series. items: Items to concatenate. how: {'vertical', 'vertical_relaxed', 'diagonal', 'diagonal_relaxed', 'horizontal', 'align'}. rechunk: Rechunk result. parallel: For LazyFrames, execute in parallel. Example: pl.concat([df1, df2]) Expr.sort_by(by: IntoExpr | Iterable[IntoExpr], *more_by: IntoExpr, descending: bool | Sequence[bool] = False, nulls_last: bool | Sequence[bool] = False, multithreaded: bool = True, maintain_order: bool = False) -> Expr: Sort column by other columns' order. Projection/selection: whole column sorted. Group by: groups sorted. DataFrame.join(other: DataFrame, on: str | Expr | Sequence[str | Expr] | None = None, how: JoinStrategy = 'inner', *, left_on: str | Expr | Sequence[str | Expr] | None = None, right_on: str | Expr | Sequence[str | Expr] | None = None, suffix: str = '_right', validate: JoinValidation = 'm:m', join_nulls: bool = False, coalesce: bool | None = None, maintain_order: MaintainOrderJoin | None = None) -> DataFrame: SQL-like join. `other`: DataFrame to join. `on`: join column(s). `how`: 'inner', 'left', 'right', 'full', 'semi', 'anti', 'cross'. `left_on`, `right_on`: alternative join columns. `suffix`: for duplicate names. `validate`: 'm:m', 'm:1', '1:m', '1:1' (checks uniqueness). `join_nulls`: join on nulls. `coalesce`: controls column merging (None, True, False). `maintain_order` {'none', 'left', 'right', 'left_right', 'right_left'} sets result order, defaults to none for speed. Example: df.join(other_df, on="ham", how="left") Expr.list.join(separator: IntoExprColumn, ignore_nulls: bool = True) -> Expr: Joins strings within sublists using a separator. Errors if inner type is not String. ignore_nulls: handles nulls. DataFrame.rows_by_key(key, named=False, include_key=False, nunique=False) -> dict: Group rows by key column(s) and return a dictionary. DataFrame.to_torch(return_type='tensor', label=None, features=None, dtype=None) -> torch.Tensor | dict[str, torch.Tensor] | PolarsDataset: Convert DataFrame to PyTorch Tensor, Dataset, or dict of Tensors. return_type: 'tensor', 'dataset', 'dict'. label: column(s) for labels. features: column(s) for features. dtype: unify dtype. df = pl.DataFrame({"lbl": [0, 1, 2, 3],"feat1": [1, 0, 0, 1],"feat2": [1.5, -0.5, 0.0, -2.25]}); df.to_torch(dtype=pl.Float32) DataFrame.n_unique(subset: str | Expr | Sequence[str | Expr] | None = None) -> int: Return number of unique rows or subsets. Params: subset. Returns: int. DataFrame.fill_null(value=None, strategy=None, limit=None, matches_supertype=True) -> DataFrame: Fill null values. Value: fill value. Strategy: 'forward', 'backward', 'min', 'max', 'mean', 'zero', 'one'. Limit: consecutive nulls to fill (forward/backward). Returns: DataFrame with nulls replaced. polars.select(*exprs: IntoExpr | Iterable[IntoExpr], eager: bool = True, **named_exprs: IntoExpr) -> DataFrame | LazyFrame: Run polars expressions without context. eager: DataFrame if True, LazyFrame if False. *exprs: Columns to select. **named_exprs: Rename columns. polars.lazyframe.group_by.LazyGroupBy.head(n: int = 5) -> LazyFrame Get the first n rows of each group. n: Number of rows to return. Series.ewm_mean_by(by: IntoExpr, half_life: str|timedelta) -> Series: Time-based exponentially weighted moving average. by must be DateTime, Date, or integer type. half_life supports timedelta or strings (e.g., "4d"). Expr.limit(n: int | Expr = 10) -> Expr: Get first n rows (alias for head()). polars.from_numpy(data: np.ndarray, schema: SchemaDefinition | None = None, *, schema_overrides: SchemaDict | None = None, orient: Orientation | None = None) -> DataFrame: Constructs DataFrame from NumPy ndarray (clones data). Parameters: data: NumPy ndarray. schema: column names or {name: type} dict. schema_overrides: dict of type overrides. orient: 'col' or 'row', None infers. Returns: DataFrame. polars.int_range(start: int|IntoExprColumn=0, end: int|IntoExprColumn|None=None, step: int=1, dtype: PolarsIntegerType=Int64, eager: bool=False) -> Expr|Series: Generate integer range. end omitted defaults to start, start defaults to 0. Expr.meta.is_literal(allow_aliasing: bool = False) -> bool: Checks if the expression is a literal value. allow_aliasing=True also allows aliased literals. Expr.over(partition_by: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr, order_by: IntoExpr | Iterable[IntoExpr] | None = None, mapping_strategy: WindowMappingStrategy = 'group_to_rows') -> Expr: Compute expressions over groups, similar to SQL window functions. Parameters: partition_by (columns to group by), *more_exprs (additional group by columns), order_by, mapping_strategy ('group_to_rows', 'join', 'explode'). 'group_to_rows' assigns values back to original positions, 'join' joins as List, and 'explode' creates new rows. Series.struct.unnest() -> DataFrame: Converts a struct Series to a DataFrame with columns for each field. Series.str.find(pattern: str | Expr, *, literal: bool = False, strict: bool = True) -> Series. Returns byte offset of first substring matching pattern, or None if not found. Expr.top_k_by(by: IntoExpr | Iterable[IntoExpr], k: int | IntoExprColumn = 5, reverse: bool | Sequence[bool] = False) -> Expr: Return elements of k largest in 'by' column(s). polars.read_csv_batched(source, *, has_header=True, columns=None, separator=',', schema_overrides=None, null_values=None, n_threads=None, batch_size=50000, n_rows=None, encoding='utf8', low_memory=False) Read CSV file in batches. Returns BatchedCsvReader. DataFrame.unique(subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None, *, keep: UniqueKeepStrategy = 'any', maintain_order: bool = False) -> DataFrame: Drop duplicate rows. subset: columns to consider. keep: 'first', 'last', 'any', 'none'. maintain_order: keep original order. Fails with List columns. Expr.str.to_integer(base: int | IntoExprColumn = 10, strict: bool = True) -> Expr: Converts String column to Int64. base: Radix. strict: Raise error on parse/overflow. False silently converts to Null. DataFrame.filter(*predicates: IntoExprColumn | Iterable[IntoExprColumn] | bool | list[bool] | np.ndarray[Any, Any], **constraints: Any) -> DataFrame[source]. Filter rows based on predicates. Parameters: predicates, constraints. Note: null comparisons filter out rows. Use ne_missing for pandas behavior. Example: df.filter(pl.col("foo") > 1). Expr.eq_missing(other: Any) -> Expr. Equality operator where None == None. Differs from default eq. Expr.cat.ends_with(suffix: str) -> Expr. Check if string representations of values end with a substring. polars.read_csv(source: str | Path | IO[str] | IO[bytes] | bytes, *, has_header: bool = True, columns: Sequence[int] | Sequence[str] | None = None, new_columns: Sequence[str] | None = None, separator: str = ',', comment_prefix: str | None = None, quote_char: str | None = '"', skip_rows: int = 0, skip_lines: int = 0, schema: SchemaDict | None = None, schema_overrides: Mapping[str, PolarsDataType] | Sequence[PolarsDataType] | None = None, null_values: str | Sequence[str] | dict[str, str] | None = None, missing_utf8_is_empty_string: bool = False, ignore_errors: bool = False, try_parse_dates: bool = False, n_threads: int | None = None, infer_schema: bool = True, infer_schema_length: int | None = 100, batch_size: int = 8192, n_rows: int | None = None, encoding: CsvEncoding | str = 'utf8', low_memory: bool = False, rechunk: bool = False, use_pyarrow: bool = False, storage_options: dict[str, Any] | None = None, skip_rows_after_header: int = 0, row_index_name: str | None = None, row_index_offset: int = 0, sample_size: int = 1024, eol_char: str = '\n', raise_if_empty: bool = True, truncate_ragged_lines: bool = False, decimal_comma: bool = False, glob: bool = True) -> DataFrame: Read a CSV file into a DataFrame. source: file path or file-like object. has_header: first row is header. columns: Columns to select (indices or names). new_columns: Rename columns after parsing. separator: Separator character. comment_prefix: Prefix for comment lines. quote_char: Quoting character. skip_rows: Skip rows. skip_lines: Skip lines. schema: Provide schema. schema_overrides: Overwrite dtypes during inference. null_values: Values to interpret as null. missing_utf8_is_empty_string: missing utf8 values are treated as empty string. ignore_errors: Try to keep reading lines if errors occur. try_parse_dates: Automatically parse dates. n_threads: Number of threads. infer_schema: Infer schema from data. infer_schema_length: Rows to scan for schema inference. batch_size: Lines to read into buffer at once. n_rows: Stop reading after n_rows. encoding: Encoding type. low_memory: Reduce memory pressure. rechunk: Ensure columns are contiguous in memory. use_pyarrow: Use pyarrow's CSV parser. storage_options: Extra options for fsspec.open(). skip_rows_after_header: Skip rows after header. row_index_name: Insert row index column. row_index_offset: Start row index at offset. eol_char: End of line character. raise_if_empty: Raise error if no data. truncate_ragged_lines: Truncate lines longer than schema. decimal_comma: Use comma as decimal separator. glob: expand path via globbing. Returns DataFrame. See also scan_csv. Example: pl.read_csv("data.csv", separator="|") polars.map_batches(exprs: Sequence[str] | Sequence[Expr], function: Callable[[Sequence[Series]], Series], return_dtype: PolarsDataType | None = None) -> Expr Map a custom function over multiple columns/expressions. exprs: Input Series. function: Function to apply. return_dtype: Output Series dtype. polars.datetime_range(start: datetime | date | IntoExprColumn, end: datetime | date | IntoExprColumn, interval: str | timedelta = '1d', closed: ClosedInterval = 'both', time_unit: TimeUnit | None = None, time_zone: str | None = None, eager: bool = False) -> Series | Expr: Generate datetime range. start: Lower bound. end: Upper bound. interval: timedelta or duration string (eg "1ns", "1d"). closed: {'both', 'left', 'right', 'none'}. time_unit: {'ns', 'us', 'ms'}. time_zone: Time zone. eager: Evaluate immediately. Returns: Expr or Series of Datetime. pl.datetime_range(datetime(2022, 1, 1), datetime(2022, 3, 1), "1mo", eager=True) DataFrame.replace_column(index: int, column: Series) -> DataFrame Replace column at index location. In place. Parameters: index (int), column (Series). Returns: DataFrame. In place operation. polars.LazyFrame.shift(n: int | IntoExprColumn = 1, *, fill_value: IntoExpr | None = None) -> LazyFrame Shift values by n indices. Positive n is similar to SQL LAG, negative to LEAD. Parameters: n, fill_value. polars.json_normalize(data: dict | Sequence[dict], *, separator: str = '.', max_level: int | None = None, schema: Schema | None = None, strict: bool = True, infer_schema_length: int | None = 100) -> DataFrame: Normalize semi-structured JSON data into a flat table. Args: data, separator, max_level, schema, strict, infer_schema_length. Unstable API. Example: pl.json_normalize(data, max_level=1) Series.list.len() -> Series: Length of each list. Nulls count. Expr.str.json_path_match(json_path) -> Expr: Extracts first JSONPath match. json_path: JSONPath query string. Returns String Expr; null if no match or null input. LazyFrame.filter(*predicates: IntoExprColumn | Iterable[IntoExprColumn] | bool | list[bool] | np.ndarray[Any, Any], **constraints: Any) -> LazyFrame. Filter rows based on predicate. predicates: boolean Series expression. constraints: column filters (name=value). Null comparisons yield null and filter out rows. polars.exclude(columns: str | PolarsDataType | Collection[str] | Collection[PolarsDataType], *more_columns: str | PolarsDataType) -> Expr Represent all columns except for the given columns. Accepts regular expression input. Expr.rolling_sum_by(by: IntoExpr, window_size: timedelta | str, *, min_samples: int = 1, closed: ClosedInterval = 'right') -> Expr. Rolling sum based on column 'by'. 'window_size' timedelta or str. 'closed' interval closure. Unstable. SQLContext.execute(query: str, *, eager: bool | None = None) -> LazyFrame | DataFrame: Parses and executes a SQL query against registered frame data. Parameters: query - SQL query string, eager - Apply query eagerly (DataFrame return). Returns: LazyFrame or DataFrame. Series.str.strip_chars_end(characters: IntoExpr = None) -> Series: Removes trailing characters. characters: characters to remove, None for whitespace. s = pl.Series([" hello ", "world\t"]); s.str.strip_chars_end() Expr.rolling_max_by(by: IntoExpr, window_size: timedelta | str, min_samples: int=1, closed: ClosedInterval='right') -> Expr Rolling max based on another column. window_size can be temporal. Unstable. Series.zip_with(mask: Series, other: Series) -> Series. Take values from self or other based on mask. True mask: self, False mask: other. Series.dt.second(*, fractional: bool = False) -> Series: Extract seconds from DateTime. Parameters: fractional (bool) - include fractional component. Returns: Series (Int8 or Float64). Example: s.dt.second(fractional=True) LazyFrame.explode(columns: str|Expr|Sequence[str|Expr], *more_columns: str|Expr) -> LazyFrame: Explode DataFrame to long format. columns: column names/expressions to explode. more_columns: additional columns to explode. Expr.struct.rename_fields(names: Sequence[str]) -> Expr: Rename fields of the struct. Parameters: names: New names (same order as struct's fields). Example: df.select(pl.col("struct_col").struct.rename_fields(["www", "xxx", "yyy", "zzz"])).unnest("struct_col") Series.list.gather_every(n: int | IntoExprColumn, offset: int | IntoExprColumn = 0) -> Series: Take every nth value from offset in sublists. n: Gather every nth element. offset: Starting index. Example: s.list.gather_every(2, offset=1) polars.Series.dt.round(every: str | dt.timedelta | IntoExprColumn) -> Series: Round date/datetime to buckets. Parameters: every (str | timedelta). Returns Series of Date or Datetime. Expr.dt.replace_time_zone(time_zone: str | None, *, ambiguous: Ambiguous | Expr = 'raise', non_existent: NonExistent = 'raise') -> Expr. Replace datetime time zone. Modifies timestamp. ambiguous/non_existent: 'raise', 'earliest', 'latest', 'null'. polars.read_excel(source: FileSource, *, sheet_id: int | Sequence[int] | None = None, sheet_name: str | list[str] | tuple[str] | None = None, table_name: str | None = None, engine: ExcelSpreadsheetEngine = 'calamine', engine_options: dict[str, Any] | None = None, read_options: dict[str, Any] | None = None, has_header: bool = True, columns: Sequence[int] | Sequence[str] | None = None, schema_overrides: SchemaDict | None = None, infer_schema_length: int | None = 100, include_file_paths: str | None = None, drop_empty_rows: bool = True, drop_empty_cols: bool = True, raise_if_empty: bool = True) -> DataFrame | dict[str, DataFrame]: Read Excel data into a DataFrame. source: file path. sheet_id, sheet_name, table_name: specify sheet/table. engine: 'calamine', 'openpyxl', 'xlsx2csv'. schema_overrides: override column types. Returns DataFrame or dict of DataFrames. Series.dt.combine(time: dt.time | Series, time_unit: TimeUnit = 'us') -> Series: Create Datetime from Date/Datetime and Time. time: Time literal or Series. time_unit: 'ns', 'us', 'ms'. Returns: Datetime Series. If input is Datetime, replaces time component. If Date, creates new Datetime. Example: s.dt.combine(time(1, 2, 3)) Series.rolling_var(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False, ddof: int = 1) -> Series: Compute rolling variance. window_size is the window length. min_samples is the minimum non-null samples. ddof is the delta degrees of freedom. Example: s.rolling_var(window_size=3) Series.dt.replace_time_zone(time_zone: str|None, ambiguous: str='raise', non_existent: str='raise') -> Series: Replaces timezone in Datetime Series. Modifies timestamp; ignores original timezone. Options for ambiguous/non_existent times. DataFrame.partition_by(by: ColumnNameOrSelector | Sequence[ColumnNameOrSelector], *more_by: ColumnNameOrSelector, maintain_order: bool = True, include_key: bool = True, as_dict: bool = False) -> list[DataFrame] | dict[tuple[object, ...], DataFrame]. Group DataFrame by columns, return list of DataFrames or dict of DataFrames. Options for maintain_order, include_key, as_dict. polars.any_horizontal(*exprs: IntoExpr | Iterable[IntoExpr]) -> Expr. Compute bitwise OR horizontally across columns. Kleene logic for nulls. DataFrame.unstack(*, step: int, how: UnstackDirection = 'vertical', columns: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, fill_values: list[Any] | None = None) -> DataFrame Unstack a long table to a wide form without aggregation. step: int. Expr.pipe(function: Callable[Concatenate[Expr, P], T], *args: P.args, **kwargs: P.kwargs) -> T: Apply UDFs sequentially. DataFrame.n_chunks(strategy:'first'|'all'='first') -> int|list[int]. Get number of chunks in ChunkedArrays. Parameters: strategy ('first', 'all'). Returns: int or list[int]. Expr.gather(indices: int | Sequence[int] | IntoExpr | Series | np.ndarray[Any, Any]) -> Expr: Take values by index. indices: UInt32 Series expression. LazyFrame.drop(*columns:ColumnNameOrSelector|Iterable[ColumnNameOrSelector], strict:bool=True) -> LazyFrame. Remove columns from DataFrame. Parameters: *columns:Columns to drop, strict:Validate column names. Returns: LazyFrame. Expr.list.sample(n=None, fraction=None, with_replacement=False, shuffle=False, seed=None) Sample from list. n: Number of items. fraction: Fraction of items. Cannot use both. with_replacement: Allow sampling same value more than once. shuffle: Shuffle order. seed: Random seed. Returns Expr. Series.replace_strict(old, new, default=None, return_dtype=None) Replace values in Series. old: value(s) to replace. new: value(s) to replace with. default: Value for non-replaced values. return_dtype: Output data type. Raises InvalidOperationError if non-null values are not replaced and no default is set. Global string cache must be enabled for categorical values. polars.from_arrow(data: pa.Table | pa.Array | pa.ChunkedArray | pa.RecordBatch | Iterable[pa.RecordBatch | pa.Table], schema: SchemaDefinition | None = None, *, schema_overrides: SchemaDict | None = None, rechunk: bool = True) -> DataFrame | Series: Create DataFrame/Series from Arrow data. Supports schema, schema_overrides, and rechunk. Series.ewm_var(*, com: float | None = None, span: float | None = None, half_life: float | None = None, alpha: float | None = None, adjust: bool = True, bias: bool = False, min_samples: int = 1, ignore_nulls: bool = False) -> Series: Compute exponentially-weighted moving variance. Parameters: com, span, half_life, alpha (specify decay). adjust: Divide by decaying adjustment factor. bias: Apply correction for unbiased estimate. min_samples: Minimum observations in window. ignore_nulls: Ignore missing values when calculating weights. Example: s = pl.Series("a", [1, 2, 3]); s.ewm_var(com=1, ignore_nulls=False) Expr.rolling_quantile(quantile: float, interpolation: RollingInterpolationMethod = 'nearest', window_size: int = 2, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr: Compute rolling quantile. quantile: Quantile (0.0 to 1.0). interpolation: Interpolation method. window_size: Window length. weights: Optional weights. min_samples: Min non-null samples. center: Center labels. Example: pl.col("A").rolling_quantile(quantile=0.25, window_size=4) LazyFrame.unique(subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None, keep: UniqueKeepStrategy = 'any', maintain_order: bool = False) -> LazyFrame: Drop duplicate rows. keep: 'first', 'last', 'any', 'none'. Fails with List column in subset. polars.Series.diff(n: int = 1, null_behavior: NullBehavior = 'ignore') -> Series Calculates the first discrete difference between shifted items. n: Number of slots to shift. null_behavior: How to handle null values ('ignore', 'drop'). Series.shift(n: int = 1, *, fill_value: IntoExpr | None = None) -> Series. Shift values by n indices (negative n shifts opposite). fill_value: Value to fill nulls. Series.dt.time() -> Series: Extracts local time from Date/Datetime/Time columns. Returns Series of Time. polars.from_records(data: Sequence[Any], schema: SchemaDefinition | None = None, *, schema_overrides: SchemaDict | None = None, strict: bool = True, orient: Orientation | None = None, infer_schema_length: int | None = 100) -> DataFrame: Construct a DataFrame from a sequence of sequences (clones data). 'data' is a sequence of sequences. 'schema' defines column names/types. 'schema_overrides' allows dtype overrides. 'strict' controls error throwing on type mismatches. 'orient' specifies row/column orientation ('row', 'col', or None for inference). 'infer_schema_length' limits row scan for inference. Example: pl.from_records([[1, 2, 3], [4, 5, 6]], schema=["a", "b"]) Expr.list.contains(item: float | str | bool | int | date | datetime | time | IntoExprColumn) -> Expr. Checks if sublists contain the item. Series.str.encode(encoding: TransferEncoding) Returns Series[String]. Encode string value. Parameters: encoding: 'hex', 'base64'. Example: s.str.encode('hex'). Series.list.gather(indices: Series | list[int] | list[list[int]], *, null_on_oob: bool = False) -> Series: Take sublists by multiple indices. indices: Indices to return per sublist. null_on_oob: Behavior for out-of-bounds indices; if True, set as null, otherwise raise an error. Returns: Series. polars.LazyFrame.join_where(other: LazyFrame, *predicates: Expr | Iterable[Expr], suffix: str = '_right') -> LazyFrame: Join based on inequality predicates. Parameters: other (LazyFrame), predicates (Expr | Iterable[Expr]), suffix (str). Inner join, experimental. Expr.gather_every(n: int, offset: int = 0) -> Expr. Take every nth value in the Series. polars.Expr.bin.ends_with(suffix: IntoExpr) -> Expr: Checks if string values end with a binary substring. Returns: Boolean Expr. Series.dt.microsecond() -> Series[Int32] Extract microseconds from DateTime. DataFrame.hash_rows(seed: int = 0, seed_1: int | None = None, seed_2: int | None = None, seed_3: int | None = None) -> Series: Hash and combine rows. Returns UInt64 Series. Seed params for random seed. Not stable across Polars versions. GroupBy.agg(*aggs: IntoExpr | Iterable[IntoExpr], **named_aggs: IntoExpr) -> DataFrame: Compute aggregations for each group. aggs: Positional aggregations. named_aggs: Keyword aggregations. Series.limit(n: int = 10) -> Series. Get first n elements. Alias for head(). n: Negative value returns all elements except last abs(n). Series.list.to_struct(n_field_strategy: ListToStructWidthStrategy = 'first_non_null', fields: Callable[[int], str] | Sequence[str] | None = None) -> Series: Convert List Series to Struct Series. Parameters: n_field_strategy: 'first_non_null'|'max_width', fields: field names or function. Expr.str.extract_groups(pattern: str) -> Expr Extract all capture groups for regex pattern. Returns Struct of strings. Unnamed groups are numerical strings. Expr.cum_sum(reverse: bool=False) -> Expr: Cumulative sum. Small integer types cast to Int64 to prevent overflow. Nulls excluded; use forward_fill for inclusion. Series.append(other: Series) -> Self: Append a Series to this one in-place. Returns series for convenience. Resulting series will consist of multiple chunks. See also: extend. a = pl.Series("a", [1, 2, 3]); b = pl.Series("b", [4, 5]); a.append(b) Expr.list.count_matches(element:IntoExpr) -> Expr. Count element occurrences in list. Series.str.strptime(dtype: PolarsTemporalType, format: str | None = None, *, strict: bool = True, exact: bool = True, cache: bool = True, ambiguous: Ambiguous | Series = 'raise') -> Series Converts a String column into a Date/Datetime/Time column. format defaults to inferred. Expr.map_batches(function: Callable[[Series], Series | Any], return_dtype: PolarsDataType | None = None, *, agg_list: bool = False, is_elementwise: bool = False, returns_scalar: bool = False) -> Expr. Apply custom python function to Series. Params: function, return_dtype, agg_list, is_elementwise, returns_scalar. Returns: Expr. Warning: return_dtype may lead to unexpected results if not provided. Series.str.contains_any(patterns: Series | list[str], *, ascii_case_insensitive: bool = False) -> Series: Uses Aho-Corasick to check if any patterns are contained. ascii_case_insensitive: Enables ASCII case-insensitive matching. Supports literal string matches only. Expr.arr.sort(*, descending: bool = False, nulls_last: bool = False) -> Expr Sort the arrays in this column. polars.Expr.cum_max(*, reverse: bool = False) -> Expr: Computes cumulative max. Ignores nulls. `reverse` argument flips order. DataFrame.transpose(*, include_header: bool = False, header_name: str = 'column', column_names: str | Iterable[str] | None = None) -> DataFrame. Transpose DataFrame over diagonal. Expensive. Options for header and column names. Expr.dt.microsecond() -> Expr: Extract microseconds from underlying DateTime representation. Applies to Datetime columns. polars.concat_arr(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr: Horizontally concatenate columns into a single array column. Expr.arr.contains(item: float | str | bool | int | date | datetime | time | IntoExprColumn) -> Expr. Check if sub-arrays contain item. Expr.rolling_mean_by(by:IntoExpr,window_size:timedelta|str,min_samples:int=1,closed:ClosedInterval='right')->Expr: Rolling mean by column. Unstable API. by column must be DateTime, Date, UInt64, UInt32, Int64, or Int32. window_size: timedelta or string. DataFrame.with_columns(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> DataFrame: Add columns to DataFrame. Replaces existing columns. Parameters: *exprs (positional expressions/column names), **named_exprs (keyword expressions, renames column). Returns: new DataFrame. Example: df.with_columns((pl.col("a") ** 2).alias("a^2")) polars.business_day_count(start: date | IntoExprColumn, end: date | IntoExprColumn, week_mask: Iterable[bool] = (True, True, True, True, True, False, False), holidays: Iterable[date] = ()) -> Expr Count business days between start and end (excluding end). Series.bin.encode(encoding: TransferEncoding) -> Series: Encode values using encoding. encoding: 'hex', 'base64'. Returns String Series. s = pl.Series("colors", [b"\x00\x00\x00", b"\xff\xff\x00", b"\x00\x00\xff"]); s.bin.encode("hex") Series.kurtosis(*, fisher: bool = True, bias: bool = True) -> float | None. Compute kurtosis (Fisher/Pearson). Params: fisher, bias. Series.rolling_min(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series: Applies rolling min. window_size: window length. weights: optional weights. min_samples: min non-null values (default window_size). center: center labels. Expr.head(n: int | Expr = 10) -> Expr. Gets first n rows. n: Number of rows to return. Expr.backward_fill(limit: int | None = None) -> Expr: Fill missing values with next non-null value. limit: consecutive null values to fill. See also: forward_fill, shift. Example: df = pl.DataFrame({"a": [1, 2, None], "b": [4, None, 6], "c": [None, None, 2]}); df.select(pl.all().backward_fill()) Expr.append(other: IntoExpr, *, upcast: bool = True) -> Expr Append expressions by adding chunks of other. Expr.str.pad_start(length: int, fill_char: str = ' ') -> Expr: Pad the start of string to given length. length: Target length, strings longer are unchanged. fill_char: Character to pad with. See also: pad_end, zfill. Expr.dt.total_minutes() -> Expr. Extract total minutes from Duration type. Returns Expr[Int64]. DataFrame.drop_nans(subset=None) Drop rows containing NaN values. subset: Column names to consider for NaNs; if None, use all columns with float type. Returns DataFrame. Expr.sub(other: Any) -> Expr. Subtraction operator (expr - other). Parameters: other: Numeric literal or expression. LazyGroupBy.quantile(quantile: float, interpolation: RollingInterpolationMethod='nearest') -> LazyFrame: Compute quantile per group. Expr.ne(other: Any) -> Expr: Inequality operator (expr != other). other: Literal or expression. df.with_columns(pl.col("x").ne(pl.col("y")).alias("x != y")) Series.is_between(lower_bound: IntoExpr, upper_bound: IntoExpr, closed: ClosedInterval = 'both') -> Series Get boolean mask between bounds. Parameters: lower_bound, upper_bound: Boundary values. closed: Interval closure type ('both', 'left', 'right', 'none'). Note: False if lower > upper. polars.read_ods(source: FileSource, *, sheet_id: int | Sequence[int] | None = None, sheet_name: str | list[str] | tuple[str] | None = None, has_header: bool = True, columns: Sequence[int] | Sequence[str] | None = None, schema_overrides: SchemaDict | None = None, infer_schema_length: int | None = 100, include_file_paths: str | None = None, drop_empty_rows: bool = True, drop_empty_cols: bool = True, raise_if_empty: bool = True) -> DataFrame | dict[str, DataFrame]: Reads ODS file into DataFrame. Handles multiple sheets. Allows schema overrides. LazyFrame.describe(percentiles: Sequence[float] | float | None = (0.25, 0.5, 0.75), *, interpolation: RollingInterpolationMethod = 'nearest') -> DataFrame. Creates summary stats for a LazyFrame. percentiles: percentiles to include (range [0, 1]). interpolation: {'nearest','higher','lower','midpoint','linear'}. Returns: DataFrame. Warning: collects the final result. Expr.hist(bins: IntoExpr | None = None, *, bin_count: int | None = None, include_category: bool = False, include_breakpoint: bool = False) -> Expr. Bins values into buckets and counts occurrences. LazyFrame.interpolate() Returns LazyFrame. Interpolate intermediate values linearly. Example: lf.interpolate().collect(). Expr.rolling_min_by(by: IntoExpr, window_size: timedelta | str, *, min_samples: int = 1, closed: ClosedInterval = 'right') -> Expr. Apply rolling min based on another column. Parameters: by (DateTime, Date, UInt64, UInt32, Int64, Int32), window_size (timedelta or duration string "1ns", "1us", "1ms", "1s", "1m", "1h", "1d", "1w", "1mo", "1q", "1y", "1i"), min_samples (int), closed ('left', 'right', 'both', 'none'). Series.str.concat(delimiter: str | None = None, *, ignore_nulls: bool = True) -> Series: Vertically concatenate string values. Deprecated, use join(). delimiter: Separator. ignore_nulls: Ignore nulls or propagate. Expr.str.to_datetime(format: str | None = None, time_unit: TimeUnit | None = None, time_zone: str | None = None, strict: bool = True, exact: bool = True, cache: bool = True, ambiguous: Ambiguous | Expr = 'raise') -> Expr: Convert String column to Datetime column. Parameters: format (strftime), time_unit (ns, us, ms), time_zone, strict (raise on failure), exact (require exact format), cache, ambiguous ('raise', 'earliest', 'latest', 'null'). polars.date_range(start: date | datetime | IntoExprColumn, end: date | datetime | IntoExprColumn, interval: str | timedelta = '1d', *, closed: ClosedInterval = 'both', eager: bool = False) -> Series | Expr: Generate date range. Parameters: start, end, interval, closed, eager (bool). Returns Expr or Series of Date. Series.dt.convert_time_zone(time_zone: str) -> Series: Convert Datetime Series to given time zone. For naive datetimes, assumes UTC conversion. polars.align_frames(*frames: FrameType, on: str | Expr | Sequence[str | Expr], how: JoinStrategy = 'full', select: str | Expr | Sequence[str | Expr] | None = None, descending: bool | Sequence[bool] = False) Returns list[FrameType]. Align frames by key columns. Parameters: *frames: frames, on: key columns, how: 'full'/'left', select: columns, descending. Returns aligned frames list. Example: pl.align_frames([df1, df2], on="id"). DataFrame.sample(n: int | Series | None = None, fraction: float | Series | None = None, with_replacement: bool = False, shuffle: bool = False, seed: int | None = None) -> DataFrame: Sample DataFrame rows. `n` and `fraction` are mutually exclusive. Series.rolling_mean(window_size: int, weights=None, min_samples=None, center=False) -> Series: Apply rolling mean. window_size: window length. weights: optional weights. min_samples: min non-null values. center: center labels. Series.list.diff(n: int = 1, null_behavior: NullBehavior = 'ignore') -> Series. Calculate first discrete difference between shifted items in sublists. n: Shift slots. null_behavior: 'ignore' or 'drop'. polars.Series.list.shift(n: int | IntoExprColumn = 1) -> Series. Shift list values by n indices. n: Number of indices to shift (positive is forward, negative is backward). Similar to SQL LAG (positive n) or LEAD (negative n). Example: s.list.shift(-2) LazyFrame.fill_null(value: Any | Expr | None = None, strategy: FillNullStrategy | None = None, limit: int | None = None, *, matches_supertype: bool = True) -> LazyFrame: Fill null values. Parameters: value (fill value), strategy (None, 'forward', 'backward', 'min', 'max', 'mean', 'zero', 'one'), limit (consecutive nulls to fill), matches_supertype. See also: fill_nan. Example: lf.fill_null(99) DataFrame.get_column(name: str, *, default: Any | NoDefault = <no_default>) -> Series | Any: Get column by name. Raises ColumnNotFoundError if no default. Expr.rolling_var(window_size: int, weights: list[float]|None=None, min_samples: int|None=None, center: bool=False, ddof: int=1) -> Expr: Rolling variance. Use rolling for multiple stats on same window. DataFrame.tail(n: int = 5) -> DataFrame: Get the last n rows. If n is negative, return all rows except the first abs(n). See also: head, slice. Series.dt.week() -> Series. Extracts the week from date/datetime. Returns ISO week (1-53). Series.arr.join(separator: IntoExprColumn, *, ignore_nulls: bool = True) -> Series. Join string array items with separator. separator: string separator. ignore_nulls: ignore nulls (default), propagate if False. Error if inner type != String. Returns String Series. Expr.arr.arg_max() -> Expr: Get index of maximum value in each sub-array. Returns UInt32/UInt64 Expr. polars.from_pandas(data: pd.DataFrame | pd.Series[Any] | pd.Index[Any] | pd.DatetimeIndex, *, schema_overrides: SchemaDict | None = None, rechunk: bool = True, nan_to_null: bool = True, include_index: bool = False) -> DataFrame | Series: Construct Polars DataFrame/Series from pandas DataFrame/Series/Index. Clones data. Requires pandas and pyarrow. schema_overrides: Override inferred types. rechunk: Ensure contiguous memory. nan_to_null: Convert NaN to None. include_index: Load non-default indexes as columns. Returns: DataFrame. Example: pl.from_pandas(pd_df) Series.floor() -> Series. Rounds down to the nearest integer value. Only works on floating point Series. Series.dt.total_seconds() -> Series: Extracts total seconds from Duration. Returns Series of Int64. Series.list.set_union(other: Series) -> Series: Computes set union between elements in list and elements of other Series. Example: a.list.set_union(b) Expr.str.extract_all(pattern: str | Expr) -> Expr Extract all regex pattern matches as list. Parameters: pattern (str|Expr). Returns: Expr[List(String)]. Example: df.select(pl.col("foo").str.extract_all(r"\\d+")) Expr.cum_min(*, reverse: bool = False) -> Expr: Calculates cumulative minimum. Expr.bin.size(unit: SizeUnit = 'b') -> Expr. Get binary value size in unit. unit: 'b', 'kb', 'mb', 'gb', 'tb'. Returns UInt32 or Float64 Expr. polars.sum_horizontal(*exprs:IntoExpr|Iterable[IntoExpr], ignore_nulls:bool=True) -> Expr. Sum values horizontally across columns. Parameters: *exprs:Columns to sum, ignore_nulls:Ignore nulls (default True). Returns: Expr. Expr.arr.join(separator: IntoExprColumn, *, ignore_nulls: bool=True) -> Expr. Join string items in array with separator. Params: separator, ignore_nulls. Returns: Expr. Errors if array not string type. Nulls propagated if ignore_nulls=False. Expr.dt.total_seconds() -> Expr: Extract total seconds from Duration. Returns Int64 Expr. map_groups(exprs: Sequence[str | Expr], function: Callable[[Sequence[Series]], Series | Any], return_dtype: PolarsDataType | None = None, *, returns_scalar: bool = True) -> Expr: Apply UDF in GroupBy context. Warning: Slower than native expressions. DataFrame.deserialize(source: str | Path | IOBase, *, format: SerializationFormat = 'binary') -> DataFrame: Read DataFrame from file. source: file path/object. format: "binary" (default), "json". GroupBy.median() -> DataFrame: Returns the median per group. Example: df.group_by("d", maintain_order=True).median() polars.Expr.rolling_std(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False, ddof: int = 1) -> Expr Compute a rolling standard deviation. Parameters: window_size, weights (optional), min_samples (defaults to window_size), center (bool), ddof (int). Expr.all(*, ignore_nulls: bool = True) -> Expr: Check if all values are True. Works on Boolean columns only. ignore_nulls: Ignore nulls (default: True). If False, uses Kleene logic. Expr.list.diff(n:int=1, null_behavior:'ignore'|'drop'='ignore') -> Expr. Calculate discrete difference between shifted sublist items. Parameters: n (shift slots); null_behavior ('ignore', 'drop'). Returns: Expr. LazyFrame.sort(by: IntoExpr | Iterable[IntoExpr], *more_by: IntoExpr, descending: bool | Sequence[bool] = False, nulls_last: bool | Sequence[bool] = False, maintain_order: bool = False, multithreaded: bool = True) -> LazyFrame: Sort LazyFrame by columns. Params: by, more_by, descending, nulls_last, maintain_order, multithreaded. Returns: LazyFrame. LazyFrame.tail(n=5) -> LazyFrame: Get the last n rows. DataFrame.item(row: int | None = None, column: int | str | None = None) -> Any: Return DataFrame as scalar or element at row/column. If no row/col, equivalent to df[0,0], shape (1,1) check. Example: df.item(1, 1) returns element at row 1, column 1. DataFrame.__getitem__(key) -> DataFrame | Series | Any: Get part of DataFrame. key: row/column selector. Returns DataFrame, Series, or scalar. df = pl.DataFrame({"a": [1, 2, 3], "d": [4, 5, 6], "c": [1, 3, 2], "b": [7, 8, 9]}); df[0]; df["a"]; df[0:2, "a"] Expr.str.concat(delimiter: str | None = None, ignore_nulls: bool = True) -> Expr: Concatenate string values. Parameters: delimiter, ignore_nulls (default True, False propagates nulls). Returns: Expr of String type. Deprecated: Use join(). DataFrame.write_ipc(file: str | Path | IO[bytes] | None, *, compression: IpcCompression = 'uncompressed', compat_level: CompatLevel | None = None, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> BytesIO | None Write to Arrow IPC binary stream or Feather file. file: str | Path | IO[bytes] | None. Expr.repeat_by(by: Series | Expr | str | int) -> Expr: Repeat elements as specified, expanding into a List. Parameters: by: Numeric column, coerced to UInt32. Returns: Expr of type List. Example: df.select(pl.col("a").repeat_by("n")) LazyFrame.map_batches(function: Callable[[DataFrame], DataFrame], predicate_pushdown=True, projection_pushdown=True, slice_pushdown=True, no_optimizations=False, schema=None, validate_output_schema=True, streamable=False) -> LazyFrame: Apply custom function. Function must return Polars DataFrame. Optimization flags control pushdown. Streamable: function is streamable. DataFrame.sql(query:str, table_name:str='self') -> DataFrame. Execute SQL query against DataFrame. Parameters: query (SQL query string); table_name (optional, default 'self'). Returns: DataFrame. Note: Unstable feature. Expr.rolling_median(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr. Compute rolling median. Params: window_size, weights, min_samples, center. Expr.cast(dtype: PolarsDataType | type[Any], *, strict: bool = True, wrap_numerical: bool = False) -> Expr: Cast between data types. 'dtype' is the target type. 'strict' controls error raising. 'wrap_numerical' enables wrapping on overflow. Example: pl.col("a").cast(pl.Float64) Expr.str.count_matches(pattern: str | Expr, *, literal: bool = False) -> Expr: Count regex matches. pattern: regex pattern. literal: treat pattern as literal string. Returns UInt32 Expr, null if original is null. Expr.is_first_distinct() -> Expr: Boolean mask for first occurrence of each distinct value. Expr.arr.shift(n: int | IntoExprColumn = 1) -> Expr: Shift array values by n indices. n<0 shifts opposite. SQL LAG (n>0) or LEAD (n<0). LazyFrame.update(other: LazyFrame, on: str | Sequence[str] | None = None, how: Literal['left', 'inner', 'full'] = 'left', *, left_on, right_on, include_nulls: bool = False) -> LazyFrame. Update LazyFrame values with other LazyFrame. How: 'left', 'inner', 'full'. Unstable. Example: lf.update(new_lf).collect() Expr.str.replace(pattern: str | Expr, value: str | Expr, *, literal: bool = False, n: int = 1) -> Expr: Replace first matching regex/literal substring. Args: pattern (regex), value (replacement), literal (bool), n (num replacements). Use (?iLmsuxU) for regex flags. Escape $ as $$. Example: df.with_columns(cost_usd=pl.col("cost").str.replace(r"#(\d+)", "$${1}")) DataFrame.group_by(*by: IntoExpr | Iterable[IntoExpr], maintain_order: bool = False, **named_by: IntoExpr) -> GroupBy: Start a group by operation. by: Columns to group by. maintain_order: Ensure group order matches input (slower, blocks streaming). Returns: GroupBy object. Example: df.group_by("a").agg(pl.col("b").sum()) Series.str.contains(pattern: str | Expr, *, literal: bool = False, strict: bool = True) -> Series[Boolean]: Check if string contains pattern. literal=True for literal match. strict=True error on invalid regex. Expr.radians() -> Expr: Convert degrees to radians. Returns Float64. Expr.rolling_median_by(by: IntoExpr, window_size: timedelta | str, *, min_samples: int = 1, closed: ClosedInterval = 'right') -> Expr: Compute a rolling median based on another column. Parameters: by (DateTime, Date, UInt64, UInt32, Int64, Int32), window_size (timedelta or str), min_samples, closed ('left', 'right', 'both', 'none'). Expr.arr.any() -> Expr. True if any boolean value is true for subarray. Expr.bottom_k_by(by, k=5, reverse=False) Get k smallest elements from column `by`. Args: by (column to sort by), k (number of elements), reverse (sort reversed). Returns Expr. Non-null elements are preferred over null. Output order not guaranteed; use sort(). polars.tail(column:str, n:int=10) -> Expr. Get last n rows of column. Parameters: column (column name); n (rows to return). Returns: Expr. Series.arr.unique(maintain_order=False) -> Series: Get unique values in the array. Expr.exclude(columns: str | PolarsDataType | Collection[str] | Collection[PolarsDataType], *more_columns: str | PolarsDataType) -> Expr: Exclude columns from multi-column expression. polars.read_ndjson(source: str | Path | list[str] | list[Path] | IOBase | bytes, *, schema: SchemaDefinition | None = None, schema_overrides: SchemaDefinition | None = None, infer_schema_length: int | None = 100, batch_size: int | None = 1024, n_rows: int | None = None, low_memory: bool = False, rechunk: bool = False, row_index_name: str | None = None, row_index_offset: int = 0, ignore_errors: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, file_cache_ttl: int | None = None, include_file_paths: str | None = None,) -> DataFrame: Read DataFrame from newline delimited JSON file. Parameters: source (str|Path|...); schema (SchemaDefinition); schema_overrides (dict); infer_schema_length (int); batch_size (int); n_rows (int); low_memory (bool); rechunk (bool); row_index_name (str); row_index_offset (int); ignore_errors (bool); storage_options (dict); credential_provider (function); retries (int); file_cache_ttl (int); include_file_paths(str). Example: pl.read_ndjson(StringIO(json_str)) polars.mean_horizontal(*exprs: IntoExpr | Iterable[IntoExpr], ignore_nulls: bool = True) -> Expr Compute mean horizontally across columns. Parameters: *exprs (columns/expressions), ignore_nulls (ignore nulls, default True; if False, null input leads to null output). Example: df.with_columns(mean=pl.mean_horizontal("a", "b")) computes the horizontal mean of columns "a" and "b". DataFrame.drop(*columns: ColumnNameOrSelector, strict: bool = True) Returns DataFrame. Remove columns. Parameters: *columns: column names/selectors, strict: validate column existence. Example: df.drop("col1"). DataFrame.sort(by: IntoExpr | Iterable[IntoExpr], *more_by: IntoExpr, descending: bool | Sequence[bool] = False, nulls_last: bool | Sequence[bool] = False, multithreaded: bool = True, maintain_order: bool = False) -> DataFrame. Sort DataFrame by columns 'by', '*more_by'. Options: descending, nulls_last, multithreaded, maintain_order. DataFrame.to_series(index: int = 0) -> Series: Select column as Series at index location. index: Location of selection. Example: df.to_series(1) Series.list.sort(descending: bool = False, nulls_last: bool = False, multithreaded: bool = True) -> Series Sort arrays in this column. Series.tail(n: int = 10) -> Series: Gets the last n elements. n: Number of elements. Negative n returns all elements except the first abs(n). Series.pow(exponent: int | float | Series) -> Series. Raise Series to the power of exponent. Series.str.normalize(form: UnicodeForm = 'NFC') -> Series Returns Unicode normal form of string values. Parameters: form (str 'NFC', 'NFKC', 'NFD', 'NFKD'). Returns: Series. Expr.shift(n: int|IntoExprColumn=1, fill_value: IntoExpr|None=None) -> Expr: Shift values by n indices. Similar to SQL LAG/LEAD. fill_value for nulls. Expr.mul(other: Any) -> Expr. Multiplication operator (expr * other). DataFrame.upsample(time_column: str, *, every: str | timedelta, group_by: str | Sequence[str] | None = None, maintain_order: bool = False) -> DataFrame: Upsample DataFrame at regular frequency. time_column: Time column for date range. every: Interval (e.g., "1mo"). group_by: Group by these columns first. maintain_order: Keep order. Returns sorted DataFrame. Example: df.upsample(time_column="time", every="1mo", group_by="groups", maintain_order=True) Series.list.join(separator: IntoExprColumn, ignore_nulls: bool=True) -> Series: Join strings in sublist with separator. Errors if inner type != String. ignore_nulls=False propagates nulls. Series.str.replace_many(patterns: Series | list[str] | Mapping[str, str], replace_with: Series | list[str] | str | NoDefault = , *, ascii_case_insensitive: bool = False) -> Series: Replace multiple string patterns using Aho-Corasick. patterns: str patterns to search/replace, or mapping of patterns to replacements. replace_with: replacement strings. ascii_case_insensitive: enable ASCII case-insensitive matching. Supports string literals only. Example: s.str.replace_many(["you", "me"], ["me", "you"]) polars.max_horizontal(*exprs: IntoExpr | Iterable[IntoExpr]) -> Expr: Get max value horizontally across columns. Parameters: *exprs: Column(s) (str or Expr). Example: df.with_columns(max=pl.max_horizontal("a", "b")) polars.LazyFrame.cast(dtypes: Mapping[ColumnNameOrSelector | PolarsDataType, PolarsDataType | PythonDataType] | PolarsDataType, *, strict: bool = True) -> LazyFrame: Casts LazyFrame columns to specified dtypes. Uses selectors. Throws error if cast fails when strict=True. Series.str.replace(pattern: str, value: str, *, literal: bool = False, n: int = 1) -> Series: Replace first matching regex/literal substring. Parameters: pattern (str): Regex pattern; value (str): Replacement string; literal (bool): Treat pattern as literal; n (int): Number of matches to replace. $ must be escaped as $$. Series.dt.strftime(format: str) -> Series. Convert Date/Time/Datetime column into a String column with the given format. Expr.rank(method='average', descending=False, seed=None) -> Expr: Assign ranks to data, handling ties. Method options: 'average', 'min', 'max', 'dense', 'ordinal', 'random'. Descending: rank in descending order. Seed: for method="random". Example: df.select(pl.col("a").rank("ordinal")) DataFrame.serialize(file: IOBase | str | Path | None = None, *, format: SerializationFormat = 'binary') -> bytes | str | None: Serialize DataFrame to file or string. format: "binary" (default) or "json". file: File path or object. If None, return string. Returns bytes or str depending on format. Not stable across Polars versions. Series.dt.min() -> dt.date|dt.datetime|dt.timedelta|None: Returns minimum as Python datetime. Series.str.decode(encoding: TransferEncoding, *, strict: bool = True) -> Series: Decode values using encoding. encoding: 'hex', 'base64'. strict: Raise error or mask with null. Returns Binary Series. Example: s.str.decode("hex") DataFrame.with_row_index(name: str = 'index', offset: int = 0) -> DataFrame. Add row index as first column. Index starts at offset. Series.dt.total_hours() -> Series. Extract the total hours from a Duration type. Expr.dt.time() -> Expr. Extract time from Datetime column. Fails on Date columns. Expr.list.var(ddof: int = 1) -> Expr: Compute variance of lists in array. Params: ddof (int, default 1). Example: df.with_columns(pl.col("values").list.var().alias("var")) DataFrame.join_asof(other: DataFrame, *, left_on: str | None | Expr = None, right_on: str | None | Expr = None, on: str | None | Expr = None, by_left: str | Sequence[str] | None = None, by_right: str | Sequence[str] | None = None, by: str | Sequence[str] | None = None, strategy: AsofJoinStrategy = 'backward', suffix: str = '_right', tolerance: str | int | float | timedelta | None = None, allow_parallel: bool = True, force_parallel: bool = False, coalesce: bool = True, allow_exact_matches: bool = True, check_sortedness: bool=True) -> DataFrame: Perform an asof join, similar to a left-join but matches on nearest key. DataFrames must be sorted by the 'on' key. Strategies: 'backward' (selects last row in right whose 'on' key <= left's), 'forward' (selects first row in right whose 'on' key >= left's), 'nearest'. Parameters: other, left_on, right_on, on, by, by_left, by_right, strategy, suffix, tolerance, allow_parallel, force_parallel, coalesce, allow_exact_matches, check_sortedness. Tolerance: Sets numeric/temporal tolerance. check_sortedness: Checks if keys are sorted. Expr.list.unique(*, maintain_order: bool = False) -> Expr: Get unique/distinct values in the list. maintain_order: Maintain order (slower). Example: df.with_columns(unique=pl.col("a").list.unique()) polars.struct(*exprs: IntoExpr | Iterable[IntoExpr], schema: SchemaDict | None = None, eager: bool = False, **named_exprs: IntoExpr) -> Expr | Series Collect columns into a struct column. Parameters: *exprs (positional, parsed as column names or literals), schema, eager (evaluate immediately), **named_exprs (keyword arguments, rename columns). Use pl.all() to collect all. Series.str.extract(pattern: IntoExprColumn, group_index: int = 1) -> Series Extract capture group from regex pattern. pattern: regex; group_index: capture group index. Expr.extend_constant(value: IntoExpr, n: int | IntoExprColumn) -> Expr: Extend Series with n copies of value. Fast method. Expr.dt.to_string(format: str | None = None) -> Expr Convert Date/Time/Datetime column into a String column with the given format. format: str or None. polars.datetime(year, month, day, hour=None, minute=None, second=None, microsecond=None, time_unit='us', time_zone=None, ambiguous='raise') Create Datetime literal expression. year, month, day, hour, minute, second, microsecond: Column or literal values. time_unit: 'us', 'ms', 'ns'. time_zone: Time zone string. ambiguous: 'raise', 'earliest', 'latest', 'null'. Returns Expr. Expr.dt.ordinal_day() -> Expr: Extract ordinal day from Date/Datetime (day of year, 1-366). Expr.interpolate_by(by: IntoExpr) -> Expr: Interpolates null values using another column. Expr.where(predicate: Expr) -> Expr: Filters a column based on predicate. Deprecated; use filter(). Series.str.zfill(length: int | IntoExprColumn) -> Series: Pad start of string with zeros to length. Handles sign prefix. Parameters: length. See also: pad_start(). Note: Intended for numeric strings; for non-ASCII, use pad_start(). Example: s.cast(pl.String).str.zfill(4) GroupBy.sum() -> DataFrame: Reduce the groups to the sum. Series.arr.get(index: int | IntoExprColumn, *, null_on_oob: bool = False) -> Series: Get value by index in sub-arrays. null_on_oob: True -> None if out of bounds, False -> error. polars.fold(acc: IntoExpr, function: Callable[[Series, Series], Series], exprs: Sequence[Expr | str] | Expr) -> Expr. Accumulate over columns horizontally/row wise with left fold. Expr.list.to_struct(n_field_strategy: ListToStructWidthStrategy = 'first_non_null', fields: Sequence[str] | Callable[[int], str] | None = None, upper_bound: int = 0, _eager: bool = False) -> Expr: Convert list to struct. Requires correct 'upper_bound' for LazyFrame. Series.search_sorted(element: IntoExpr | np.ndarray[Any, Any] | None, side: SearchSortedSide = 'any') -> int | Series. Find indices to maintain order. side: 'any', 'left', 'right'. polars.testing.assert_series_equal(left: Series, right: Series, *, check_dtypes: bool = True, check_names: bool = True, check_order: bool = True, check_exact: bool = False, rtol: float = 1e-05, atol: float = 1e-08, categorical_as_str: bool = False) -> None: Assert Series equality, raise AssertionError if different. Parameters: left, right, check_dtypes, check_names, check_order, check_exact, rtol, atol, categorical_as_str. DataFrame.write_csv(file: str | Path | IO[str] | IO[bytes] | None = None, *, include_bom: bool = False, include_header: bool = True, separator: str = ',', line_terminator: str = '\n', quote_char: str = '"', batch_size: int = 1024, datetime_format: str | None = None, date_format: str | None = None, time_format: str | None = None, float_scientific: bool | None = None, float_precision: int | None = None, null_value: str | None = None, quote_style: CsvQuoteStyle | None = None, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> str | None: Write DataFrame to CSV file. file is the file path. separator is the field separator. quote_style is the quoting strategy. storage_options are cloud provider options. Returns output as string if file is None. polars.concat_list(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr. Horizontally concatenate columns into a single list column. Linear time. exprs: Columns to concatenate. Accepts expression input, strings as names. *more_exprs: Additional columns. Example: df.with_columns(concat_list=pl.concat_list("a", "b")) Series.struct.field(name: str) -> Series: Retrieve field of Struct as a new Series. Params: name (str). Example: s = pl.Series([{"a": 1, "b": 2}, {"a": 3, "b": 4}]); s.struct.field("a") DataFrame.equals(other: DataFrame, *, null_equal: bool = True) -> bool. Checks if DataFrame equals another DataFrame. null_equal: consider null values as equal. Expr.list.tail(n: int | str | Expr = 5) -> Expr Slice the last n values of every sublist. polars.Series.pct_change(n: int | IntoExprColumn = 1) -> Series: Computes percentage change between values. Handles nulls. `n` specifies the period. Expr.diff(n: int = 1, null_behavior: NullBehavior = 'ignore') -> Expr. Calculate first discrete difference between shifted items. LazyFrame.unpivot(on: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, *, index: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, variable_name: str | None = None, value_name: str | None = None) -> LazyFrame. Unpivot DataFrame from wide to long. Similar to pandas.melt. Series.ewm_mean(*, com: float | None = None, span: float | None = None, half_life: float | None = None, alpha: float | None = None, adjust: bool = True, min_samples: int = 1, ignore_nulls: bool = False) -> Series: Compute exponentially-weighted moving average. Expr.struct.unnest() -> Expr. Expand struct into individual fields. Alias for Expr.struct.field("*"). Returns Expr. polars.read_delta(source: str | DeltaTable, *, version: int | str | datetime | None = None, columns: list[str] | None = None, rechunk: bool | None = None, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', delta_table_options: dict[str, Any] | None = None, use_pyarrow: bool = False, pyarrow_options: dict[str, Any] | None = None) -> DataFrame: Reads DataFrame from Delta lake table. Parameters: source: DeltaTable or path/URI. version: Table version. columns: Columns to select. storage_options: Storage backend options. Returns: DataFrame. polars.read_ipc(source: str | Path | IO[bytes] | bytes, *, columns: list[int] | list[str] | None = None, n_rows: int | None = None, use_pyarrow: bool = False, memory_map: bool = True, storage_options: dict[str, Any] | None = None, row_index_name: str | None = None, row_index_offset: int = 0, rechunk: bool = True) -> DataFrame: Read Arrow IPC (Feather v2) file. source: File path or object. columns: Columns to select. n_rows: Number of rows to read. use_pyarrow: Use pyarrow reader. memory_map: Memory map file. storage_options: Storage options. row_index_name: Row index name. row_index_offset: Row index offset. rechunk: Rechunk data. Warning: memory_map prevents writing to the same file. Expr.list.gather(indices: Expr | Series | list[int] | list[list[int]], null_on_oob: bool = False) -> Expr: Take sublists by indices. indices: Indices to return per sublist. null_on_oob: True -> set out of bounds index as null, False -> raise error. Expr.str.replace_many(patterns: IntoExpr | Mapping[str, str], replace_with: IntoExpr | NoDefault = , *, ascii_case_insensitive: bool = False) -> Expr: Replaces many matches using the Aho-Corasick algorithm. patterns: String patterns to replace, or a mapping of patterns to replacements. replace_with: Strings to replace with. ascii_case_insensitive: Enable ASCII case-insensitive matching. Supports string literals only; no regex. Example: pl.col("lyrics").str.replace_many(["me", "you"], ["you", "me"]) Expr.replace(old: IntoExpr | Sequence[Any] | Mapping[Any, Any], new: IntoExpr | Sequence[Any] | NoDefault = , *, default: IntoExpr | NoDefault = , return_dtype: PolarsDataType | None = None) -> Expr: Replace values. Accepts expressions, sequences, mappings. Deprecated: Use replace_strict(). Expr.rolling_max(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr. Moving max over array. window_size: Length of window. weights: Multiply elementwise with values in window. min_samples: Non-null values required, defaults to window_size. center: Center labels. Example: df.with_columns(pl.col("A").rolling_max(window_size=2, weights=[0.25, 0.75])) Series.to_dummies(*, separator: str = '_', drop_first: bool = False) -> DataFrame: Get dummy/indicator variables. Params: separator (str), drop_first (bool). Example: s = pl.Series("a", [1, 2, 3]); s.to_dummies() DataFrame.drop_in_place(name: str) -> Series: Drops a single column in-place and returns it. Parameters: name: Column name to drop. Returns: The dropped column. Config.set_decimal_separator(separator: str|None=None) -> type[Config]: Set the decimal separator character. separator: character to use, None reverts to default. Series.is_in(other: Series | Collection[Any]) -> Series. Check if Series elements are in other Series/Collection. Returns Boolean Series. Example: s2.is_in(s) Expr.ewm_var(com:float=None, span:float=None, half_life:float=None, alpha:float=None, adjust:bool=True, bias:bool=False, min_samples:int=1, ignore_nulls:bool=False) -> Expr. Compute exponentially-weighted moving variance. Parameters: com, span, half_life, alpha (decay specifiers); adjust (adjust for imbalance); bias (unbiased estimate); min_samples; ignore_nulls. Returns: Expr. Expr.list.sort(descending: bool = False, nulls_last: bool = False) -> Expr: Sort lists in column. descending=True for descending sort, nulls_last=True to place nulls last. Series.str.splitn(by, n) -> Series: Split string by substring, return at most n items. If splits < n-1, remaining elements will be null. If splits >= n-1, last substring contains remainder. Series.cast(dtype: type[int | float | str | bool] | PolarsDataType, *, strict: bool = True, wrap_numerical: bool = False) -> Self: Cast between data types. Parameters: dtype - DataType to cast to. strict - If True, invalid casts raise exceptions. wrap_numerical - If True, numeric casts wrap values. Example: s.cast(pl.UInt32) GroupBy.quantile(quantile: float, interpolation: RollingInterpolationMethod = 'nearest') -> DataFrame. Compute quantile per group. Example: df.group_by("d").quantile(1) GroupBy.len(name: str | None = None) -> DataFrame: Returns number of rows per group. name: Result column name (default: len) Series.extend(other: Series) -> Self Extend Series with values from another Series. Modifies in-place. polars.Series.str.extract_groups(pattern: str) -> Series: Extract regex capture groups. Parameters: pattern (str regex with capture groups). Returns Series of Struct[String]. DataFrame.insert_column(index: int, column: IntoExprColumn) -> DataFrame. Insert Series 'column' at 'index'. In-place. SQLContext.register_globals(n: int | None, *, all_compatible: bool) -> Self: Register frames from globals scope as tables. Params: n, all_compatible. Returns: Self. LazyFrame.serialize(file=None,format='binary') -> bytes|str|None: Serializes LazyFrame's logical plan. file: file path or None (returns string). format: 'binary' or 'json'. Serialization is not version-stable. Expr.is_last_distinct() -> Expr: Returns boolean mask indicating last occurrence of each distinct value. Result is Boolean Expr. Example: df.with_columns(pl.col("a").is_last_distinct().alias("last")) polars.std(column: str, ddof: int = 1) -> Expr: Get standard deviation. Parameters: column (column name), ddof (delta degrees of freedom, N-ddof, default 1). Series.dt.year() -> Series. Extract year from Date/Datetime columns. Returns: Series[Int32]. Expr.list.std(ddof=1) Compute the std value of the lists in the array. ddof: Delta Degrees of Freedom (divisor is N - ddof). Returns Expr. Expr.str.starts_with(prefix: str|Expr) -> Expr: Check if string values start with a substring. prefix:prefix substring. Expr.list.to_array(width: int) -> Expr. Convert List column to Array. Params: width. Returns: Expr. Expr.dt.total_days() -> Expr: Extract the total days from a Duration type. Returns: Expr of data type Int64. LazyFrame.slice(offset: int, length: int = None) -> LazyFrame. Get a slice of DataFrame. Parameters: offset (start index, negative indexing supported), length (slice length, None for all rows from offset). polars.Series.rolling_median(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series Compute a rolling median. Unstable. Includes the row itself and window_size - 1 elements before it. Parameters: window_size, weights (elementwise multiplication), min_samples (default window_size), center. Example: s = pl.Series("a", [1.0, 2.0, 3.0, 4.0, 6.0, 8.0]); s.rolling_median(window_size=3) returns [null, null, 2.0, 3.0, 4.0, 6.0]. Series.gather_every(n: int, offset: int = 0) -> Series. Take every nth value in Series. Start from offset. DataFrame.head(n: int = 5) -> DataFrame: Get the first 'n' rows. If 'n' is negative, return all rows except the last abs(n). Example: df.head(3) DataFrame.update(other: DataFrame, on: str | Sequence[str] | None = None, how: Literal['left', 'inner', 'full'] = 'left', *, left_on: str | Sequence[str] | None = None, right_on: str | Sequence[str] | None = None, include_nulls: bool = False) -> DataFrame: Update values with values from other DataFrame. on: Join columns. how: 'left', 'inner', 'full'. left_on: Left join columns. right_on: Right join columns. include_nulls: Overwrite with nulls from right. Unstable. Example: df.update(new_df) LazyFrame.top_k(k: int, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool] = False) -> LazyFrame. Return the k largest rows. Expr.list.set_intersection(other: IntoExpr) -> Expr: Compute set intersection between lists. Parameter: other: Right hand side expression. Series.index_of(element: IntoExpr) -> int | None: Get index of first occurrence of value; None if not found. Series.replace(old: IntoExpr | Sequence[Any] | Mapping[Any, Any], new: IntoExpr | Sequence[Any] | NoDefault, default: IntoExpr | NoDefault = , return_dtype: PolarsDataType | None = None) -> Self: Replace values. Requires global string cache for categorical values. polars.scan_iceberg(source: str | Table, snapshot_id: int | None = None, storage_options: dict | None = None) -> LazyFrame: Lazily read Apache Iceberg table. source: path or table. snapshot_id: specific snapshot. storage_options: cloud storage config. Series.hist(bins: list[float] | None = None, *, bin_count: int | None = None, include_category: bool = True, include_breakpoint: bool = True) -> DataFrame: Bin values into buckets and count occurrences. Unstable. Parameters: bins, bin_count, include_category, include_breakpoint. LazyFrame.collect_schema() -> Schema: Resolves the schema of the LazyFrame. Example: lf.collect_schema() Series.dt.add_business_days(n: int | IntoExpr, week_mask: Iterable[bool] = (True, True, True, True, True, False, False), holidays: Iterable[dt.date] = (), roll: Roll = 'raise') -> Series: Offset by n business days. n: Number of days. week_mask: Which days to count (default Mon-Fri). holidays: Holidays to exclude. roll: 'raise', 'forward', 'backward' if start date is non-business day. Example: s.dt.add_business_days(5) Config.set_tbl_rows(n: int | None) -> type[Config] Set max rows to draw in table. If n < 0, display all rows. Series.str.explode() -> Series: Returns column with separate row for every string character. Returns: String Series. Deprecated: Use .str.split("").explode(). Empty strings result in null. Example: s.str.explode() polars.median(*columns: str): Get median value. Sugar for pl.col(columns).median(). Returns Expr. Parameters: columns. DataFrame.write_avro(file: str | Path | IO[bytes], compression: AvroCompression = 'uncompressed', name: str = '') -> None: Writes to Apache Avro file. file: File path or object. compression: Compression method. name: Schema name. Expr.cut(breaks: Sequence[float], *, labels: Sequence[str] | None = None, left_closed: bool = False, include_breaks: bool = False) -> Expr: Bin continuous values into categories. 'breaks' are cut points. 'labels' are category names (len(labels) == len(breaks) + 1). 'left_closed' makes intervals left-closed. 'include_breaks' adds a column with the right endpoint. Returns Categorical or Struct. Example: pl.col("foo").cut([-1, 1], labels=["a", "b", "c"]) Expr.dt.add_business_days(n: int | IntoExpr, *, week_mask: Iterable[bool] = (True, True, True, True, True, False, False), holidays: Iterable[dt.date] = (), roll: Roll = 'raise') -> Expr: Offset by n business days. Parameters: n, week_mask, holidays, roll. Returns: Expr, data type is preserved. Series.sample(n: int | None = None, *, fraction: float | None = None, with_replacement: bool = False, shuffle: bool = False, seed: int | None = None) -> Series: Sample from Series. Parameters: n (items to return, not with fraction), fraction (fraction of items, not with n), with_replacement, shuffle, seed. Example: s.sample(2, seed=0) LazyFrame.join(other: LazyFrame, on: str | Expr | Sequence[str | Expr] | None = None, how: JoinStrategy = 'inner', *, left_on: str | Expr | Sequence[str | Expr] | None = None, right_on: str | Expr | Sequence[str | Expr] | None = None, suffix: str = '_right', validate: JoinValidation = 'm:m', join_nulls: bool = False, coalesce: bool | None = None, maintain_order: MaintainOrderJoin | None = None, allow_parallel: bool = True, force_parallel: bool = False) -> LazyFrame. Join operation. Parameters: other: LazyFrame, on: join columns, how: {'inner', 'left', 'right', 'full', 'semi', 'anti', 'cross'}, left_on, right_on, suffix, validate: {'m:m', 'm:1', '1:m', '1:1'}, join_nulls, coalesce, maintain_order: {'none', 'left', 'right', 'left_right', 'right_left'}, allow_parallel, force_parallel. Expr.value_counts(*, sort: bool = False, parallel: bool = False, name: str | None = None, normalize: bool = False) -> Expr. Count unique value occurrences. Parameters: sort: sort by count, parallel, name: count column name, normalize: relative frequencies. Returns: Struct Expr (value, count). DataFrame.join_where(other: DataFrame, *predicates: Expr | Iterable[Expr], suffix: str = '_right') -> DataFrame. Perform an inner join based on (in)equality predicates. Experimental functionality. Row order not preserved. predicates: (In)Equality conditions, apply suffix if column names duplicate. suffix: For duplicate names. Example: east.join_where(west, pl.col("dur") < pl.col("time"), pl.col("rev") < pl.col("cost")). polars.Expr.str.splitn(by: IntoExpr, n: int) -> Expr Split the string by a substring, restricted to returning at most n items. Parameters: by (substring to split by), n (max number of items to return). Returns: Expr (Struct with String fields). Expr.tail(n: int | Expr = 10) -> Expr: Gets the last n rows. n: Number of rows to return. Expr.str.to_date(format: str | None = None, *, strict: bool = True, exact: bool = True, cache: bool = True) -> Expr[source]. Convert String column to Date. Parameters: format, strict, exact, cache. Example: s.str.to_date(). Series.arr.median() -> Series: Compute the median of sub-arrays. LazyFrame.with_row_index(name: str = 'index', offset: int = 0) -> LazyFrame: Add row index as first column. name: index column name, offset: start offset. Warning: Negative performance impact. Example: lf.with_row_index().collect() Series.explode() -> Series: Explode a list Series into new rows. Returns Series with data type of list elements. See also Series.list.explode. Example: s = pl.Series("a", [[1, 2, 3], [4, 5, 6]]); s.explode() DataFrame.drop_nulls(subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None) -> DataFrame. Drop rows with null values. Subset columns optional. polars.repeat(value: IntoExpr|None, n: int|Expr, dtype: PolarsDataType|None=None, eager: bool=False) -> Expr|Series: Repeat value n times. eager=True returns Series, otherwise Expr. DataFrame.to_pandas(use_pyarrow_extension_array: bool=False, **kwargs) -> pandas.DataFrame: Convert to pandas DataFrame. Requires pandas and pyarrow. use_pyarrow_extension_array for zero-copy. Expr.cum_prod(*, reverse: bool = False) -> Expr: Cumulative product at every element. reverse=True reverses operation. DataFrame.limit(n: int = 5) -> DataFrame. Alias for DataFrame.head(). Returns the first n rows. n: Number of rows, if negative returns all rows except last abs(n). Example: df.limit(3). classmethod Config.set_ascii_tables(active:bool|None=True) -> type[Config]. Use ASCII table outlines. Parameters: active:Use ASCII tables. Returns: type[Config]. Series.str.join(delimiter: str = '', *, ignore_nulls: bool = True) -> Series: Concatenate string values to single string. delimiter: Delimiter between values. ignore_nulls: Ignore nulls (default True). Returns: String Series. If ignore_nulls=False and column contains nulls, output is null. Example: s.str.join("-") Expr.cat.starts_with(prefix:str) -> Expr. Check if string representations start with substring. Parameters: prefix (substring). Returns: Expr. Note: prefix must be literal string. LazyFrame.head(n: int = 5) -> LazyFrame: Get first n rows. Expr.dt.combine(time: dt.time | Expr, time_unit: TimeUnit) -> Expr Create naive Datetime from Date/Datetime and Time. Replaces time in Datetime or combines Date and Time. Series.str.tail(n: int | Expr) -> Series: Return last n characters of each string. n: length of slice (integer or expression). Negative indexing supported. If string length is less than n, full string is returned. Example: s = pl.Series(["pear", None, "papaya", "dragonfruit"]); s.str.tail(5) # returns "pear", null, "apaya", "fruit" DataFrame.rolling(index_column: IntoExpr, period: str | timedelta, offset: str | timedelta | None = None, closed: ClosedInterval = 'right', group_by: IntoExpr | Iterable[IntoExpr] | None = None) -> RollingGroupBy: Create rolling groups on DataFrame based on temporal or integer column. index_column: column for window grouping (sorted ascending within groups if group_by), period: window length, offset: window offset, closed: interval closure, group_by: optional grouping columns. Returns RollingGroupBy object. See also: group_by_dynamic. Example: df.rolling(index_column='dt', period='2d').agg([...]) polars.arange(start: int | IntoExprColumn = 0, end: int | IntoExprColumn | None = None, step: int = 1, *, dtype: PolarsIntegerType = Int64, eager: bool = False) -> Expr | Series. Generate range of integers. Alias for int_range(). eager=True returns Series. polars.min_horizontal(*exprs: IntoExpr | Iterable[IntoExpr]) -> Expr: Get minimum value horizontally across columns. exprs: Columns to aggregate. GroupBy.tail(n: int = 5) -> DataFrame: Get the last n rows of each group. n: Number of rows. Example: df.group_by("letters").tail(2) LazyGroupBy.map_groups(function: Callable[[DataFrame], DataFrame], schema: SchemaDict | None) -> LazyFrame: Applies a function to each group. Slow; use expressions when possible. schema: output schema. Series.to_pandas(*, use_pyarrow_extension_array: bool = False, **kwargs: Any) -> pd.Series. Converts Series to pandas Series. use_pyarrow_extension_array: Use PyArrow-backed extension array for zero copy, preserves nulls. Expr.rolling(index_column: str, period: str | timedelta, offset: str | timedelta | None = None, closed: ClosedInterval = 'right') -> Expr: Create rolling groups based on temporal or integer column. index_column: column for time window grouping (sorted ascending), period: window length, offset: window offset (default -period), closed: interval closure ('right', 'left', 'both', 'none'). Time string language supported (e.g., "1d", "3mo"). Example: pl.sum('a').rolling(index_column='dt', period='2d') Series.new_from_index(index: int, length: int) -> Self: Create Series from index. Example: s = pl.Series("a", [1, 2, 3, 4, 5]); s.new_from_index(1, 3) Expr.dt.year() -> Expr: Extract year from Date or Datetime Expr. SQLContext.unregister(names: str | Collection[str]) -> Self: Unregister eager/lazy frames by name. names: Names of tables to unregister. See also: register, register_globals, register_many. Expr.reinterpret(*, signed: bool = True) -> Expr: Reinterpret bits as signed/unsigned integer. Only for 64bit integers. signed: True=Int64, False=UInt64. LazyGroupBy.tail(n=5) -> LazyFrame: Gets the last n rows of each group. Series.arr.sort(*, descending: bool = False, nulls_last: bool = False, multithreaded: bool = True) -> Series: Sort arrays in column. Params: descending, nulls_last, multithreaded. Returns: Series. Series.list.head(n: int | Expr = 5) -> Series: Slice the first n values of every sublist. Parameter: n (number of values to return). Example: s = pl.Series("a", [[1, 2, 3, 4], [10, 2, 1]]); s.list.head(2). DataFrame.group_by_dynamic(index_column: IntoExpr, *, every: str | timedelta, period: str | timedelta | None = None, offset: str | timedelta | None = None, include_boundaries: bool = False, closed: ClosedInterval = 'left', label: Label = 'left', group_by: IntoExpr | Iterable[IntoExpr] | None = None, start_by: StartBy = 'window') -> DynamicGroupBy: Group by time windows. index_column: Column to group by. every: Window interval. period: Window length. offset: Window offset. include_boundaries: Include boundaries. closed: Closed interval type. label: Window label. group_by: Additional grouping columns. start_by: Start window strategy. Warning: index_column must be sorted. Series.bin.decode(encoding: TransferEncoding, *, strict: bool = True) -> Series. Decode binary values using encoding ('hex', 'base64'). Strict param for error handling. Expr.list.set_difference(other: IntoExpr) -> Expr. Compute the SET DIFFERENCE between the elements in this list and the elements of other. DataFrame.write_ndjson(file: IOBase | str | Path | None = None) -> str | None: Serialize to newline delimited JSON. Parameters: file: File path or writable file-like object. If None, returns string. Example: df = pl.DataFrame({"foo": [1, 2, 3], "bar": [6, 7, 8]}); df.write_ndjson() Series.ne_missing(other: Any) -> Series | Expr: Equivalent of != where None == None. other: comparison value. polars.Series.str.len_bytes() -> Series Return the length of each string as the number of bytes. Returns: Series of data type UInt32. Note: Inefficient for non-ASCII text, use len_chars() instead. polars.Expr.str.extract_many(patterns: IntoExpr, *, ascii_case_insensitive: bool = False, overlapping: bool = False) -> Expr. Extract multiple matches using Aho-Corasick algorithm. patterns to search. ascii_case_insensitive: Enable ASCII case-insensitive matching. overlapping: Allow overlapping matches. Supports string literals, not regex. polars.Expr.cumulative_eval(expr: Expr, *, min_samples: int = 1, parallel: bool = False) -> Expr Run an expression over a sliding window that increases 1 slot every iteration. expr: Expression to evaluate. min_samples: Minimum valid values. parallel: Run in parallel. Warning: Potentially slow, O(n^2) complexity. DataFrame.pipe(function: Callable, *args, **kwargs) -> T: Apply sequence of UDFs. Args: function (Callable, receives frame as first param), *args, **kwargs. Use LazyFrame for optimization. Example: df.pipe(func, col_name="b") DataFrame.estimated_size(unit: SizeUnit = 'b') -> int | float. Returns estimated total allocated size of DataFrame in given unit (bytes default). LazyGroupBy.agg(*aggs: IntoExpr | Iterable[IntoExpr], **named_aggs: IntoExpr) -> LazyFrame. Compute aggregations for groups. Params: *aggs, **named_aggs. Returns: LazyFrame. LazyFrame.rename(mapping: dict[str, str] | Callable[[str], str], *, strict: bool = True) -> LazyFrame Rename columns. mapping: dict or function to map old names to new. strict: validate column names exist. lf.rename({"foo": "apple"}).collect() polars.time(hour: Expr | str | int | None = None, minute: Expr | str | int | None = None, second: Expr | str | int | None = None, microsecond: Expr | str | int | None = None) -> Expr: Create a Polars literal expression of type Time. Returns: Expr of type Time. Example: df.with_columns(pl.time(pl.col("hour"), pl.col("minute"))) LazyFrame.mean() -> LazyFrame: Aggregate columns to their mean value. Example: lf.mean().collect() Series.to_jax(device=None) Convert Series to Jax Array. Params: device. Returns jax.Array. Warning: unstable. Expr.le(other: Any) -> Expr: Less than or equal comparison. Expr.entropy(base=2.718281828459045, *, normalize=True): Compute entropy. Formula: -sum(pk * log(pk)). Returns Expr. Parameters: base=e, normalize=True. Expr.str.split_exact(by: IntoExpr, n: int, *, inclusive: bool = False) -> Expr: Split string by substring using n splits. Returns struct of n+1 fields. Series.list.tail(n: int | Expr = 5) -> Series: Slice the last n values of every sublist. Parameters: n: Number of values to return. Example: s.list.tail(2) Series.dot(other: Series | ArrayLike) -> int | float | None: Compute dot/inner product between two Series. Example: s.dot(s2) DataFrame.write_delta(target: str | Path | deltalake.DeltaTable, mode: Literal['error', 'append', 'overwrite', 'ignore', 'merge'] = 'error', overwrite_schema: bool | None = None, storage_options: dict[str, str] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', delta_write_options: dict[str, Any] | None = None, delta_merge_options: dict[str, Any] | None = None) -> deltalake.table.TableMerger | None: Writes DataFrame as a delta table. mode: Error, append, overwrite, ignore, or merge existing data. Raises TypeError for unsupported data types, ArrowInvalidError for uncastable types, TableNotFoundError if merge on non-existent table. LazyFrame.std(ddof: int = 1) -> LazyFrame: Aggregate columns to their standard deviation. ddof: Delta Degrees of Freedom (N - ddof). Example: lf.std().collect() polars.date(year: Expr | str | int, month: Expr | str | int, day: Expr | str | int) -> Expr. Create Date literal expression. month: 1-12, day: 1-31. Series.eq_missing(other: Any) -> Series | Expr. Equality operator where None == None. Differs from eq where nulls are propagated. Expr.pow(exponent: IntoExprColumn | int | float) -> Expr. Exponentiation operator. Result dtype depends on exponent type. polars.cum_fold(acc:IntoExpr, function:Callable, exprs:Expr, include_init:bool=False) -> Expr. Cumulative horizontal fold across columns. Returns Struct column. Series.list.concat(other: list[Series] | Series | list[Any]) -> Series Concat arrays in a List Series. other: Columns to concat. s1.list.concat(s2) Expr.is_infinite() -> Expr: Returns boolean Series indicating infinite values. Returns: Expr of data type Boolean. Example: df.select(pl.all().is_infinite()) polars.scan_csv(source: str | Path | IO[str] | IO[bytes] | bytes | list[str] | list[Path] | list[IO[str]] | list[IO[bytes]] | list[bytes], *, has_header: bool = True, separator: str = ',', comment_prefix: str | None = None, quote_char: str | None = '"', skip_rows: int = 0, skip_lines: int = 0, schema: SchemaDict | None = None, schema_overrides: SchemaDict | Sequence[PolarsDataType] | None = None, null_values: str | Sequence[str] | dict[str, str] | None = None, missing_utf8_is_empty_string: bool = False, ignore_errors: bool = False, cache: bool = True, with_column_names: Callable[[list[str]], list[str]] | None = None, infer_schema: bool = True, infer_schema_length: int | None = 100, n_rows: int | None = None, encoding: CsvEncoding = 'utf8', low_memory: bool = False, rechunk: bool = False, skip_rows_after_header: int = 0, row_index_name: str | None = None, row_index_offset: int = 0, try_parse_dates: bool = False, eol_char: str = '\n', new_columns: Sequence[str] | None = None, raise_if_empty: bool = True, truncate_ragged_lines: bool = False, decimal_comma: bool = False, glob: bool = True, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, file_cache_ttl: int | None = None, include_file_paths: str | None = None) -> LazyFrame: Lazily read from CSV file(s). Parameters: source: Path(s), has_header: bool, separator: str, schema: SchemaDict, null_values: str/list[str]/dict[str, str], infer_schema: bool, n_rows: int, encoding: str, ignore_errors: bool. Returns: LazyFrame. Example: pl.scan_csv("my_long_file.csv").select(["a", "c"]).filter(pl.col("a") > 10).head(100) Series.log1p() -> Series: Computes natural logarithm of (array + 1) element-wise. Series.rolling_max(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series: Applies rolling max. window_size: window length. weights: optional weights. min_samples: min non-null values (default window_size). center: center labels. LazyFrame.sink_parquet(path: str | Path, *, compression: str = 'zstd', compression_level: int | None = None, statistics: bool | str | dict[str, bool] = True, row_group_size: int | None = None, data_page_size: int | None = None, maintain_order: bool = True, type_coercion: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, collapse_joins: bool = True, no_optimization: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> None. Evaluate query in streaming mode and write to Parquet file. Parameters: path: File path. compression: compression algorithm. compression_level: compression level. statistics: write statistics. row_group_size: row group size. data_page_size: data page size. maintain_order: maintain order. type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, collapse_joins, no_optimization: optimizations. storage_options: cloud storage options. credential_provider: credential provider function. retries: retries on cloud failure. Warning: Streaming mode unstable. Expr.str.contains_any(patterns: IntoExpr, *, ascii_case_insensitive: bool = False) -> Expr: Check if any patterns are contained in the string (Aho-Corasick). patterns: string patterns. ascii_case_insensitive: ASCII case-insensitive matching. Series.dt.hour() -> Series: Extract hour (0-23) from Datetime Series. Expr.dt.replace(*, year: int | Expr | None = None, month: int | Expr | None = None, day: int | Expr | None = None, hour: int | Expr | None = None, minute: int | Expr | None = None, second: int | Expr | None = None, microsecond: int | Expr | None = None, ambiguous: Ambiguous | Expr = 'raise') -> Expr: Replace time unit (year, month, day, hour, minute, second, microsecond). Ambiguous: 'raise', 'earliest', 'latest', 'null'. Returns Expr of Date/Datetime. Example: df.with_columns(pl.col("date").dt.replace(day="new_day").alias("replaced")) polars.time_range(start: time | IntoExprColumn | None = None, end: time | IntoExprColumn | None = None, interval: str | timedelta = '1h', closed: ClosedInterval = 'both', eager: bool = False) -> Series | Expr: Generate time range. interval examples: '1ns', '1us', '1ms', '1s', '1m', '1h', '1d', '1w', '1mo', '1q', '1y'. LazyFrame.bottom_k(k: int, *, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool] = False) -> LazyFrame: Return the k smallest rows. by: Column(s) to determine bottom rows. reverse: Consider k largest elements. Not guaranteed to be in order. Example: lf.bottom_k(4, by="b").collect() polars.Series.is_not_null() -> Series[bool] Returns a boolean Series indicating which values are not null. Example: s = pl.Series("a", [1.0, 2.0, 3.0, None]) s.is_not_null() Expr.str.tail(n: int | IntoExprColumn) -> Expr. Return last n chars of string. Params: n. Returns: Expr. Supports negative indexing. Series.rolling_sum(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series: Apply rolling sum. window_size: Window length. weights: Optional weights. min_samples: Min non-null samples. center: Center labels. Series.str.starts_with(prefix: str | Expr) -> Series Checks if string values start with a substring. LazyFrame.fill_nan(value: int | float | Expr | None) -> LazyFrame: Fill floating point NaN values. Args: value (replacement). Warning: NaN != null, use fill_null for nulls. Example: lf.fill_nan(99).collect() Expr.qcut(quantiles: Sequence[float] | int, labels: Sequence[str] | None = None, left_closed: bool = False, allow_duplicates: bool = False, include_breaks: bool = False) -> Expr: Bin values into categories based on quantiles. Parameters: quantiles (probabilities or number of bins), labels, left_closed, allow_duplicates, include_breaks (changes output to Struct). Returns: Expr (Categorical or Struct). Unstable. Expr.str.to_time(format: str | None = None, *, strict: bool = True, cache: bool = True) -> Expr. Converts String column to Time. format: str format, inferred if None. strict: error if conversion fails. cache: use cache for conversion. corr(a: IntoExpr, b: IntoExpr, method: CorrelationMethod='pearson', propagate_nans: bool=False) -> Expr Compute Pearson's or Spearman correlation between two columns. DataFrame.map_rows(function: Callable[[tuple[Any, ...]], Any], return_dtype: PolarsDataType|None=None, inference_size: int=256) -> DataFrame: Apply a UDF row-wise. Slower than native expressions. Use native expressions when possible. LazyFrame.drop_nulls(subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None) -> LazyFrame. Drops rows containing null values. Parameters: subset (column names to consider, default None uses all). Example: lf.drop_nulls(subset=["foo", "bar"]).collect() Series.str.to_date(format: str | None = None, *, strict: bool = True, exact: bool = True, cache: bool = True) -> Series: Convert a String column into a Date column. format is the format string. strict raises error on failure. exact requires exact format match. Example: s.str.to_date() Series.dt.total_minutes() -> Series[Int64]. Extract total minutes from Duration type. Expr.rle() -> Expr: Compresses data using run-length encoding. Returns a Struct with 'len' (UInt32) and 'value' (original type) fields. polars.Expr.dt.nanosecond() -> Expr: Extract nanoseconds from DateTime. Returns Expr[Int32]. DataFrame.height -> int. Get number of rows. Series.dt.truncate(every) -> Series: Divides date/datetime range into buckets using specified interval. Returns Series of Date or Datetime. every: string (e.g., '1h', '1d', '3d12h'). DataFrame.to_dummies(columns: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, *, separator: str = '_', drop_first: bool = False) -> DataFrame: Convert categorical variables to dummy/indicator variables. columns: Columns to convert, if None, convert all. separator: Separator for new column names. drop_first: Remove the first category. Example: df = pl.DataFrame({"foo": [1, 2], "bar": ["a", "b"]}); df.to_dummies() Expr.str.escape_regex() -> Expr Escape regex meta characters in string values. Expr.list.shift(n: int | IntoExprColumn = 1) -> Expr Shift list values by n indices. n: shift forward, negative shifts backward. pl.col("a").list.shift() Series.struct.json_encode() -> Series Convert struct Series to string Series with json values. polars.ones(n: int | Expr, dtype: PolarsDataType = Float64, *, eager: bool = False) -> Expr | Series: Construct a column of length n filled with ones. n: Length. dtype: Data type. eager: Evaluate immediately. Returns: Expr or Series. Expr.rolling_var_by(by: IntoExpr, window_size: timedelta | str, *, min_samples: int = 1, closed: ClosedInterval = 'right', ddof: int = 1) -> Expr: Compute rolling variance based on another column. by: DateTime, Date, UInt64, UInt32, Int64, or Int32 data type. window_size: Length of the window (timedelta or string like '1ns', '1d', '1mo'). min_samples: Minimum non-null values. closed: {'left', 'right', 'both', 'none'}. ddof: Delta Degrees of Freedom. Unstable API. Series.list.contains(item: float | str | bool | int | date | datetime | time | IntoExprColumn) -> Series. Checks if sublists contain item. Series.str.to_integer(base: int = 10, strict: bool = True) -> Series Convert String column to Int64. base: radix; strict: raise error or return Null on failure. polars.head(column: str, n: int = 10) -> Expr: Get first n rows of column. Parameters: column: Column name. n: Number of rows. Example: df.select(pl.head("a", 2)) Expr.str.find(pattern: str | Expr, literal: bool = False, strict: bool = True) -> Expr: Return the bytes offset of first substring matching pattern, returns None if not found. pattern: regex pattern. literal: Treat pattern as literal string. strict: Raise error for invalid regex, else mask with null. df.select(pl.col("txt").str.find("a|e").alias("a|e (regex)")) Expr.forward_fill(limit: int | None = None) -> Expr. Fill null values with last non-null value. Params: limit consecutive nulls to fill. Series.list.to_array(width: int) -> Series: Convert a List column to an Array column. width: Width of resulting Array. Returns Series of Array type. Example: s = pl.Series([[1, 2], [3, 4]], dtype=pl.List(pl.Int8)); s.list.to_array(2) Expr.alias(name) Rename expression. name: New name (str). Returns Expr. Series.list.all() -> Series: Evaluates if all boolean values in a list are true. Returns: Series of boolean type. Series.arccos() -> Series. Compute element-wise inverse cosine. Config.set_fmt_str_lengths(n: int | None) -> type[Config]: Set number of characters to display for string values. n: Number of characters. with pl.Config(fmt_str_lengths=50): print(df) Expr.str.to_decimal(*, inference_length: int = 100) -> Expr. Convert String column into a Decimal column. Infers precision and scale. Returns Expr. DataFrame.gather_every(n: int, offset: int = 0) -> DataFrame: Take every nth row as new DataFrame. Params: n (int), offset (int). Example: df = pl.DataFrame({"a": [1, 2, 3, 4], "b": [5, 6, 7, 8]}); df.gather_every(2) Expr.list.get(index: int | Expr | str, *, null_on_oob: bool = False) -> Expr. Get value by index in sublists. null_on_oob: if index out of bounds, return null or raise error. Expr.reshape(dimensions: tuple[int, ...]) -> Expr: Reshape this Expr to a flat column or an Array column. Parameters: dimensions (tuple of dimension sizes, -1 infers dimension). Returns: Expr (original type if single dimension, Array if multiple). See also: Expr.list.explode. Example: df.select(pl.col("foo").reshape((3, 3))) Series.ceil() -> Series: Rounds up to the nearest integer value. Only works on floating point Series. Example: s = pl.Series("a", [1.12345, 2.56789, 3.901234]); s.ceil() # returns [2.0, 3.0, 4.0] Series.dt.with_time_unit(time_unit: TimeUnit) -> Series. Set time unit for Datetime/Duration Series. Deprecated: cast to Int64 then cast back. Example: s.dt.with_time_unit("us") polars.sql_expr(sql: str | Sequence[str]) -> Expr | list[Expr]. Parse SQL expressions to Polars Exprs. Example: pl.sql_expr("MAX(a)") Expr.get(index: int | Expr) -> Expr: Get single value by index. Returns expression of same data type. Expr.index_of(element: IntoExpr) -> Expr. Get index of first occurrence. Returns None if not found. Expr.rolling_quantile_by(by: IntoExpr, window_size: timedelta | str, *, quantile: float, interpolation: RollingInterpolationMethod = 'nearest', min_samples: int = 1, closed: ClosedInterval = 'right') -> Expr Compute rolling quantile based on another column. Parameters: by (DateTime, Date, UInt64, UInt32, Int64, Int32), window_size (timedelta|str), quantile (float 0.0-1.0), interpolation (str), min_samples (int), closed (str 'left', 'right', 'both', 'none'). Unstable. Series.str.len_chars() -> Series: Return the length of each string as the number of characters (UInt32). See also: len_bytes (faster for ASCII). Example: s = pl.Series(["CafÃ©", "345", "ä¸œäº¬", None]); s.str.len_chars() Series.rank(method: RankMethod = 'average', *, descending: bool = False, seed: int | None = None) -> Series: Assign ranks to data. 'method' can be 'average', 'min', 'max', 'dense', 'ordinal', 'random'. 'descending' for descending order. 'seed' for random seed. Example: s.rank("ordinal") Series.reinterpret(*, signed: bool = True) -> Series: Reinterpret underlying bits as signed/unsigned integer (64bit integers only). Parameters: signed (bool) - If True, reinterpret as pl.Int64, else pl.UInt64. Returns: Series. Example: s.reinterpret(signed=False) Expr.dt.second(*, fractional: bool = False) -> Expr. Extract seconds from DateTime. Param: fractional: include fractional component. Returns: Expr[Int8 | Float64]. Series.value_counts(*, sort: bool = False, parallel: bool = False, name: str | None = None, normalize: bool = False) -> DataFrame: Count unique value occurrences. Parameters: sort (sort by count descending), parallel (enable parallel computation), name (resulting count column name), normalize (return proportions instead of counts. Returns: DataFrame of unique values and counts. Series.dt.millisecond() -> Series: Extract milliseconds from DateTime column. Returns Int32 Series. Series.rolling_map(function: Callable[[Series], Any], window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series. Compute custom rolling window function. Parameters: function: Custom aggregation function. window_size: window length. weights: optional window weights. min_samples: min non-null values. center: center labels. Warning: Slow, use specialized rolling functions if possible. Expr.eq(other: Any) -> Expr Equality operator (==). Parameters: other (Any). Returns: Expr. Example: pl.col("x").eq(pl.col("y")) Expr.list.any() -> Expr: Check if any boolean in list is true. LazyGroupBy.sum() -> LazyFrame. Reduce groups to sum. Returns: LazyFrame with sum aggregations. polars.from_dicts(data: Iterable[dict[str, Any]], schema: SchemaDefinition | None = None, schema_overrides: SchemaDict | None = None, strict: bool = True, infer_schema_length: int | None = 100) -> DataFrame: Construct DataFrame from sequence of dictionaries (clones data). schema: dict, list of names, or list of (name, type). schema_overrides: override inferred types. strict: throw error if data doesn't match type (else cast or set to null). infer_schema_length: rows to scan for inference. Example: pl.from_dicts(data) DataFrame.to_dict(as_series=True) -> dict: Convert DataFrame to dictionary. as_series: True -> Series, False -> list. Expr.ewm_std(*, com: float | None = None, span: float | None = None, half_life: float | None = None, alpha: float | None = None, adjust: bool = True, bias: bool = False, min_samples: int = 1, ignore_nulls: bool = False) -> Expr: Compute exponentially-weighted moving standard deviation. polars.DataFrame.reverse() -> DataFrame Reverse the DataFrame. DataFrame.iter_slices(n_rows: int = 10000) -> Iterator[DataFrame]. Returns iterator of DataFrame slices. Parameters: n_rows. See also: iter_rows, partition_by. Series.cat.ends_with(suffix: str) -> Series. Check if string values end with substring. Parameters: suffix: Substring. Expr.str.len_chars(): Return string length in characters. Returns Expr[UInt32]. See also len_bytes. Note: len_bytes faster for ASCII. LazyFrame.sql(query: str, *, table_name: str = 'self') -> LazyFrame: Executes SQL query against LazyFrame. query: SQL query. table_name: name for the LazyFrame in SQL context (default 'self'). polars.zeros(n: int | Expr, dtype: PolarsDataType = Float64, *, eager: bool = False) -> Expr | Series. Construct column of length n filled with zeros. Returns Expr or Series. Series.dt.datetime() -> Series[Datetime]: Extract local datetime. Deprecated: use dt.replace_time_zone(None). Expr.dt.epoch(time_unit: EpochTimeUnit = 'us') -> Expr: Get time passed since Unix EPOCH. time_unit: {'ns', 'us', 'ms', 's', 'd'}. df.with_columns(pl.col("date").dt.epoch().alias("epoch_ns")) Series.arr.contains(item: float | str | bool | int | date | datetime | time | IntoExprColumn) -> Series[bool]. Check if sub-arrays contain item. Returns boolean Series. DataFrame.glimpse(*, max_items_per_column: int = 10, max_colname_length: int = 50, return_as_string: bool = False) -> str | None: Returns a dense preview of the DataFrame (one line per column: name, type, and first few values). max_items_per_column controls the number of items shown per column. max_colname_length controls the max column name length. return_as_string returns string if set to True. DataFrame.collect_schema() -> Schema: Get ordered mapping of column names to data type. Alias for schema property. Facilitates generic code for DataFrame/LazyFrame. DataFrame.bottom_k(k: int, *, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool] = False) -> DataFrame. Return the k smallest rows. DataFrame.write_ipc_stream(file: str | Path | IO[bytes] | None, *, compression: IpcCompression = 'uncompressed', compat_level: CompatLevel | None = None) -> BytesIO | None. Write to Arrow IPC stream. file=None returns BytesIO. compression options: uncompressed, lz4, zstd. Expr.not_() -> Expr: Negate boolean expression. Series.rolling_skew(window_size: int, *, bias: bool = True) -> Series: Compute rolling skew. Parameters: window_size, bias (correct for statistical bias). Unstable API. Example: pl.Series([1, 4, 2, 9]).rolling_skew(3) Expr.unique_counts() -> Expr: Return count of unique values in order of appearance. Returns counts only (faster than value_counts). Example: df.select(pl.col("id").unique_counts()) DataFrame.with_row_count(name='row_nr', offset=0) -> DataFrame: Add a row count column. Deprecated, use with_row_index. DataFrame.sum_horizontal(*, ignore_nulls: bool = True) -> Series: Sums values horizontally across columns. ignore_nulls: Ignore nulls (default). Returns Series named "sum". polars.first(*columns: str) -> Expr: Get first column or value. No columns: returns first column expression; otherwise, first value(s) of given column(s). polars.min(*names: str) -> Expr Syntactic sugar for col(names).min(). Get minimum value of column(s). Parameters: *names (column names). See Also: min_horizontal. Example: df.select(pl.min("a")) returns the minimum of column "a". Expr.dt.hour() -> Expr: Extracts hour (0-23) from Datetime. Returns Int8 Expr. Expr.arr.median() -> Expr Compute median of values in sub-arrays. DataFrame.write_parquet(file: str | Path | IO[bytes], compression: ParquetCompression = 'zstd', compression_level: int | None = None, statistics: bool | str | dict[str, bool] = True, row_group_size: int | None = None, data_page_size: int | None = None, use_pyarrow: bool = False, pyarrow_options: dict[str, Any] | None = None, partition_by: str | Sequence[str] | None = None, partition_chunk_size_bytes: int = 4294967296, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> None: Write DataFrame to Parquet file. UNSTABLE features: `partition_by`, cloud storage. polars.Config.set_tbl_cols(n: int | None) -> type[Config]: Set visible table columns. Parameters: n (int | None). Series.list.arg_min() -> Series: Index of minimal value in each sublist. Returns UInt32 or UInt64 Series. Expr.list.arg_min() -> Expr. Returns index of minimal value in each sublist. Expr.tan() -> Expr: Compute element-wise tangent. Returns: Expr of Float64. Example: df.select(pl.col("a").tan().round(2)) DataFrame.clone() -> DataFrame Create a copy of this DataFrame. (cheap operation, no data copied) GroupBy.mean() -> DataFrame: Reduce groups to mean values after DataFrame.group_by(). Series.dt.date() -> Series: Extract (local) date from Date/Datetime columns. LazyGroupBy.first() -> LazyFrame: Aggregates first values per group. Expr.arr.last() -> Expr: Get the last value of the sub-arrays. Example: df.with_columns(last=pl.col("a").arr.last()) Series.all(*, ignore_nulls: bool = True) -> bool | None: Returns True if all values are True. Works on Boolean Series. ignore_nulls: If True (default), ignore nulls. If False, use Kleene logic: returns None if any nulls and no False values. Example: pl.Series([True,True]).all() Series.sin() -> Series: Computes element-wise sine values. Expr.str.strip_prefix(prefix: IntoExpr) -> Expr. Remove prefix from string once if found. prefix: The prefix to be removed. See also: strip_chars_start, strip_suffix. Example: df.with_columns(pl.col("a").str.strip_prefix("foo").alias("stripped")) Expr.sample(n: int | IntoExprColumn | None = None, *, fraction: float | IntoExprColumn | None = None, with_replacement: bool = False, shuffle: bool = False, seed: int | None = None) -> Expr. Samples from expression. n: Number of items. fraction: Fraction of items. with_replacement: Allow sampling more than once. shuffle: Shuffle order. seed: Random seed. Expr.interpolate(method: str = 'linear') -> Expr: Fill null values using interpolation. Args: method ({"linear", "nearest"}). Example: df.select(pl.all().interpolate()) Expr.dt.month_end() -> Expr: Roll forward to the last day of the month. Returns: Expr of Date or Datetime type. LazyFrame.gather_every(n: int, offset: int = 0) -> LazyFrame. Takes every nth row. Series.len() -> int: Return the number of elements in the Series, including null values. Expr.lt(other: Any) -> Expr: Method equivalent of "less than" operator expr < other. Returns a boolean Expr. Example: pl.col("x").lt(pl.col("y")) Series.dt.cast_time_unit(time_unit: TimeUnit) -> Series: Cast underlying data to another time unit ('ns', 'us', 'ms'). May lose precision. Example: date.dt.cast_time_unit("ms") GroupBy.first() -> DataFrame: Aggregate the first values in each group. Example: df.group_by("d", maintain_order=True).first() LazyFrame.join_asof(other: LazyFrame, *, left_on: str | None | Expr = None, right_on: str | None | Expr = None, on: str | None | Expr = None, by_left: str | Sequence[str] | None = None, by_right: str | Sequence[str] | None = None, by: str | Sequence[str] | None = None, strategy: AsofJoinStrategy = 'backward', suffix: str = '_right', tolerance: str | int | float | timedelta | None = None, allow_parallel: bool = True, force_parallel: bool = False, coalesce: bool = True, allow_exact_matches: bool = True, check_sortedness: bool = True) -> LazyFrame: Performs an asof join (nearest key). Parameters: other - LazyFrame to join, left_on/right_on/on - Join columns, by - Join on these columns first, strategy - {'backward', 'forward', 'nearest'}, suffix - Column name suffix, tolerance - Numeric tolerance, allow_parallel/force_parallel - Parallel execution, coalesce - Coalesce join columns, allow_exact_matches - Allow matching with the same on value, check_sortedness - Check sortedness of asof keys. Series.clip(lower_bound: NumericLiteral | TemporalLiteral | IntoExprColumn | None = None, upper_bound: NumericLiteral | TemporalLiteral | IntoExprColumn | None = None) -> Series: Set values outside the given boundaries to the boundary value. Parameters: lower_bound, upper_bound (accepts expression input or literal). Works for numeric and temporal columns. s.clip(1, 10) Expr.list.max() -> Expr: Computes the max value of the lists in the array. Example: df.with_columns(max=pl.col("values").list.max()) polars.int_ranges(start: int | Expr = 0, end: int | Expr | None = None, step: int | Expr = 1, dtype: PolarsIntegerType = Int64, eager: bool = False) -> Expr | Series: Generate range of integers for each row. start: inclusive, end: exclusive (defaults to start, start defaults to 0), step. dtype: integer type. eager: return Series if True. Example: pl.int_ranges("start", "end") polars.read_json(source: str | Path | IOBase | bytes, *, schema: SchemaDefinition | None = None, schema_overrides: SchemaDefinition | None = None, infer_schema_length: int | None = 100) -> DataFrame: Reads JSON file into DataFrame. Supports schema definition and overrides. DataFrame.merge_sorted(other: DataFrame, key: str) -> DataFrame: Merge two sorted DataFrames by sorted key. other: DataFrame to merge. key: Sorted key. Schemas must be equal. Frames must be sorted by key. Example: df0.merge_sorted(df1, key="age") Series.exp() -> Series: Compute element-wise exponential. Series.is_first_distinct() -> Series: Boolean mask of first distinct value occurrence. Series.arr.any() -> Series Evaluate whether any boolean value is true for every subarray. Expr.str.strip_suffix(suffix: Expr) -> Expr. Remove suffix once if found. Strips exact suffix. See also: strip_chars_end, strip_prefix. Expr.list.concat(other) -> Expr: Concatenates arrays in a List Series. other: columns to concatenate. Expr.round(decimals: int = 0) -> Expr: Round floating point data by decimals digits. DataFrame.rows(*, named: bool = False) -> list[tuple[Any, ...]] | list[dict[str, Any]]: Returns all data as a list of rows (tuples or dictionaries). Parameters: named (return dictionaries instead of tuples). Warning: Row-iteration is not optimal. Consider iter_rows for better memory usage. Note: ns-precision temporal values are truncated to Âµs. Config.set_tbl_cell_alignment(format) Set table cell alignment. Params: format: Literal['LEFT', 'CENTER', 'RIGHT'] | None. Returns type[Config]. Raises: ValueError if format invalid. Series.rolling_std(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False, ddof: int = 1) -> Series Compute rolling std dev. window_size: window length. weights: optional weights. min_samples: min non-null samples. center: center labels. ddof: Delta Degrees of Freedom. Series.slice(offset:int, length:int|None=None) -> Series. Get a slice of this Series. Parameters: offset:Start index, length:Length of the slice. Returns: Series. LazyFrame.rolling(index_column: IntoExpr, *, period: str | timedelta, offset: str | timedelta | None = None, closed: ClosedInterval = 'right', group_by: IntoExpr | Iterable[IntoExpr] | None = None) -> LazyGroupBy. Create rolling groups based on a temporal or integer column. index_column: Column to group by (Date/Datetime, sorted ascending). period: Length of window. offset: Offset of window (default -period). closed: {'right','left','both','none'}. group_by: Also group by this column/these columns. Returns: LazyGroupBy, sorted by index_column (within group_by groups). Expr.str.json_decode(dtype: PolarsDataType | None = None, *, infer_schema_length: int | None = 100) -> Expr. Parses string values as JSON. dtype: cast to dtype, inferred if None. infer_schema_length: rows to scan for schema inference. Series.map_elements(function: Callable[[Any], Any], return_dtype: PolarsDataType | None = None, *, skip_nulls: bool = True) -> Self. Map UDF over Series elements. Slower than native API. Example: s.map_elements(lambda x: x + 10, return_dtype=pl.Int64) SQLContext.register_many(frames: Mapping[str, FrameType] | None = None, **named_frames: FrameType | None) Returns Self. Register multiple frames as tables. Parameters: frames: {name:frame}, **named_frames: named frames kwargs. See also register, unregister. polars.date_ranges(start: date | datetime | IntoExprColumn, end: date | datetime | IntoExprColumn, interval: str | timedelta = '1d', *, closed: ClosedInterval = 'both', eager: bool = False) -> Series | Expr. Create a column of date ranges. Parameters: start (lower bound), end (upper bound), interval (timedelta or duration string like "1d", "1w", "1mo", "1q", "1y"), closed ('both', 'left', 'right', 'none'), eager (bool). Returns: Expr or Series (List(Date)). Expr.abs() Compute absolute values. Returns Expr. Expr.list.all() -> Expr. Evaluate if all boolean values in a list are true. Example: df.with_columns(all=pl.col("a").list.all()). Expr.list.set_symmetric_difference(other: IntoExpr) -> Expr. Compute set symmetric difference between list elements with other list elements. Series.mean() -> PythonLiteral | None: Reduce Series to the mean value. Example: s = pl.Series("a", [1, 2, 3]); s.mean() Series.quantile(quantile: float, interpolation: str = 'nearest') -> float | None: Get quantile value of Series. Parameters: quantile (float): Quantile between 0.0 and 1.0; interpolation (str): Interpolation method ('nearest', 'higher', 'lower', 'midpoint', 'linear'). Expr.str.strip_chars(characters: IntoExpr = None) -> Expr: Remove leading and trailing characters. Parameters: characters. If characters is None, remove whitespace. Series.bin.ends_with(suffix: IntoExpr) -> Series: Checks if binary values end with a substring. suffix: Substring to check. polars.LazyFrame.group_by(*by: IntoExpr | Iterable[IntoExpr], maintain_order: bool = False, **named_by: IntoExpr) -> LazyGroupBy. Start a group by operation. by: Column(s) to group by. maintain_order: Keep group order consistent. Slower, blocks streaming. named_by: Additional group by columns. Example: lf.group_by("a").agg(pl.col("b").sum()).collect() Series.rle() -> Series: Compress Series using run-length encoding (RLE). Returns Struct Series: 'len'(UInt32), 'value'(original dtype). polars.coalesce(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr. Folds columns left to right, keeping first non-null value. Series.dt.timestamp(time_unit: TimeUnit = 'us') -> Series: Return timestamp in given time unit. Parameters: time_unit: 'us', 'ns', or 'ms'. Expr.str.head(n: int | IntoExprColumn) -> Expr: Returns the first n characters of each string in a String Series. n can be negative. Returns full string if n is greater than string length. Example: pl.col("s").str.head(5) polars.api.register_dataframe_namespace(name: str) -> Callable[[type[NS]], type[NS]]: Register custom DataFrame functionality. name: Access name. DataFrame.fill_nan(value: Expr | int | float | None) -> DataFrame. Replace floating point NaN values. value: Replacement value. Note: Floating point NaNs are not missing values; use fill_null() for missing values. Example: df.fill_nan(99). Expr.rolling_skew(window_size: int, *, bias: bool = True) -> Expr. Compute rolling skew. Unstable. window_size: window size, bias: correct for statistical bias. Window includes current row and window_size-1 previous. Series.entropy(base: float = 2.71828, normalize: bool = True) -> float | None. Computes entropy. Parameters: base (default e), normalize (if pk sums to 1). Series.round_sig_figs(digits: int) -> Series: Rounds to a number of significant figures. digits: Number of significant figures. Example: s.round_sig_figs(2) Expr.dt.cast_time_unit(time_unit: TimeUnit) -> Expr: Casts the underlying data to another time unit ('ns', 'us', 'ms'). May lose precision. time_unit: Time unit for the Datetime expression. Series.str.strip_chars(characters: IntoExpr = None) -> Series: Remove leading and trailing characters. `characters`: set of characters to remove, or None (default, removes whitespace). Example: s.str.strip_chars("o ") LazyFrame.var(ddof: int = 1) -> LazyFrame. Aggregate columns to variance. Parameters: ddof (delta degrees of freedom, default 1). DataFrame.is_duplicated() Get mask of duplicated rows. Returns Series[bool]. LazyFrame.collect_async(*, gevent: bool = False, type_coercion: bool = True, ..., streaming: bool = False) -> Awaitable[DataFrame] | _GeventDataFrameResult[DataFrame]. Collect DataFrame asynchronously. Params: gevent, type_coercion, ..., streaming. Returns: Awaitable[DataFrame] or GeventDataFrameResult[DataFrame]. Warning: unstable functionality, streaming mode unstable. Expr.clip(lower_bound: Numeric | Temporal | Expr | None = None, upper_bound: Numeric | Temporal | Expr | None = None) -> Expr: Clip values outside boundaries. Args: lower_bound, upper_bound. Works for numeric/temporal. Example: df.with_columns(clip=pl.col("a").clip(1, 10)) Expr.explode() Explode list expression. Item to new row. Returns Expr. Expr.dt.with_time_unit(time_unit: TimeUnit) -> Expr: Sets time unit of Datetime or Duration expression. Deprecated. time_unit: 'ns', 'us', 'ms'. Does not modify underlying data. polars.arg_sort_by(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr, descending: bool | Sequence[bool] = False, nulls_last: bool | Sequence[bool] = False, multithreaded: bool = True, maintain_order: bool = False) -> Expr. Return row indices that would sort the column(s). exprs: Column(s) to sort by. descending: Sort descending. nulls_last: Place nulls last. multithreaded: Use multiple threads. maintain_order: maintain order if elements are equal. See also: Expr.gather, Expr.rank. Example: df.select(pl.arg_sort_by("a")) Series.item(index:int=None) -> Any. Return Series as scalar or element at index. Parameters: index (optional, defaults to 0). Returns: Any. If no index, shape must be (1,). Expr.list.head(n: int | str | Expr = 5) -> Expr: Slice first n sublist values. Param: n: values to return. DataFrame.vstack(other: DataFrame, in_place: bool = False) -> DataFrame Vertically stack DataFrames. other: DataFrame to stack; in_place: modify in place. Series.str.to_decimal(inference_length: int = 100) -> Series: Convert String column to Decimal. inference_length: elements to parse for precision/scale. Expr.str.reverse() -> Expr. Returns reversed string values. LazyFrame.collect(*, type_coercion: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, comm_subplan_elim: bool = True, comm_subexpr_elim: bool = True, cluster_with_columns: bool = True, collapse_joins: bool = True, no_optimization: bool = False, streaming: bool = False, engine: EngineType = 'cpu', background: bool = False) -> DataFrame | InProcessQuery. Materialize LazyFrame to DataFrame. Query optimizations enabled by default. engine: 'cpu' or 'gpu'. streaming/background: unstable. Series.str.slice(offset: int | IntoExprColumn, length: int | IntoExprColumn | None = None) -> Series: Extract substring from each string. Parameters: offset (start index, negative indexing), length (slice length, None=end). Returns: Series of type String. Example: s.str.slice(-3) Expr.dt.convert_time_zone(time_zone: str) -> Expr: Convert Datetime to given time_zone. If converting from a time-zone-naive datetime, conversion happens as if converting from UTC. Example: df.select(pl.col("date").dt.convert_time_zone(time_zone="Europe/London")) polars.Series.list.set_intersection(other: Series) -> Series Compute SET INTERSECTION between elements in this list and elements of other. Parameters: other (Series). Example: a.list.set_intersection(b) returns a Series with the intersection of lists in a and b. Expr.quantile(quantile: float | Expr, interpolation: RollingInterpolationMethod = 'nearest') -> Expr: Get quantile value. Parameters: quantile (0.0 to 1.0), interpolation ('nearest', 'higher', 'lower', 'midpoint', 'linear'). LazyFrame.deserialize(source: str | Path | IOBase, *, format: SerializationFormat = 'binary') -> LazyFrame: Read logical plan from file to LazyFrame. Format: 'binary', 'json'. Warning: Uses pickle for Python UDFs, security risk. Not stable across Polars versions. Expr.unique(maintain_order: bool = False) -> Expr: Get unique values. Parameters: maintain_order (default False, requires more work). Expr.sort(descending: bool=False, nulls_last: bool=False) -> Expr: Sort column or groups. descending: Sort descending. nulls_last: Nulls last. Series.arr.min() -> Series Compute min values of sub-arrays. Config.set_tbl_formatting(format: Literal['ASCII_FULL', 'ASCII_FULL_CONDENSED', 'ASCII_NO_BORDERS', 'ASCII_BORDERS_ONLY', 'ASCII_BORDERS_ONLY_CONDENSED', 'ASCII_HORIZONTAL_ONLY', 'ASCII_MARKDOWN', 'MARKDOWN', 'UTF8_FULL', 'UTF8_FULL_CONDENSED', 'UTF8_NO_BORDERS', 'UTF8_BORDERS_ONLY', 'UTF8_HORIZONTAL_ONLY', 'NOTHING'] | None, rounded_corners: bool | None) -> Config: Set table formatting style. Raises ValueError if format unrecognised. Python API Introduction SQL interface entry points: SQLContext, polars.sql(), DataFrame.sql(), LazyFrame.sql(), polars.sql_expr(). Querying: Global SQL polars.sql(): execute SQL against DataFrame, LazyFrame, Series, Pandas DataFrame/Series, PyArrow Table/RecordBatch. Frame SQL DataFrame.sql()/LazyFrame.sql(): execute SQL against frame ("self"). Expression SQL polars.sql_expr(): create Polars expressions from SQL fragments. Class SQLContext Run SQL queries against DataFrame/LazyFrame. SQLContext.__init__(frames=None, register_globals=False, eager=False, **named_frames): initialize SQLContext. Parameters: frames, register_globals, eager, named_frames. SQLContext.__enter__(): track registered tables. SQLContext.__exit__(exc_type, exc_val, exc_tb): unregister tables on exit. SQLContext.execute(query, *, eager): execute SQL query. SQLContext.execute_global(query, *, eager): execute SQL, auto-register globals. SQLContext.register(name, frame): register frame as table. SQLContext.register_globals([n, all_compatible]): register global frames. SQLContext.register_many([frames]): register frames as tables. SQLContext.tables(): return table names. SQLContext.unregister(names): unregister tables. Expr.struct.with_fields(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> Expr[source]. Add or overwrite fields of struct. Parameters: *exprs, **named_exprs. Example: df.with_columns(pl.col("coords").struct.with_fields(pl.field("x").sqrt(), y_mul=pl.field("y") * pl.col("multiply"))). DataFrame.count() Return number of non-null elements per column. Returns DataFrame. DataFrame.shift(n: int=1, fill_value: IntoExpr|None=None) -> DataFrame: Shift values by n indices. n>0: forward, n<0: backward. fill_value fills nulls. LazyFrame.drop_nans(subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None) -> LazyFrame. Drop rows with NaN values. Params: subset columns to consider. polars.Expr.str.encode(encoding: TransferEncoding) -> Expr: Encode values using encoding (hex, base64). Returns Expr[String]. Series.alias(name: str) -> Series: Rename Series. Expr.list.last() -> Expr: Get the last value of the sublists. Returns: Expr. If list is empty returns null. DataFrame.unpivot(on: ColumnNameOrSelector|Sequence[ColumnNameOrSelector]|None=None, index: ColumnNameOrSelector|Sequence[ColumnNameOrSelector]|None=None, variable_name: str|None=None, value_name: str|None=None) -> DataFrame: Unpivot DataFrame from wide to long format. Similar to pandas.DataFrame.melt. Series.implode() -> Self. Aggregate values into a list. Expr.str.replace_all(pattern: str | Expr, value: str | Expr, *, literal: bool = False) -> Expr. Replace all matching regex/literal substrings with a new string value. Returns Expr. SQLContext.register(name: str, frame: CompatibleFrameType | None) -> Self: Register a DataFrame as a SQL table. Series.dt.total_microseconds() -> Series. Extract total microseconds from a Duration type. Returns Series of Int64. Series.str.strip_chars_start(characters) -> Series: Removes leading characters. characters: string or None (removes whitespace). polars.Series.list.std(ddof: int = 1) -> Series: Computes the standard deviation of arrays in a list. Returns a Series of floats. polars.sum(*names: str) -> Expr: Sum all values. Syntactic sugar for col(name).sum(). GroupBy.max() -> DataFrame: Reduce groups to maximal value. Example: df.group_by("d", maintain_order=True).max() Expr.ewm_mean(com, span, half_life, alpha, adjust=True, min_samples=1, ignore_nulls=False) -> Expr: Computes exponentially-weighted moving average. Requires one of com, span, half_life, or alpha. adjust divides by a decaying factor. min_samples is minimum non-null values. ignore_nulls affects weight calculation. Expr.and_(*others: Any) -> Expr Bitwise "and" operator. others: Integer or boolean expressions. (pl.col("x") >= pl.col("z")).and_(pl.col("y") >= pl.col("z")) Series.str.to_datetime(format: str | None = None, time_unit: TimeUnit | None = None, time_zone: str | None = None, strict: bool = True, exact: bool = True, cache: bool = True, ambiguous: Ambiguous | Series = 'raise') -> Series: Convert String Series to Datetime Series. Series.str.escape_regex() -> Series. Escape regex meta characters in string values. Returns String Series. Series.shuffle(seed: int | None = None) -> Series. Shuffle Series contents. seed: Random seed. classmethod SQLContext.execute_global(query: str, *, eager: bool = False) -> DataFrame | LazyFrame. Execute SQL query with global frame registration. Parameters: query: SQL string, eager: return DataFrame if True. Expr.list.n_unique() -> Expr: Count the number of unique values in every sub-list. Example: df = pl.DataFrame({"a": [[1, 1, 2], [2, 3, 4]]}); df.with_columns(pl.col("a").list.n_unique()) Expr.str.extract(pattern: IntoExprColumn, group_index: int = 1) -> Expr: Extracts capture group from regex pattern. pattern: regex. group_index: capture group index (1 is first). Returns: String Expr, null if no capture. Expr.shuffle(seed=None) -> Expr: Shuffle expression contents. Note: shuffled independently. Seed: random number generator seed. Example: df.select(pl.col("a").shuffle(seed=1)) polars.any(*names: str, ignore_nulls: bool = True) -> Expr | bool | None: Performs a bitwise OR operation on columns. ignore_nulls: handles nulls. Series.dt.offset_by(by: str | Expr) -> Series: Offset date by relative time offset (e.g., "1y", "1mo"). Parameters: by - offset string (1ns, 1us, 1ms, 1s, 1m, 1h, 1d, 1w, 1mo, 1q, 1y). Returns: Series of Date or Datetime type. Only single minus sign allowed. Example: dates.dt.offset_by("1y") Series.ewm_std(*, com: float | None = None, span: float | None = None, half_life: float | None = None, alpha: float | None = None, adjust: bool = True, bias: bool = False, min_samples: int = 1, ignore_nulls: bool = False) -> Series: Compute exponentially-weighted moving standard deviation. com: Center of mass. span: Span. half_life: Half-life. alpha: Smoothing factor. adjust: Adjust for imbalance. bias: Unbiased estimate correction. min_samples: Min observations. ignore_nulls: Ignore missing values. Example: s.ewm_std(com=1, ignore_nulls=False) Series.cumulative_eval(expr: Expr, *, min_samples: int = 1, parallel: bool = False) -> Series: Run expression over increasing sliding window. Parameters: expr: expression, min_samples, parallel. Warning: potentially slow O(n^2). Series.dt.month_start() -> Series: Roll backward to first day of month. Returns: Date or Datetime Series. Analogous to pandas.tseries.offsets.MonthBegin().rollback(datetime). Example: s.dt.month_start() Config.set_verbose(active: bool = True) -> Config: Enables verbose logging. Series.arr.explode() -> Series. Explode array elements to separate rows. Returns Series of array element type. Example: s.arr.explode() Expr.arr.all() -> Expr: Evaluates if all boolean values are true for every subarray. Expr.ceil() -> Expr. Rounds to nearest integer value. Works on float Series. polars.DataFrame.to_dicts() -> list[dict[str, Any]] Convert every row to a dictionary. Note: ns-precision temporal values are truncated to microseconds. Expr.arr.to_list() -> Expr: Convert Array column to List column. Expr.is_null() -> Expr: Returns a boolean Series indicating which values are null. Expr.str.ends_with(suffix: str | Expr) -> Expr: Check if string values end with substring suffix. Returns boolean Expr, null if original value is null. See contains, starts_with. Series.list.set_difference(other: Series) -> Series Compute set difference between lists. a = pl.Series([[1, 2, 3]]); b = pl.Series([[2, 3, 4]]); a.list.set_difference(b) Expr.name.suffix(suffix: str) -> Expr: Add a suffix to the root column name. Parameters: suffix. Undoes previous renaming. Only one name operation per chain. Use .name.map for advanced renaming. Series.str.pad_start(length: int, fill_char: str = ' ') -> Series Pad string start to length. length: target length; fill_char: padding character. Series.arr.to_struct(fields: Callable[[int], str] | Sequence[str] | None = None) -> Series[Struct]: Convert Array Series to Struct Series. fields to name struct fields. Expr.str.decode(encoding: TransferEncoding, *, strict: bool = True) -> Expr. Decode string values. encoding: 'hex' or 'base64'. strict: Raise error if decoding fails, else mask with null. Returns: Expr of Binary type. Example: df.with_columns(pl.col("color").str.decode("hex").alias("decoded")). Expr.arr.get(index: int | IntoExprColumn, null_on_oob: bool = False) -> Expr: Gets value by index in sub-arrays. null_on_oob: True=null, False=error for out-of-bounds index. GroupBy.map_groups(function: Callable[[DataFrame], DataFrame]) -> DataFrame. Apply custom function over groups as sub-DataFrame. Warning: slower than native API. Expr.rolling_std_by(by: IntoExpr, window_size: timedelta | str, min_samples: int = 1, closed: ClosedInterval = 'right', ddof: int = 1) -> Expr: Computes a rolling standard deviation based on another column. Warning: Unstable functionality. Expr.cat.len_chars() -> Expr[UInt32]. Returns character length of string representation. Notes: use len_bytes for ASCII for better performance. Extending the API: register_expr_namespace(name), register_dataframe_namespace(name), register_lazyframe_namespace(name), register_series_namespace(name). Decorators for custom functionality. Cannot override existing Polars namespaces (str, dt). Example: @pl.api.register_expr_namespace("greetings") class Greetings: def __init__(self, expr: pl.Expr): self._expr = expr def hello(self) -> pl.Expr: return (pl.lit("Hello ") + self._expr).alias("hi there") Series.to_frame(name: str | None = None) -> DataFrame: Casts the Series to a DataFrame. name: Optionally rename the Series column. GroupBy.head(n: int = 5) -> DataFrame: Get first n rows of each group. polars.Expr.bin.decode(encoding: TransferEncoding, *, strict: bool = True) -> Expr: Decode binary values using encoding. Parameters: encoding ('hex'|'base64'), strict (bool). Returns Expr of String. Series.to_init_repr(n: int = 1000) -> str: Convert Series to instantiable string representation. DataFrame.width: int. Get the number of columns. Returns: int. Example: df = pl.DataFrame({"foo": [1, 2, 3], "bar": [4, 5, 6]}); df.width # 2 Series.interpolate(method: InterpolationMethod = 'linear') -> Series: Fill null values using interpolation. method: 'linear', 'nearest'. Expr.any(ignore_nulls: bool = True) -> Expr: Checks if any value is True. Works on Boolean columns. ignore_nulls: Ignores nulls (default True); False uses Kleene logic. polars.from_dataframe(df: SupportsInterchange, *, allow_copy: bool = True) -> DataFrame: Create DataFrame from interchange protocol. Parameters: df: Object supporting dataframe interchange; allow_copy (bool): Allow memory copy. Returns: DataFrame. DataFrame.extend(other: DataFrame) -> DataFrame. Extend DataFrame memory with values from other. Modifies DataFrame in-place. Series.bin.contains(literal: IntoExpr) -> Series[Boolean]: Check if binaries contain binary substring. Series.dt.millennium() -> Series Extract millennium from Date/Datetime. Returns millennium as Int32. LazyFrame.quantile(quantile: float | Expr, interpolation: RollingInterpolationMethod = 'nearest') -> LazyFrame: Compute quantile aggregation on LazyFrame columns. Series.is_nan() -> Series: Returns boolean Series indicating NaN values. Returns: Series of Boolean type. Example: s.is_nan() Expr.dt.strftime(format: str) -> Expr Convert Date/Time/Datetime to String with format. Alias for to_string(). Format: chrono strftime spec. df = pl.DataFrame({'datetime': [datetime(2020, 3, 1)]}); df.with_columns(pl.col("datetime").dt.strftime("%Y/%m/%d").alias("datetime_string")) Series.has_nulls() -> bool: Check if Series contains null values. Expr.str.find_many(patterns: IntoExpr, *, ascii_case_insensitive: bool = False, overlapping: bool = False) -> Expr: Find multiple string matches using Aho-Corasick. Returns List of byte offsets. Parameters: patterns (string patterns to search), ascii_case_insensitive (enable ASCII case-insensitive matching), overlapping (allow overlapping matches). Note: Supports string literals only, not regex. DataFrame.shape -> tuple[int, int]: Get the shape of the DataFrame (rows, cols). Example: df.shape polars.scan_parquet(source: FileSource, *, n_rows: int | None = None, row_index_name: str | None = None, row_index_offset: int = 0, parallel: ParallelStrategy = 'auto', use_statistics: bool = True, hive_partitioning: bool | None = None, glob: bool = True, schema: SchemaDict | None = None, hive_schema: SchemaDict | None = None, try_parse_hive_dates: bool = True, rechunk: bool = False, low_memory: bool = False, cache: bool = True, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, include_file_paths: str | None = None, allow_missing_columns: bool = False) -> LazyFrame. Lazily read parquet file(s). Optimizer pushdown. source: file path, n_rows, row_index_name, row_index_offset, parallel: parallelism strategy, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, cache, storage_options, credential_provider, retries, include_file_paths, allow_missing_columns. DataFrame.to_struct(name: str = '') -> Series Convert a DataFrame to a Series of type Struct. Series.extend_constant(value: IntoExpr, n: int | IntoExprColumn) -> Series: Extend Series with n copies of value. value can be None for nulls. Example: s.extend_constant(99, n=2) DataFrame.slice(offset: int, length: int | None = None) -> DataFrame: Gets slice of DataFrame. offset: Start index, length: Length of slice. Example: df.slice(1, 2) BatchedCsvReader.next_batches(n: int) -> list[DataFrame] | None: Read n batches. Batches parallelized over threads. Returns list of DataFrames. Series.dt.total_nanoseconds() -> Series: Extract total nanoseconds from a Duration type as Int64. Example: date.diff().dt.total_nanoseconds() Expr.dt.total_hours() Extract total hours from Duration type. Returns Expr[Int64]. Series.scatter(indices:Series|Iterable[int]|int|np.ndarray,values:Series|Iterable|PythonLiteral|None) -> Series: Set values at indices. Anti-pattern; use pl.when().then().otherwise() instead. polars.Expr.truediv(other: Any) -> Expr: Float division (expr / other). other: numeric literal or expression. Follows IEEE-754 for zero division. GroupBy.count() -> DataFrame: Returns the number of rows in each group. Deprecated: Use len() polars.Expr.name.prefix_fields(prefix: str) -> Expr Add prefix to all field names of a struct. Parameters: prefix. See Also: map_fields, suffix_fields. Notes: Only affects struct columns. Example: df.select(pl.col("x").name.prefix_fields("prefix_")).schema shows prefixed field names inside the struct. Series.to_torch() -> torch.Tensor. Converts Series to PyTorch Tensor. Warning: unstable. Notes: UInt16, UInt32, UInt64 cast to Int32, Int64, Int64. DataFrame.is_unique() -> Series[bool]: Returns a boolean Series masking rows that are unique. df.filter(df.is_unique()) shows unique rows. Series.fill_nan(value: int | float | Expr | None) -> Series. Fill NaN values. Use fill_null() for missing values. polars.datatypes.List(inner: PolarsDataType | PythonDataType): Variable length list type. List.__init__(inner) Expr.list.reverse() -> Expr: Reverse arrays in list. Expr.fill_nan(value: int | float | Expr | None) -> Expr: Fill floating point NaN values. Parameter: value (used to fill NaN). Warning: NaNs are not missing values; use fill_null() for missing values. See also: fill_null. Example: df.with_columns(pl.col("b").fill_nan(0)). Series.dt.month_end() -> Series: Roll forward to the last day of the month (Date or Datetime). Series.drop_nulls() -> Series. Drop all null values, preserving order. See also: drop_nans. Notes: Null != NaN. Use drop_nans() for NaN. Expr.log1p() -> Expr[source]. Compute natural logarithm of each element plus one. More numerically stable for x close to zero than log(1 + x). Example: df.select(pl.col("a").log1p()). polars.Series.is_null() -> Series Returns a boolean Series indicating which values are null. Returns: Series (Boolean). DataFrame.row(index: int | None = None, *, by_predicate: Expr | None = None, named: bool = False) -> tuple[Any, ...] | dict[str, Any]. Get values of a single row by index or predicate. Returns tuple or dict (named=True). Series.get_chunks() -> list[Series]: Get chunks of Series as list of Series. Series.dt.day() -> Series: Extract day from Date/Datetime column (1-31). Expr.cos() -> Expr. Compute element-wise cosine. Returns Float64 Expr. Series.list.slice(offset:int|Expr, length:int|Expr|None=None) -> Series: Slice sublists. Negative indexing supported. Series.str.reverse() -> Series. Returns string values in reversed order. polars.Expr.list.slice(offset: int|str|Expr, length: int|str|Expr|None=None) -> Expr: Slice sublists. offset: Start index. length: Slice length. None=to end. polars.nth(*indices: int | Sequence[int]) -> Expr: Get nth column(s) of context. Expr.struct.json_encode() -> Expr: Convert struct to string column with JSON values. Example: pl.DataFrame({"a": [{"a": [1, 2], "b": [45]}]}).with_columns(pl.col("a").struct.json_encode().alias("encoded")) Expr.bin.reinterpret(*, dtype: PolarsDataType, endianness: Endianness = 'little') -> Expr: Interpret a buffer as a numerical Polars type. Params: dtype: PolarsDataType, endianness: "big", "little". Returns: Expr of data type dtype. If binary array is too short value will be null. If binary array is too long, remainder will be ignored. Expr.var(ddof: int = 1) -> Expr Get variance. polars.var(column: str, ddof: int = 1) -> Expr Get variance. column: column name; ddof: delta degrees of freedom. LazyGroupBy.len(name: str | None = None) -> LazyFrame: Return number of rows in each group. Parameters: name - Assign name to resulting column (defaults to "len"). Example: lf.group_by("a").len().collect() Expr.search_sorted(element: IntoExpr | np.ndarray[Any, Any], side: SearchSortedSide = 'any') -> Expr: Find indices to insert elements to maintain order. element: Expression or scalar. side: 'any', 'left', 'right' (first, leftmost, rightmost suitable location). Example: df.select(pl.col("values").search_sorted(3)) DataFrame.iter_rows(*, named: bool = False, buffer_size: int = 512) -> Iterator[tuple[Any, ...]] | Iterator[dict[str, Any]]: Returns iterator over DataFrame rows. named=True returns dicts instead of tuples. buffer_size: modify only in specific cases. Warning: Row iteration is not optimal. See also: rows, rows_by_key. Notes: ns-precision temporal values truncated to microseconds. Series.arg_max() -> int | None: Get index of maximal value. Series.to_numpy(*, writable: bool = False, allow_copy: bool = True, use_pyarrow: bool | None = None, zero_copy_only: bool | None = None) -> ndarray. Converts Series to NumPy ndarray. Parameters: writable (ensure writable array, forces copy), allow_copy (allow memory copy, fails if False). Zero copy if no nulls, numeric type, single chunk, writable=False. Integer Series with nulls cast to float NaN. Example: s.to_numpy(allow_copy=False). Use .to_arrow().to_numpy() for PyArrow conversion. LazyGroupBy.median() -> LazyFrame: Computes the median for each group. Expr.name.to_lowercase() -> Expr: Make the root column name lowercase. See also: prefix, suffix, to_uppercase. Note: Undoes previous renaming. Must be last in chain. Only one name operation allowed. Example: df.with_columns(pl.all().name.to_lowercase()) Expr.name.prefix(prefix: str) -> Expr: Add a prefix to the root column name. Parameters: prefix. See also: suffix. Can only be called as the last expression in a chain. Expr.dt.date() -> Expr: Extracts date from date(time). Applies to Date and Datetime columns. Returns: Date Expr. Expr.set_sorted(*, descending: bool = False) -> Expr: Flags the expression as 'sorted'. descending: Whether the Series order is descending. Warning: Can lead to incorrect results if data is NOT sorted. Example: df.select(pl.col("values").set_sorted().max()) Expr.std(ddof: int = 1) -> Expr: Get standard deviation. ddof: Delta Degrees of Freedom (N - ddof). Default ddof is 1. Example: df.select(pl.col("a").std()) Series.is_finite() -> Series: Returns boolean Series indicating finite values. Example: s = pl.Series("a", [1.0, 2.0, np.inf]); s.is_finite() Expr.sign() -> Expr: Compute element-wise sign function on numeric types. Returns -1 if x < 0, 1 if x > 0, x otherwise (typically 0 or NaN). Null values preserved. Example: df.select(pl.col.a.sign()) Expr.arctan() -> Expr: Compute the element-wise inverse tangent. Returns Expr of Float64. polars.Expr.str.strip_chars_end(characters: IntoExpr = None) -> Expr: Remove trailing characters. characters: set of chars or None(whitespace). See also strip_suffix, strip_chars_start. Expr.ge(other: Any) -> Expr: Returns expression expr >= other. other: Literal or expression to compare. Example: pl.col("x").ge(pl.col("y")) polars.arg_where(condition: Expr | Series, *, eager: bool = False) -> Expr | Series. Return indices where condition is True. eager=True returns Series. Expr.arr.max() -> Expr. Compute the max values of sub-arrays. Example: df.select(pl.col("a").arr.max()). DataFrame.get_columns() -> list[Series]: Get DataFrame as List of Series. Series.bin.starts_with(prefix: IntoExpr) -> Series: Check if values start with binary substring. Parameter: prefix: Prefix substring. DataFrame.to_init_repr(n=1000) -> str: Convert DataFrame to instantiable string representation. Params: n (rows to use). Series.min() -> PythonLiteral | None: Get the minimal value in this Series. Series.sort(descending: bool=False, nulls_last: bool=False, multithreaded: bool=True, in_place: bool=False) -> Self. Sort Series. Params: descending, nulls_last, multithreaded, in_place. polars.Series.head(n: int = 10) -> Series: Get first n elements. n: number of elements (positive or negative). Expr.meta.undo_aliases() -> Expr: Undoes renaming (alias, name.keep). Expr.hash(seed: int = 0, seed_1: int | None = None, seed_2: int | None = None, seed_3: int | None = None) -> Expr: Hash elements. Hash value is UInt64. seed: Random seed (defaults to 0). seed_1, seed_2, seed_3: Random seed parameters (defaults to seed). Note: Unstable across Polars versions. polars.api.register_series_namespace(name:str) -> Callable[[type[NS]], type[NS]]. Decorator for registering custom Series functionality. Parameters: name:Namespace name. Returns: Callable. Expr.log(base: float = 2.718) -> Expr. Compute logarithm to given base (default e). Series.arr.arg_max() -> Series: Retrieve index of maximum value in every sub-array. Returns UInt32/UInt64 Series. Example: s = pl.Series("a", [[0, 9, 3], [9, 1, 2]], dtype=pl.Array(pl.Int64, 3)); s.arr.arg_max() Series.str.extract_many(patterns: Series | list[str], *, ascii_case_insensitive: bool = False, overlapping: bool = False) -> Series: Extract many matches using the Aho-Corasick algorithm. Parameters: patterns, ascii_case_insensitive, overlapping. Only supports matching on string literals. Example: s.str.extract_many(patterns=[...], overlapping=True) Expr.dt.month() -> Expr: Extract month (1-12) from Date/Datetime column. Returns Int8 Expr. Expr.dt.total_microseconds() -> Expr[Int64]. Extract total microseconds from Duration type. Series.str.json_path_match(json_path: IntoExprColumn) -> Series: Extracts the first match of JSON string with provided JSONPath expression. Returns: Series of type String. Null values if the original value is null or the json_path returns nothing. Expr.floordiv(other: Any) -> Expr: Integer division (//) of expression with other (numeric literal or Expr). See also: truediv. Example: df.with_columns(pl.col("x").floordiv(2).alias("x//2")). Note: Differs subtly from Python's floor division with floating-point numbers. Expr.arr.var(ddof: int = 1) -> Expr: Compute variance of sub-arrays. Example: df.select(pl.col("a").arr.var()) DataFrame.iter_columns() -> Iterator[Series]. Returns iterator over columns of DataFrame. Expr.gt(other: Any) -> Expr: Evaluates if expr > other. Example: df.with_columns(pl.col("x").gt(pl.col("y")).alias("x > y")). Expr.arg_unique() -> Expr: Get index of first unique value. Returns: Expr. Series.to_arrow(compat_level:CompatLevel=None) -> Array. Get underlying Arrow array. Zero copy single chunk. Series.list.set_symmetric_difference(other: Series) -> Series: Compute set symmetric difference between elements in list and other. other: Right hand side Series. Example: a.list.set_symmetric_difference(b) Series.str.to_uppercase() -> Series. Convert strings to uppercase. Example: s = pl.Series("foo", ["cat", "dog"]); s.str.to_uppercase() # ["CAT", "DOG"] Expr.str.pad_end(length: int, fill_char: str=' ') -> Expr. Pads string end to length. Params: length, fill_char. Returns: Expr. Series.list.drop_nulls()->Series:Drop nulls from list. Preserves order. polars.Series.list.any() -> Series Evaluate whether any boolean value in a list is true. Returns: Series (Boolean). Series.list.var(ddof: int = 1) -> Series: Compute variance of arrays in the list. ddof: degrees of freedom. Example: s.list.var() Series.sinh() -> Series: Computes the element-wise hyperbolic sine. LazyFrame.sink_csv(path: str | Path, *, include_bom: bool = False, include_header: bool = True, separator: str = ',', line_terminator: str = '\n', quote_char: str = '"', batch_size: int = 1024, datetime_format: str | None = None, date_format: str | None = None, time_format: str | None = None, float_scientific: bool | None = None, float_precision: int | None = None, null_value: str | None = None, quote_style: CsvQuoteStyle | None = None, maintain_order: bool = True, type_coercion: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, collapse_joins: bool = True, no_optimization: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> None: Evaluate query in streaming mode and write to a CSV file. Parameters: path, include_bom, include_header, separator, line_terminator, quote_char, batch_size, datetime_format, date_format, time_format, float_scientific, float_precision, null_value, quote_style, maintain_order, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, collapse_joins, no_optimization, storage_options, credential_provider, retries. DataFrame.describe(percentiles: Sequence[float] | float | None = (0.25, 0.5, 0.75), *, interpolation: RollingInterpolationMethod = 'nearest') -> DataFrame: Returns summary statistics for a DataFrame. percentiles: Percentiles to include. interpolation: Interpolation method. Warning: Output is not guaranteed to be stable. polars.DataFrame.median() -> DataFrame Aggregate the columns of this DataFrame to their median value. Expr.dt.round(every: str | dt.timedelta | IntoExprColumn) -> Expr. Divide date/datetime range into buckets. Rounds values to bucket start/end. Returns Expr of Date or Datetime. DataFrame.clear(n: int) -> DataFrame Create empty (n=0) or n-row null-filled (n>0) DataFrame copy. Series.log(base: float) -> Series Compute logarithm to base. Series.dt.nanosecond() -> Series: Extracts nanoseconds from DateTime representation. Returns: Series of Int32. Example: date.dt.nanosecond() DataFrame.unnest(columns: ColumnNameOrSelector | Collection[ColumnNameOrSelector], *more_columns: ColumnNameOrSelector) -> DataFrame: Decompose struct columns into separate columns. columns: Name of struct column(s) to unnest. *more_columns: Additional columns. Example: df.unnest("t_struct") Series.arr.n_unique() -> Series: Count unique values in every sub-arrays. Series.dt.base_utc_offset() -> Series: Base offset from UTC as Duration. See also: Series.dt.dst_offset Expr.list.explode() -> Expr: Returns a column with a separate row for every list element. Returns: Expr with the data type of the list elements. See also: Expr.reshape. Example: df.select(pl.col("a").list.explode()) Expr.is_nan() -> Expr. Returns boolean Series indicating which values are NaN. Returns Expr. polars.testing.assert_series_not_equal(left: Series, right: Series, *, check_dtypes: bool = True, check_names: bool = True, check_order: bool = True, check_exact: bool = False, rtol: float = 1e-05, atol: float = 1e-08, categorical_as_str: bool = False) -> None. Assert Series not equal for unit tests. Checks dtypes, names, order, exactness, tolerances, categorical_as_str. polars.Series.arr.all() -> Series: Checks if all boolean values are true in each subarray. Returns Series of booleans. Expr.or_(*others: Any) -> Expr: Method equivalent of bitwise "or" operator (expr | other | ....). others: one or more expressions. Example: (pl.col('x') == pl.col('y')).or_(pl.col('y') == pl.col('z')) Series.first() -> PythonLiteral|None. Get first Series element. None if empty. DataFrame.hstack(columns: list[Series] | DataFrame, *, in_place: bool = False) -> DataFrame: Add Series to DataFrame horizontally. Args: columns (Series list), in_place (bool). Example: df.hstack([x]) LazyFrame.sink_ndjson(path: str | Path, *, maintain_order: bool = True, type_coercion: bool = True, _type_check: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, collapse_joins: bool = True, no_optimization: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> None: Evaluate the query in streaming mode and write to an NDJSON file. Parameters: path (file path), maintain_order, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, collapse_joins, no_optimization, storage_options (cloud options), credential_provider (cloud credentials), retries (cloud retries). Streaming mode is unstable. lf = pl.scan_csv("/path/to/my_larger_than_ram_file.csv"); lf.sink_ndjson("out.ndjson") Config.set_tbl_width_chars(width: int | None) -> type[Config]: Set max table width in characters. polars.testing.parametric.dataframes(cols: int | column | Sequence[column] | None = None, lazy: bool = False, min_cols: int = 1, max_cols: int = 5, min_size: int = 0, max_size: int = 5, include_cols: Sequence[column] | column | None = None, allow_null: bool | Mapping[str, bool] = True, allow_chunks: bool = True, allow_masked_out: bool = True, allowed_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None, excluded_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None, allow_time_zones: bool = True, **kwargs: Any) -> SearchStrategy[DataFrame | LazyFrame]: Hypothesis strategy for generating Polars DataFrames or LazyFrames. UNSTABLE. Expr.list.set_union(other: IntoExpr) -> Expr: Compute SET UNION of list elements. LazyFrame.min() -> LazyFrame: Aggregate columns to their minimum value. Series.cum_min(reverse:bool=False) -> Series. Cumulative min at every element. polars.testing.parametric.column(name: str | None = None, dtype: PolarsDataType | None = None, strategy: SearchStrategy[Any] | None = None, allow_null: bool | None = None, unique: bool = False, null_probability: float | None = None) Define a column for hypothesis dataframes strategy. Parameters: name (column name), dtype, strategy (override default strategy), allow_null, unique (all values unique), null_probability (0.0 => 1.0 chance of None, deprecated, use allow_null). __init__(name, dtype, strategy, allow_null, unique, null_probability): Initializes a column definition. Attributes: allow_null, dtype, name, null_probability, strategy, unique. polars.Expr.cot() -> Expr: Compute cotangent. Returns Expr[Float64]. Series.dt.replace(*, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, ambiguous='raise') -> Series: Replace datetime components (year, month, day, hour, minute, second, microsecond). Params: year, month, day, hour, minute, second, microsecond, ambiguous ('raise', 'earliest', 'latest', 'null'). Expr.dt.millennium() Extract millennium from Date/Datetime columns. Returns millennium number. Returns Expr[Int32]. Series.list.mean(): Compute mean of list arrays. Returns Series. polars.Expr.arr.unique(*, maintain_order: bool = False) -> Expr: Get unique array values. Parameters: maintain_order (bool). Expr.arr.first() -> Expr: Get first value of sub-arrays. polars.all_horizontal(*exprs: IntoExpr | Iterable[IntoExpr]) -> Expr Compute bitwise AND horizontally across columns. Parameters: *exprs: Column expressions. Note: Kleene logic for nulls. Series.unique_counts() -> Series. Returns counts of unique values in order of appearance. LazyFrame.clear(n: int = 0) -> LazyFrame: Returns a copy of the LazyFrame with the same schema but with zero to 'n' empty rows. Expr.list.arg_max() -> Expr: Index of max value in every sublist. Returns: UInt32 or UInt64 Expr. polars.Expr.dt.quarter() -> Expr Extract quarter from underlying Date/Datetime representation. Returns: Expression of data type Int8 (1 to 4). Series.arr.reverse() -> Series: Reverse the arrays in the column. Expr.name.map_fields(function: Callable[[str], str]) -> Expr: Renames struct fields using a mapping function. Series.str.json_decode(dtype: PolarsDataType | None = None, *, infer_schema_length: int | None = 100) Returns Series. Parse string as JSON. Parameters: dtype: cast dtype, infer_schema_length: rows for inference. Error on invalid JSON. See also json_path_match. Expr.str.contains(pattern: str | Expr, *, literal: bool = False, strict: bool = True) -> Expr. Check if string contains substring matching pattern. Parameters: pattern (regex), literal (treat pattern as literal), strict (raise error on invalid regex). See also: starts_with, ends_with, find. Use inline (?iLmsuxU) regex flags. Example: pl.col("s").str.contains("(?i)AA"). Expr.str.explode() -> Expr[String]. Returns column with separate row for each character. Deprecated: Use .str.split('').explode(). Empty strings to null. DataFrame.min_horizontal() -> Series. Get minimum value horizontally across columns. Returns Series named "min". polars.testing.assert_frame_not_equal(left: DataFrame | LazyFrame, right: DataFrame | LazyFrame, *, check_row_order: bool = True, check_column_order: bool = True, check_dtypes: bool = True, check_exact: bool = False, rtol: float = 1e-05, atol: float = 1e-08, categorical_as_str: bool = False) -> None: Asserts that two frames are not equal. Config.set_float_precision(precision: int | None = None) -> type[Config]: Sets float precision for display. None reverts to default. Affects Float32/64 only. polars.scan_ndjson(source: str | Path | IO[str] | IO[bytes] | bytes | list[str] | list[Path] | list[IO[str]] | list[IO[bytes]], *, schema: SchemaDefinition | None = None, schema_overrides: SchemaDefinition | None = None, infer_schema_length: int | None = 100, batch_size: int | None = 1024, n_rows: int | None = None, low_memory: bool = False, rechunk: bool = False, row_index_name: str | None = None, row_index_offset: int = 0, ignore_errors: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, file_cache_ttl: int | None = None, include_file_paths: str | None = None) -> LazyFrame: Lazily read newline delimited JSON files. Parameters: source (path to file), schema (dict of {name:type}), schema_overrides (dict, override dtypes), infer_schema_length (rows to scan for schema), batch_size (rows per batch), n_rows (stop after reading), low_memory, rechunk, row_index_name, row_index_offset, ignore_errors (return Null on schema mismatch), storage_options (cloud provider options), credential_provider (callable for cloud credentials), retries (cloud retries), file_cache_ttl (cloud file cache ttl), include_file_paths (filename column). Allows query optimizer pushdown. polars.arctan2d(y: str | Expr, x: str | Expr) -> Expr: Computes two-argument arctan in degrees. Deprecated: Use arctan2 followed by Expr.degrees(). Returns angle between positive x-axis and ray from origin to (x,y). Expr.deserialize(source: str | Path | IOBase | bytes, *, format: SerializationFormat = 'binary') -> Expr. Read serialized expression from file. Format: 'binary', 'json'. Warning: Pickle for Python UDFs, security risk. Not stable across Polars versions. Expr.sqrt() -> Expr: Computes the square root of elements. Expr.cat.len_bytes() -> Expr: Return the byte-length of string representation. Returns: Expr of UInt32. See Also: len_chars. Note: len_bytes() is O(1), len_chars() is O(n). Example: df.with_columns(pl.col("a").cat.len_bytes().alias("n_bytes")). Expr.arr.n_unique() -> Expr. Count unique values in every sub-array. LazyGroupBy.count() -> LazyFrame:Deprecated. Use LazyGroupBy.len(). Counts rows in each group, including nulls. Config.set_fmt_float(fmt: FloatFmt | None = 'mixed') -> type[Config]: Controls floating point value display. fmt: {'mixed', 'full'}. 'mixed' limits decimal places and uses scientific notation; 'full' prints full precision. polars.read_parquet(source: FileSource, *, columns: list[int] | list[str] | None = None, n_rows: int | None = None, row_index_name: str | None = None, row_index_offset: int = 0, parallel: ParallelStrategy = 'auto', use_statistics: bool = True, hive_partitioning: bool | None = None, glob: bool = True, schema: SchemaDict | None = None, hive_schema: SchemaDict | None = None, try_parse_hive_dates: bool = True, rechunk: bool = False, low_memory: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, use_pyarrow: bool = False, pyarrow_options: dict[str, Any] | None = None, memory_map: bool = True, include_file_paths: str | None = None, allow_missing_columns: bool = False) -> DataFrame: Read DataFrame from parquet file. Parameters: source: file path, columns, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, storage_options, credential_provider, retries, use_pyarrow, pyarrow_options, memory_map, include_file_paths, allow_missing_columns. Series.arcsinh() -> Series: Compute element-wise inverse hyperbolic sine. Expr.meta.is_column_selection(allow_aliasing: bool = False) -> bool: Check if expression selects columns (optionally with aliasing). Expr.dt.datetime() -> Expr: Returns datetime. Deprecated: Use dt.replace_time_zone(None). Applies to Datetime columns. Returns: Datetime Expr. LazyGroupBy.min() -> LazyFrame: Reduce groups to the minimal value. Expr.str.len_bytes() -> Expr Returns string length in bytes (UInt32). Faster than len_chars(). df = pl.DataFrame({'a': ["CafÃ©", "345"]}); df.with_columns(pl.col("a").str.len_bytes().alias("n_bytes")) Expr.arr.min() -> Expr: Compute the min values of the sub-arrays. Example: pl.DataFrame({"a": [[1, 2], [4, 3]]}).select(pl.col("a").arr.min()) DataFrame.var(ddof: int = 1) -> DataFrame: Aggregate columns to variance. Parameters: ddof: Delta Degrees of Freedom. DataFrame.mean() -> DataFrame Aggregate the columns of this DataFrame to their mean value. Expr.arr.count_matches(element: IntoExpr) -> Expr: Count occurrences of element's value in array. element: Expression producing a single value. Returns Expr. Expr.slice(offset: int | Expr, length: int | Expr | None = None) -> Expr: Gets slice of expression. offset: Start index, length: Length of slice. Example: df.select(pl.all().slice(1, 2)) Series.shrink_dtype() -> Series: Shrinks numeric columns to minimal datatype to reduce memory pressure. s = pl.Series("a", [1,2,3,4,5,6]); s.shrink_dtype() polars.format(f_string: str, *args: Expr|str) -> Expr: Format expressions as a string. f_string: string with placeholders. args: expressions filling placeholders. Series.clear(n: int = 0) -> Series: Creates an empty copy of the Series. polars.n_unique(*columns: str) -> Expr: Count unique values in columns. Sugar for pl.col(columns).n_unique(). Example: df.select(pl.n_unique("a")) Expr.meta.pop() -> list[Expr]: Pop the latest expression. Returns list of Expr (usually unit length). Multiple inputs exist in fold expressions. Example: e = pl.col("foo").alias("bar"); first = e.meta.pop()[0] LazyFrame.unnest(columns: ColumnNameOrSelector | Collection[ColumnNameOrSelector], *more_columns: ColumnNameOrSelector) -> LazyFrame. Decompose struct columns into separate columns for each field. Returns LazyFrame. Series.cos() -> Series: Computes element-wise cosine. Series.upper_bound() -> Self. Returns the upper bound of Series dtype as a unit Series. GroupBy.all() -> DataFrame: Aggregate the groups into Series. Example: df = pl.DataFrame({"a": ["one", "two", "one", "two"], "b": [1, 2, 3, 4]}); df.group_by("a", maintain_order=True).all() # returns lists [1,3] and [2,4] for groups "one" and "two" Expr.meta.output_name(raise_if_undetermined:bool=True)->str|None: Get output column name. Raises ComputeError if undetermined and raise_if_undetermined is True. Series.str.strip_prefix(prefix: IntoExpr) -> Series: Remove prefix once from string if found. Example: s.str.strip_prefix("foo") Expr.is_finite() -> Expr: Returns boolean Series indicating finite values. DataFrame.lazy() -> LazyFrame. Start a lazy query. Operations are not executed until collect(), explain(), show_graph(), or collect_schema(). Enables query optimization and parallelism. Series.list.arg_max() -> Series: Returns the index of the maximum value in every sublist. Returns: Series of UInt32 or UInt64. Example: s.list.arg_max() polars.lit(value: Any, dtype: PolarsDataType | None = None, *, allow_object: bool = False) -> Expr. Return literal value expression. Params: value, dtype, allow_object. Returns: Expr. Expr.max() -> Expr: Get maximum value. Example: df = pl.DataFrame({"a": [-1.0, float("nan"), 1.0]}); df.select(pl.col("a").max()) Expr.meta.serialize(file: IOBase | str | Path | None = None, *, format: SerializationFormat = 'binary') -> bytes | str | None: Serialize expression to file/string in JSON. file: File path (None returns string). format: 'binary', 'json'. Returns: bytes or str. Not stable across Polars versions. See also: Expr.deserialize. Example: expr.meta.serialize() Expr.round_sig_figs(digits: int) -> Expr: Round to significant figures. digits: Number of significant figures. Expr.arccosh() -> Expr: Compute the element-wise inverse hyperbolic cosine. SQLContext.tables() -> list[str]. Return list of registered table names. Equivalent to SHOW TABLES SQL. Series.__getitem__(key: SingleIndexSelector | MultiIndexSelector) -> Any | Series: Get part of Series as new Series or scalar. Parameters: key (row(s) to select). Returns: Series or scalar. Example: s[0:2] Series.round(decimals: int = 0) -> Series. Round floating point data. decimals: number of decimals. Example: s = pl.Series("a", [1.12345, 2.56789, 3.901234]); s.round(2) # [1.12, 2.57, 3.9] Expr.dt.day() -> Expr[Int8]. Extract day from Date or Datetime. Returns day of month (1-31). Series.arg_true() -> Series. Get index values where Boolean Series evaluate True. Returns: Series (UInt32). Example: (s == 2).arg_true() polars.Series.arr.arg_min() -> Series Retrieve the index of the minimal value in every sub-array. Returns: Series (UInt32 or UInt64). Series.rolling_quantile(quantile: float, interpolation: RollingInterpolationMethod = 'nearest', window_size: int = 2, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series Compute rolling quantile. Series.median() -> PythonLiteral | None. Get median of Series. Series.str.pad_end(length: int, fill_char: str = ' ') -> Series. Pad end of string to length. Parameters: length: target length, fill_char: padding char. Expr.dt.total_milliseconds() -> Expr: Extracts total milliseconds from a Duration. LazyFrame.last() -> LazyFrame. Get last row of DataFrame. Series.any(*, ignore_nulls: bool = True) -> bool | None. Return if any value is True. Boolean dtype. ignore_nulls=False for Kleene logic. Series.le(other: Any) -> Series | Expr: Series less than or equal to other. Series.list.last() -> Series Get the last value of the sublists. DataFrame.std(ddof: int = 1) -> DataFrame: Aggregate columns to standard deviation. Params: ddof. Returns: DataFrame. Series.describe(percentiles: float | Sequence[float] = (0.25, 0.5, 0.75), interpolation: str = 'nearest') -> DataFrame. Summary statistics of Series. Percentiles [0, 1]. Interpolation: 'nearest', 'higher', 'lower', 'midpoint', 'linear'. Returns DataFrame. polars.read_avro(source: str | Path | IO[bytes] | bytes, columns: list[int] | list[str] | None = None, n_rows: int | None = None) -> DataFrame Read Apache Avro data. source: file path or object; columns: columns to select; n_rows: max rows to read. Series.is_last_distinct() -> Series Boolean mask for last occurrence of distinct values. Returns: Series[Boolean]. Example: Series([1, 1, 2]).is_last_distinct() -> Series([false, true, true]) Expr.median() -> Expr: Get median value using linear interpolation. Example: df.select(pl.col("a").median()) Series.count() -> int: Counts non-null elements. polars.scan_pyarrow_dataset(source: pa.dataset.Dataset, allow_pyarrow_filter: bool = True, batch_size: int | None = None) -> LazyFrame: Scan pyarrow dataset. Unstable API. Use scan_parquet if possible. LazyGroupBy.mean() -> LazyFrame: Computes the mean for each group. Expr.is_not_null() -> Expr: Returns a boolean Series indicating which values are not null. Example: df.with_columns(pl.all().is_not_null().name.suffix("_not_null")) Series.set(filter: Series, value: int | float | str | bool | None) -> Series: Set values based on a boolean mask. Consider using `pl.when().then().otherwise()` instead. Expr.filter(*predicates: IntoExprColumn | Iterable[IntoExprColumn], **constraints: Any) -> Expr: Filter expression based on predicate expressions. Discards nulls. Use LazyFrame.filter to filter on DataFrame level. Parameters: predicates: Expression(s) that evaluates to a boolean Series. constraints: Column filters; name = value to filter columns by the supplied value. Example: df.group_by("group_col").agg(lt=pl.col("b").filter(pl.col("b") < 2).sum()) Expr.xor(other: Any) -> Expr. Bitwise exclusive-or operator (expr ^ other). other: Integer or boolean value, or Expr. DataFrame.set_sorted(column: str, *, descending: bool = False) -> DataFrame. Indicate columns are sorted for faster ops. Warning: incorrect if data unsorted. polars.Expr.last() -> Expr: Get last value. Series.list.min() -> Series: Compute the min value of the arrays in the list. Expr.ne_missing(other: Any) -> Expr: Method equivalent of expr != other where None == None. Differs from ne where nulls propagate. Expr.is_unique() -> Expr. Get mask of unique values. Series.dt.dst_offset() Returns Series[Duration]. Get daylight saving time offset. See also Series.dt.base_utc_offset. DataFrame.quantile(quantile: float, interpolation: RollingInterpolationMethod = 'nearest') -> DataFrame: Aggregate columns to quantile value. quantile: Quantile between 0.0 and 1.0. interpolation: {'nearest', 'higher', 'lower', 'midpoint', 'linear'}. Series.nan_max() -> int | float | date | datetime | timedelta | str: Get max value, propagate NaN values. s = pl.Series("a", [1, 3, 4]); s.nan_max(); s = pl.Series("a", [1.0, float("nan"), 4.0]); s.nan_max() Series.list.max() -> Series: Compute the max value of arrays in the list. polars.mean(*columns: str) -> Expr: Get the mean value of columns. Syntactic sugar for pl.col(columns).mean(). Series.shrink_to_fit(*, in_place: bool = False): Shrink Series memory usage. Returns Series. Parameters: in_place=False. Series.cum_max(reverse: bool = False) -> Series: Computes cumulative max at every element. reverse: Reverses the operation. Expr.drop_nans() -> Expr: Drop floating point NaN values, preserving original order. Note: NaN is different from null. Use drop_nulls() for null values. Series.interpolate_by(by: IntoExpr) -> Series: Fill null values using interpolation based on another column. Expr.list.eval(expr: Expr, parallel: bool = False) -> Expr: Runs a polars expression against the lists' elements. Parameters: expr - Expression to run, parallel - Run expression parallel. Example: df.with_columns(rank=pl.concat_list("a", "b").list.eval(pl.element().rank())) Expr.name.map(function: Callable[[str], str]) -> Expr: Rename the output of an expression by mapping a function over the root name. Parameters: function (maps root name to new name). See also: keep, prefix, suffix. Notes: Undoes previous renaming. Last expression in chain only. Example: df.with_columns(pl.all().reverse().name.map(lambda c: c.rstrip("_reverse").lower())) Series.peak_max() -> Self: Get boolean mask of local maximum peaks. polars.rolling_cov(a: str | Expr, b: str | Expr, *, window_size: int, min_samples: int | None = None, ddof: int = 1) -> Expr: Computes rolling covariance between two columns/expressions. Series.list.count_matches(element: IntoExpr) -> Series Count element occurrences in list. s = pl.Series("a", [[0], [1]]); s.list.count_matches(1) polars.Series.arr.last() -> Series Get the last value of the sub-arrays. Expr.meta.tree_format(*, return_as_string: bool = False) -> str | None. Format expression as tree, return string if return_as_string=True. Expr.dt.total_nanoseconds() -> Expr[Int64]. Extract total nanoseconds from Duration. Series.list.eval(expr: Expr, *, parallel: bool = False) -> Series: Run polars expr on list elements. Params: expr, parallel (use cautiously). LazyFrame.limit(n: int = 5) -> LazyFrame: Returns the first n rows. Alias for LazyFrame.head(). Expr.has_nulls() -> Expr: Check if expression contains null values. Example: df.select(pl.all().has_nulls()) Series.is_not_nan() -> Series Returns a boolean Series indicating which values are not NaN. Expr.str.strip_chars_start(characters: IntoExpr = None) -> Expr: Remove leading characters. characters: Set of chars to remove. None (default) removes whitespace. See also: strip_prefix, strip_chars_end. Note: strips any chars present in 'characters' from start. Use strip_prefix() to strip a specific "word". Example: df.with_columns(foo_strip_start=pl.col("foo").str.strip_chars_start("wod\\t")) Series.arr.var(ddof: int = 1) -> Series: Compute the variance of the sub-arrays. Expr.arg_min() -> Expr. Get index of minimal value. GroupBy.__iter__() -> Self: Iterate over groups. Each group is (name, data). Name is tuple of distinct group values. polars.count(*columns: str) -> Expr: Count non-null values in columns. No columns returns row count (deprecated, use len()). Parameters: columns: column names. Series.arctan() -> Series: Compute the element-wise inverse tangent. Series.list.unique(*, maintain_order: bool = False) -> Series: Get unique values in the list. Params: maintain_order (bool). Example: s = pl.Series("a", [[1, 1, 2], [2, 3, 3]]); s.list.unique() Series.str.find_many(patterns: Expr, ascii_case_insensitive: bool=False, overlapping: bool=False) -> Series: Finds all matches using Aho-Corasick. Returns List<UInt32> Series. Config.set_thousands_separator(separator: str | bool | None = None) -> type[Config]: Set thousands grouping separator. separator: True for default ("," thousands, "." decimal), custom char, or None to omit. See also: set_decimal_separator. Series.dt.total_milliseconds() -> Series: Extracts total milliseconds from a Duration type. Returns Int64 Series. LazyGroupBy.max() -> LazyFrame. Reduce groups to the maximal value. Expr.arccos() -> Expr: Element-wise inverse cosine. Returns Float64. Series.top_k(k: int = 5) -> Series: Return the k largest elements. Non-null elements preferred. Not guaranteed to be sorted. O(n) time complexity. Parameters: k (number of elements). See also: bottom_k. s = pl.Series("a", [2, 5, 1, 4, 3]); s.top_k(3) Series.arr.first() -> Series: Get first value of sub-arrays. Expr.dt.weekday() -> Expr: Extract the week day from the underlying Date representation. Returns the ISO weekday number where monday = 1 and sunday = 7. Returns: Expr of data type Int8. Expr.dt.century() -> Expr: Extracts the century from Date or Datetime columns. Returns Int32. Expr.mod(other: Any) -> Expr: Modulus operator (expr % other). other: Numeric literal or expression. Example: df.with_columns(pl.col("x").mod(2).alias("x%2")) Expr.dt.week() -> Expr Extract the week from the underlying Date representation. Applies to Date and Datetime columns. Returns Int8. Series.chunk_lengths() -> list[int]. Get length of each chunk. Example: pl.concat([s, s2], rechunk=False).chunk_lengths() returns [3, 3]. polars.Series.rle_id() -> Series Get distinct integer ID for each run of identical values. Starts at 0, increments when value changes. Returns: Series (UInt32). See Also: rle. Notes: Useful to group by value changes. Example: s.rle_id() returns run-length encoding IDs. GroupBy.n_unique() -> DataFrame: Count unique values per group. Expr.is_in(other: Expr | Collection[Any] | Series) -> Expr: Check if elements are in other Series. other: Series or sequence. Expr.str.to_titlecase() -> Expr: Modify strings to titlecase. Series.skew(*, bias: bool = True) -> float | None: Compute sample skewness of data set. Params: bias (bool, default True). Returns None if computation not possible. Example: s = pl.Series([1, 2, 2, 4, 5]); s.skew() Series.cat.starts_with(prefix: str) -> Series: Checks if string values start with prefix. Returns Boolean Series. LazyFrame.median() -> LazyFrame: Aggregate columns to their median value. Example: lf.median().collect() Expr.ewm_mean_by(by: str | IntoExpr, *, half_life: str | timedelta) -> Expr: Compute time-based exponentially weighted moving average. Parameters: by (str|Expr) - Times to calculate average by (DateTime, Date, UInt64, UInt32, Int64, or Int32). half_life (str|timedelta) - Unit over which observation decays to half its value. Returns: Expr (Float32 if input is Float32, else Float64). Example: df.with_columns(result=pl.col("values").ewm_mean_by("times", half_life="4d")) Expr.dt.iso_year() -> Expr. Extract ISO year from Date/Datetime columns. Returns ISO year (Int32), may differ from calendar year. Config.set_tbl_dataframe_shape_below(active=True) -> type[Config]: Set config to print DataFrame shape below tables. Params: active (bool). Series.dt.weekday() -> Series Extract weekday from Date/Datetime column. Returns ISO weekday (monday = 1, sunday = 7). Returns Int8 Series. Series.list.first() -> Series Get the first value of sublists. Expr.lower_bound() -> Expr: Calculate lower bound for dtype. Returns unit Series. Expr.arr.explode() -> Expr: Returns a column with a separate row for every array element. Returns Expr with the data type of the array elements. Example: df.select(pl.col("a").arr.explode()) DataFrame.mean_horizontal(ignore_nulls: bool = True) -> Series: Take mean of all values horizontally. ignore_nulls: if False, any null input yields null output. Returns Series named "mean". Example: df.mean_horizontal() LazyFrame.profile(*, type_coercion: bool = True, _type_check: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, no_optimization: bool = False, slice_pushdown: bool = True, comm_subplan_elim: bool = True, comm_subexpr_elim: bool = True, cluster_with_columns: bool = True, collapse_joins: bool = True, show_plot: bool = False, truncate_nodes: int = 0, figsize: tuple[int, int] = (18, 8), streaming: bool = False, _check_order: bool = True,) -> tuple[DataFrame, DataFrame]: Profile a LazyFrame. Returns: (materialized DataFrame, profiling DataFrame). Parameters: optimization flags, show_plot, truncate_nodes, figsize, streaming. Example: lf.profile() LazyFrame.approx_n_unique() -> LazyFrame: Approximate unique count using HyperLogLog++. Deprecated; use select(pl.all().approx_n_unique()). Series.gather(indices: int | list[int] | Expr | Series | np.ndarray) -> Series: Takes values by index. DataFrame.min() -> DataFrame. Aggregate columns to minimum value. Expr.list.min() -> Expr Compute the min value of the lists in the array. Series.str.to_titlecase() -> Series: Modify strings to titlecase. Capitalizes first letter of each word, lowercase rest. Non-alphanumeric characters define word boundaries. Example: s = pl.Series("quotes", ["'e.t. phone home'", "you talkin' to me?", "to infinity,and BEYOND!"]); s.str.to_titlecase() polars.LazyFrame.sum() -> LazyFrame Aggregate columns to their sum. Example: lf.sum().collect() returns a new LazyFrame with the sum of each column. Expr.exp() -> Expr: Compute the exponential element-wise. Example: pl.col("values").exp() LazyFrame.explain(*, format='plain'|'tree', optimized=True, type_coercion=True, predicate_pushdown=True, projection_pushdown=True, simplify_expression=True, slice_pushdown=True, comm_subplan_elim=True, comm_subexpr_elim=True, cluster_with_columns=True, collapse_joins=True, streaming=False, tree_format=None, _check_order=True) -> str. Returns query plan string representation. Parameters: format, optimized, optimization flags, tree_format(deprecated). polars.Expr.flatten() -> Expr: Flatten list/string column. Alias for Expr.list.explode(). Series.cum_sum(*, reverse: bool = False) -> Series: Get cumulative sum. reverse: reverse operation. Int8, UInt8, Int16, UInt16 cast to Int64 before summing. Expr.skew(bias: bool = True) -> Expr: Computes the sample skewness. polars.testing.parametric.create_list_strategy(inner_dtype, *, select_from=None, size=None, min_size=0, max_size=None, unique=False) -> SearchStrategy[list[Any]] Create strategy for Polars List data. Deprecated: use lists(). lst = create_list_strategy(inner_dtype=pl.Int32); lst.example() DataFrame.get_column_index(name: str) -> int: Find column index by name. Parameters: name: column name. Returns: column index. Raises ColumnNotFoundError. Series.dt.minute() -> Series: Extract minutes from DateTime representation. Applies to Datetime columns. Returns Series of data type Int8. Range: 0 to 59. Series.product() -> int|float. Reduce this Series to the product value. Returns: int or float. Expr.approx_n_unique() -> Expr. Approximate count of unique values using HyperLogLog++ algorithm. Config.set_tbl_column_data_type_inline(active: bool=True) -> type[Config] Set config to display data type inline with column name in tables. Expr.reverse() -> Expr. Reverse the selection. Series.not_() -> Series. Negate a boolean Series. Returns boolean Series. Series.ne(other: Any) -> Series | Expr: Method equivalent of series != other. Expr.cat.get_categories() -> Expr: Get categories from categorical data. Returns string. Expr.degrees() -> Expr Convert from radians to degrees.

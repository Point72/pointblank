---
jupyter: python3
html-table-processing: none
title: "Validation Libraries for Polars (2025 Edition)"
author: Rich Iannone
date: 2025-06-01
freeze: true
---

Data validation is a very important part of any data pipeline. And with Polars gaining popularity as
a superfast DataFrame library, developers need validation tools that work seamlessly with it. In
this survey (conducted halfway through 2025) we'll explore four Python validation libraries that
support Polars DataFrames. Each one is sufficiently different enough from another, and so they each
bring their own strengths and ideal use cases to the table.

The Python libraries we're covering are:

1. Pandera
2. Patito
3. Pointblank
4. Validoopsie

We'll run through a number of examples with each of the libraries, using this Polars DataFrame as
the table to validate:

```{python}
import polars as pl

# Standard dataset for all validation examples
user_data = pl.DataFrame({
    "user_id": [1, 2, 3, 4, 5],
    "age": [25, 30, 22, 45, 95],  # <- includes a very high age
    "email": [
        "user1@example.com", "user2@example.com", "invalid-email",  # <- has an invalid email
        "user4@example.com", "user5@example.com"
    ],
    "score": [85.5, 92.0, 78.3, 88.7, 95.2]
})
```

We'll try to run the same data validation across the surveyed libraries, so we'll check:

- the schema
- that `user_id` values are greater than `0`
- that `age` values are between `18` and `80` (inclusive)
- that `email` strings match to an email-checking regex
- that `score` is between `0` and `100` (again with inclusive bounds)

Now let's begin by looking at Pandera.

## 1. Pandera: Schema-First Validation with Statistical Checks

Pandera is a statistical data validation toolkit designed to provide a flexible and expressive API
for performing data validation on dataframe-like objects. The library centers on schema-first
validation, where you define the expected structure and constraints of your data upfront. You can
enable both runtime validation and static type checking integration. Pandera added Polars support in
version `0.19.0` (early 2024).

### Example

```{python}
import pandera.polars as pa

# Define schema using our standard dataset
schema = pa.DataFrameSchema({
    "user_id": pa.Column(pl.Int64, checks=pa.Check.gt(0)),
    "age": pa.Column(pl.Int64, checks=[pa.Check.ge(18), pa.Check.le(80)]),
    "email": pa.Column(pl.Utf8, checks=pa.Check.str_matches(r"^[^@]+@[^@]+\.[^@]+$")),
    "score": pa.Column(pl.Float64, checks=pa.Check.in_range(0, 100))
})

# Validate the schema
try:
    validated_data = schema.validate(user_data)
    print("Validation successful!")
except pa.errors.SchemaError as e:
    print(f"Validation failed: {e}")
```

This example demonstrates Pandera's declarative approach, where you define what your data should
look like rather than writing imperative validation logic. The schema acts as both documentation and
as a validation contract. Notice how multiple checks can be applied to a single column (here, the
`age` column receives two checks), and the validation either succeeds completely or provides
error information about what failed.

### Comparisons

Both Pandera and Patito use declarative, schema-first approaches, but differ in their design
philosophies:

- Pandera uses a dictionary-like schema structure with Column objects, focusing on statistical
validation capabilities
- Patito uses Pydantic model classes, focusing on integration with existing Pydantic workflows
- Unlike Pointblank's step-by-step validation reporting, both Pandera and Patito validate entire
schemas at once
- Key behavioral difference: Patito reports all validation errors in a single pass, while Pandera
stops at the first failure

The choice between them often comes down to whether you prefer Pandera's statistical focus or
Patito's Pydantic integration.

Unlike Pointblank's step-by-step validation reporting, Pandera validates the entire schema at once.
Compared to Patito's model-based approach, Pandera focuses more on statistical validation
capabilities. Unlike Validoopsie's and Pointblank's functional composition, Pandera uses a more
declarative, schema-centric approach.

## 2. Patito: Pydantic-Style Data Models for DataFrames

Patito brings Pydantic's well-received model-based validation approach to DataFrame validation,
creating a bridge between API data validation and DataFrame processing. The library's primary goal
is to provide a familiar, Pydantic-style interface for defining and validating DataFrame schemas,
making it particularly appealing to developers already using Pydantic in their applications.

Patito launched with Polars support from the beginning (in late 2022). Native Polars integration is
touted as one of its core features, reflecting the growing adoption of Polars in the Python
ecosystem.

```{python}
import patito as pt
from typing import Annotated

class UserModel(pt.Model):
    user_id: int = pt.Field(gt=0)
    age: Annotated[int, pt.Field(ge=18, le=80)]
    email: str = pt.Field(pattern=r"^[^@]+@[^@]+\.[^@]+$")
    score: float = pt.Field(ge=0.0, le=100.0)

# Validate using the model
try:
    UserModel.validate(user_data)
    print("Validation successful!")
except pt.exceptions.DataFrameValidationError as e:
    print(f"Validation failed: {e}")
```

This example showcases Patito's model-centric approach where validation rules are embedded in class
definitions. The use of Python's type hints and Pydantic's Field syntax makes the validation rules
self-documenting. Notably, Patito reports all validation errors at once, providing a fairly
comprehensive view of data quality issues, whereas other libraries (e.g., Pandera) stop at the first
failure.

## 3. Pointblank: Comprehensive Validation with Beautiful Reports

Pointblank is a comprehensive data validation framework designed to make data quality assessment
both thorough and accessible to stakeholders. Originally inspired by the R package of the same name,
Pointblank's primary mission is to provide validation workflows that generate beautiful, interactive
reports that can be shared with both technical and non-technical team members.

Pointblank launched with Polars support as a core feature from its initial Python release in late
2024, built on top of the Narwhals and Ibis compatibility layers to provide consistent DataFrame
operations across multiple backends including Polars, Pandas, and database connections.

```{python}
import pointblank as pb

schema = pb.Schema(
    columns=[("user_id", "Int64"), ("age", "Int64"), ("email", "String"), ("score", "Float64")]
)

validation = (
    pb.Validate(data=user_data, label="An example.", tbl_name="users", thresholds=(0.1, 0.2, 0.3))
    .col_vals_gt(columns="user_id", value=0)
    .col_vals_between(columns="age", left=18, right=80)
    .col_vals_regex(columns="email", pattern=r"^[^@]+@[^@]+\.[^@]+$")
    .col_vals_between(columns="score", left=0, right=100)
    .col_schema_match(schema=schema)
    .interrogate()
)

validation
```

## 4. Validoopsie: Composable Checks

Validoopsie is built around composable validation principles, providing a toolkit for creating
reusable validation functions organized into logical modules. The library's emphasizes the building
of validation logic from modular, testable components that can be combined in flexible ways to
create complex validation workflows. Like Pointblank, Validoopsie had Polars support from the very
first release (early-2025).

```{python}
from validoopsie import Validate
from narwhals.dtypes import Int64, Float64, String

# Composable validation checks using our standard dataset
validation = (
    Validate(user_data)
    .ValuesValidation.ColumnValuesToBeBetween(
        column="user_id",
        min_value=0
    )
    .ValuesValidation.ColumnValuesToBeBetween(
        column="age",
        min_value=18,
        max_value=80
    )
    .StringValidation.PatternMatch(
        column="email",
        pattern=r"^[^@]+@[^@]+\.[^@]+$"
    )
    .ValuesValidation.ColumnValuesToBeBetween(
        column="score",
        min_value=0,
        max_value=100
    )
    .TypeValidation.TypeCheck(
        frame_schema_definition={
            "user_id": Int64,
            "age": Int64,
            "email": String,
            "score": Float64
        }
    )
)

# Check if validation passed
try:
    validation.validate()
    print("Validation successful!")
except Exception as e:
    print(f"Validation failed: {e}")
```

This example showcases Validoopsie's modular approach where validation rules are organized into
namespaced categories (`ValuesValidation`, `StringValidation`, `TypeValidation`). Each validation
type is grouped logically, making it easy to discover and use the right validation for your needs.
The modular design promotes code organization and reusability across different validation scenarios.


---
title: Intro
jupyter: python3
html-table-processing: none
---

To assess the state of data quality for a table, we use the `Validate` class to collect our validation instructions and then perform the interrogation. After interrogation, we have an object that can produce reporting or enable further processing of the input table. We can use different types of tables like Polars and Pandas DataFrames, Parquet files, or a selection of DB tables. Let's walk through what a table validation looks like in pointblank!

## A Simple Example with the Basics

This is a validation table that's checking a Polars DataFrame:

```{python}
# | code-fold: true

import pointblank as pb

validation_1 = (
    pb.Validate(data=pb.load_dataset(dataset="small_table"))
    .col_vals_lt(columns="a", value=10)
    .col_vals_between(columns="d", left=0, right=5000)
    .col_vals_in_set(columns="f", set=["low", "mid", "high"])
    .col_vals_regex(columns="b", pattern=r"^[0-9]-[a-z]{3}-[0-9]{3}$")
    .interrogate()
)

validation_1.get_tabular_report()
```

Each row is a validation step. The left-hand side outlines the validation rules. The right-hand side provides the results of each validation step.

While simple in principle, there's a lot of useful information packed into this validation table! The bright green color strips at the left of each validation step indicates that all test units passed validation. The lighter green color in the second step means that there was at least one failing unit. What are test units? Each validation step could perform one or many atomic tests (e.g., one test per cell in column). It's quite a bit to take in, so here's a diagram that describes the different parts of the validation table:

![](/assets/pointblank-validation-table.png){width=100%}

The code that performs the validation on the Polars table can be revealed by interacting with the `Code` disclosure triangle. Here's a rundown of how it all works in three steps.

#### Step 1

The object that we need for this workflow is created with the `Validate` class. Such an object can handle one target table at any given time and the `data=` argument is where the table is specified.

#### Step 2

The validation process needs directives on how exactly to check the tabular data. To this end we draw upon validation methods to define the validation rules (e.g., `col_vals_gt()`, `col_vals_between()`, etc.). Each invocation translates to discrete validation steps. We can use as many of these as is necessary for testing the table in question---more is usually better.

#### Step 3

We conclude this process with the `interrogate()` method. All of the validation methods defined will not act on the target table until `interrogate()` is used. During the interrogation phase, the validation plan (the collection of validation steps) will be executed. We then get useful results within the Validate object.

That's data validation with pointblank in a nutshell! Of course, we do want reporting on how it went down, so using the `get_tabular_report()` method as the fourth step will almost always be a thing that's done. In the next section we'll go a bit further by introducing a means to gauge data quality with failure thresholds.

## Using Threshold Levels

It can be useful to gauge data quality by setting failure thresholds for validation steps. For example, it may be acceptable at some point in time to tolerate up to 5% of failing test units for a given validation. Or, having several levels of data quality might be useful and instructive, where failing test units across validations are grouped into the 0-5%, 5-10%, and 10%- bands.

We can specify failure threshold levels in the `thresholds=` argument of `Validate`. To do that, we can use the `Thresholds` class and pass in the resulting object to `thresholds=`. In the following, we use fractional failure-rate values (i.e., real numbers between `0` and `1`) to define thresholds for the `WARN` and `STOP` states.

```{python}
thresholds = pb.Thresholds(warn_at=0.1, stop_at=0.2)
```

Let's use this `thresholds` object in a new validation. It's similar to the last one but the parameters for some of the validation functions will result in more failing test units. We'll see that the interrogation messages show mention of `STOP` and `WARNING` state being met.

```{python}
validation_2 = (
    pb.Validate(data=pb.load_dataset(dataset="small_table"), thresholds=thresholds)
    .col_vals_in_set(columns="f", set=["low", "mid"])
    .col_vals_lt(columns="a", value=7)
    .col_vals_regex(columns="b", pattern=r"^[0-9]-[a-w]{3}-[2-9]{3}$")
    .col_vals_between(columns="d", left=0, right=4000)
    .interrogate()
)

validation_2.get_tabular_report()
```

As can be seen, all validation steps show some degree of failing test units. Here's a breakdown on how this can be interpreted:

- steps 1 and 3: failure rate high enough to enter the `WARN` and `STOP` states (more than 20% of test units failed in both steps)
- step 2: failure rate to enter the `WARN` state (more than 10% of test units failed)
- step 4: one failing test units but well below the 10%-failing `WARN` threshold

The availability of data extracts that show where the failures occurred can serve to get at the heart of what caused such failures in the first place. On the flip side, one could modify the rules of the validation steps should the flagged rows in the extracts turn out to be reasonable.

---
title: Intro
jupyter: python3
html-table-processing: none
---

To assess the state of data quality for a table, we use the `Validate` class to collect our validation instructions and then perform the interrogation. After interrogation, we have an object that can produce reporting or enable further processing of the input table. We can use different types of tables like Polars and Pandas DataFrames, Parquet files, or a selection of DB tables. Let's walk through what a table validation looks like in pointblank!

## The Table Validation Workflow

The object that we need for this workflow is created with the `Validate` class. Such an object can handle one target table at any given time and the `data=` argument is where the table is specified. The validation process needs directives on how exactly to check the tabular data, so, we use validation methods (e.g., `col_vals_gt()`, `col_vals_between()`, etc.) and these translate to discrete validation steps. We can use as many of these as necessary for satisfactory validation testing of the table in question.

The conclude this process with `interrogate()` method. All validation methods, when called on a `Validate` object, will not act on the target table until `interrogate()` is used. During the interrogation phase, the validation plan (the collection of validation steps) will be executed.

## A Simple Example with the Basics

For our examples going forward, we'll use the `small_table` dataset. It's included in the pointblank library and can be obtained via the `load_dataset()` function. It isn't very large, which makes it great for simple examples. Here is how you get that dataset as a Polars DataFrame:

```{python}
import pointblank as pb

small_table = pb.load_dataset(dataset="small_table")

small_table.style
```

What follows now is a very simple validation plan. We will test that:

1. the values in column `a` are all less than `10`
2. column `d` has values in the range of `0` to `5000` (this is not entirely true!)
3. column `f` only has the values `"low"`, `"mid"`, and `"high"`
4. The strings in column `b` fit a particular regex pattern (`"^[0-9]-[a-z]{3}-[0-9]{3}$"`)

This is how the validation plan is written and interrogated. When carried out interactively, you'll get status messages that describe how the interrogation is going.

```{python}
validation_1 = (
    pb.Validate(data=small_table)
    .col_vals_lt(columns="a", value=10)
    .col_vals_between(columns="d", left=0, right=5000)
    .col_vals_in_set(columns="f", set=["low", "mid", "high"])
    .col_vals_regex(columns="b", pattern=r"^[0-9]-[a-z]{3}-[0-9]{3}$")
    .interrogate()
)

validation_1.get_tabular_report()
```

Let's have a look at how to interpret this reporting table. The bright green color strips at the left of each validation step indicates that all test units passed validation. The lighter green color in the second step means that there was at least one failing unit.

The `STEP` column provides the name of the validation function used as a basis for a validation step. `COLUMNS` shows us the target column for each validation step. The `VALUES` column lists any values required for a validation step. What is `TBL`? That indicates whether the table was mutated just before interrogation in that validation step (via the `pre=` argument, available in every validation function). The right-facing arrows indicate that the table didn't undergo any transformation, so we are working with the unchanged table in every step. `EVAL` lets us know whether there would be issues in evaluating the table itself and the checkmarks down this column show us that there were no issues during interrogation.

The total number of test units is provided next in the `UNITS` column, then the absolute number and fraction of passing test units (`PASS`) and failing test units (`FAIL`). The `W`, `S`, `N` indicators tell us whether we have entered either of the `WARN`, `STOP`, or `NOTIFY` states for each these validation steps. Because we didn't set any threshold levels for these states (that can be done with the `thresolds=` argument, more on that later), these columns are irrelevant here. Finally, the `EXT` column provides an opportunity to download any data extract rows as a CSV. These rows are those that contain failed test units (i.e., having a cell that didn't pass a particular validation step). For *step 2*, the `col_vals_between()` validation step, there is a data extract available. We can either download the CSV from the report or obtain the extract with the `get_data_extracts()` method:

```{python}
validation_1.get_data_extracts(i=2)
```

Recall that validation Step 2 asserted that all values in column `d` should be between `0` and `5000`, however, this extract of `small_table` shows that column `d` has a value of `10000` which lies outside the specified range.

## Using Threshold Levels

It can be useful to gauge data quality by setting failure thresholds for validation steps. For example, it may be acceptable at some point in time to tolerate up to 5% of failing test units for a given validation. Or, having several levels of data quality might be useful and instructive, where failing test units across validations are grouped into the 0-5%, 5-10%, and 10%- bands.

We can specify failure threshold levels in the `thresholds=` argument of `Validate`. To do that, we can use the `Thresholds` class and pass in the resulting object to `thresholds=`. In the following, we use fractional failure-rate values (i.e., real numbers between `0` and `1`) to define thresholds for the `WARN` and `STOP` states.

```{python}
thresholds = pb.Thresholds(warn_at=0.1, stop_at=0.2)
```

Let's use this `thresholds` object in a new validation. It's similar to the last one but the parameters for some of the validation functions will result in more failing test units. We'll see that the interrogation messages show mention of `STOP` and `WARNING` state being met.

```{python}
validation_2 = (
    pb.Validate(data=small_table, thresholds=thresholds)
    .col_vals_in_set(columns="f", set=["low", "mid"])
    .col_vals_lt(columns="a", value=7)
    .col_vals_regex(columns="b", pattern=r"^[0-9]-[a-w]{3}-[2-9]{3}$")
    .col_vals_between(columns="d", left=0, right=4000)
    .interrogate()
)

validation_2.get_tabular_report()
```

As can be seen, all validation steps show some degree of failing test units. Here's a breakdown on how this can be interpreted:

- steps 1 and 3: failure rate high enough to enter the `WARN` and `STOP` states (more than 20% of test units failed in both steps)
- step 2: failure rate to enter the `WARN` state (more than 10% of test units failed)
- step 4: one failing test units but well below the 10%-failing `WARN` threshold

The availability of data extracts that show where the failures occurred can serve to get at the heart of what caused such failures in the first place. On the flip side, one could modify the rules of the validation steps should the flagged rows in the extracts turn out to be reasonable.

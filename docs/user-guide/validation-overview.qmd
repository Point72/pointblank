---
title: Overview
jupyter: python3
toc-expand: 2
html-table-processing: none
---

```{python}
#| echo: false
#| output: false
import pointblank as pb
pb.config(report_incl_footer=False)
```

Pointblank's core functionality revolves around validation steps, which are individual checks that
verify different aspects of your data. These steps are methods of the `Validate` class that, when
combined, create a comprehensive validation plan for your data.

Here's a validation that incorporates four different validation methods:

```{python}
import pointblank as pb

(
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="polars"),
        label="Four different validation methods."
    )
    .col_vals_gt(columns="a", value=0)
    .rows_distinct()
    .col_exists(columns="date")
    .conjointly(
        lambda df: pl.col("a") > 0,
        lambda df: pl.col("a") + pl.col("c") < pl.col("d"),
    )
    .interrogate()
)
```

This example showcases how you can combine different types of validations in a single validation
plan:

- column value validation with `col_vals_gt()`
- row-based validation with `rows_distinct()`
- table structure validation with `col_exists()`
- advanced validation using `conjointly()` with custom expressions

## Categories of Validation Steps

Pointblank provides over 20 validation methods to handle diverse data quality requirements. These
can be grouped into four main categories based on what aspects of the data they validate.

### Column Value Validations

These methods check individual values within columns against specific criteria. Some of these are
column value comparison checks, which work by comparing values in a column against a fixed value,
or, values in a different column. Here are all the validation methods of this type and comparison
checks they make:

- [`col_vals_gt()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_gt.html) -- `>`
- [`col_vals_lt()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_lt.html) -- `<`
- [`col_vals_ge()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_ge.html) -- `>=`
- [`col_vals_le()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_le.html) -- `<=`
- [`col_vals_eq()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_eq.html) -- `==`
- [`col_vals_ne()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_ne.html) -- `!=`

There are also validation methods that check column values inside or outside of a range:

- [`col_vals_between()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_between.html)
- [`col_vals_outside()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_outside.html)

We can check for values in a set, or, outside of a set:

- [`col_vals_in_set()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_in_set.html)
- [`col_vals_not_in_set()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_not_in_set.html)

Null value checking is performed with either of two validation methods:

- [`col_vals_null()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_null.html)
- [`col_vals_not_null()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_not_null.html)

Regex checks in a column are performed one validation method:

- [`col_vals_regex()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_regex.html)

Finally, we can performed checks of column values with custom expressions:

- [`col_vals_expr()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_expr.html)

This comprehensive suite of column value validations provides powerful tools for enforcing data
quality requirements across various data types and business rules, with most methods requiring
minimal coding effort.

### Row-based Validations

Row-based validations focus on examining properties that span across entire rows rather than
individual columns. These are essential for detecting issues that can't be found by looking at
columns in isolation:

- [`rows_distinct()`](https://posit-dev.github.io/pointblank/reference/Validate.rows_distinct.html):
ensures no duplicate rows exist in the table
- [`rows_complete()`](https://posit-dev.github.io/pointblank/reference/Validate.rows_complete.html):
verifies that no rows contain any missing values

These row-level validations are particularly valuable for ensuring data integrity and completeness
at the record level, which is crucial for many analytical and operational data applications.

### Table Structure Validations

Table structure validations ensure that the overall architecture of your data meets expectations.
These structural checks form a foundation for more detailed data quality assessments:

- [`col_exists()`](https://posit-dev.github.io/pointblank/reference/Validate.col_exists.html):
verifies a column exists in the table
- [`col_schema_match()`](https://posit-dev.github.io/pointblank/reference/Validate.col_schema_match.html):
ensures table matches a defined schema
- [`col_count_match()`](https://posit-dev.github.io/pointblank/reference/Validate.col_count_match.html):
 confirms the table has the expected number of columns
- [`row_count_match()`](https://posit-dev.github.io/pointblank/reference/Validate.row_count_match.html):
verifies the table has the expected number of rows

These structural validations provide essential checks on the fundamental organization of your data
tables, ensuring they have the expected dimensions and components needed for reliable data analysis.

### Advanced Validation Approaches

When standard validation methods aren't sufficient, Pointblank provides advanced approaches that
enable custom and complex validation logic:

- [`conjointly()`](https://posit-dev.github.io/pointblank/reference/Validate.conjointly.html):
combines multiple validations that must all pass together
- [`specially()`](https://posit-dev.github.io/pointblank/reference/Validate.specially.html):
implements specialized validation with customized logic

These advanced validation approaches extend Pointblank's capabilities beyond predefined checks,
allowing you to implement custom business rules and complex interdependent validations unique to
your data requirements.

## Common Features Across Validation Steps

Most validation methods share common parameters that enhance their flexibility and power. These
shared parameters create a consistent interface across all validation steps while allowing you to
customize validation behavior for specific needs. These features are tightly integrated into
validation workflows, helping you implement sophisticated data quality controls with just a few
additional parameters.

### Thresholds

Thresholds let you set acceptable levels of failure before triggering warnings, errors, or critical
notifications for individual validation steps:

```{python}
(
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="polars"),
        label="Using thresholds."
    )

    # Add validation steps with different thresholds ---
    .col_vals_gt(
        columns="a", value=1,
        thresholds=pb.Thresholds(warning=0.1, error=0.2, critical=0.3)
    )

    # Add another step with stricter thresholds ---
    .col_vals_lt(
        columns="c", value=10,
        thresholds=pb.Thresholds(warning=0.05, error=0.1)
    )
    .interrogate()
)
```

### Actions

Actions allow you to define specific responses when validation thresholds are crossed. You can use
simple string messages or custom functions for more complex behavior:

```{python}
# Example 1: Actions with string message ---

(
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="polars"),
        label="Using actions with a string message."
    )
    .col_vals_gt(
        columns="c", value=2,
        thresholds=pb.Thresholds(warning=0.1, error=0.2),

        # Add a print-to-console action for the 'warning' threshold ---
        actions=pb.Actions(
            warning="WARNING: Values below `{value}` detected in column 'c'."
        )
    )
    .interrogate()
)
```

```{python}
# Example 2: Actions with callable function ---

def custom_action():
    from datetime import datetime

    print(f"Data quality issue found ({datetime.now()}).")


(
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="polars"),
        label="Using actions with a callable function."
    )
    .col_vals_gt(
        columns="a",
        value=5,
        thresholds=pb.Thresholds(warning=0.1, error=0.2),

        # Apply the function to the 'error' threshold ---
        actions=pb.Actions(error=custom_action),
    )
    .interrogate()
)
```

With custom action functions, you can implement sophisticated responses like sending notifications
or logging to external systems.

### Column Mapping

Column mapping allows you to apply the same validation logic to multiple columns at once,
dramatically reducing repetitive code and making your validation plans more maintainable:

```{python}
import narwhals.selectors as nws

# Map validations across multiple columns
(
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="polars"),
        label="Applying column mapping in `columns`."
    )

    # Apply validation rules to multiple columns ---
    .col_vals_not_null(
        columns=["a", "b", "c"]
    )

    # Apply to numeric columns only with a Narwhals selector ---
    .col_vals_gt(
        columns=nws.numeric(),
        value=0
    )
    .interrogate()
)
```

This technique is particularly valuable when working with wide datasets containing many
similarly-structured columns or when applying standard quality checks across an entire table. It
also ensures consistency in how validation rules are applied across related data columns.

### Preprocessing

Preprocessing allows you to transform or modify your data before applying validation checks,
enabling you to validate derived or modified data without altering the original dataset:

```{python}
import polars as pl

(
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="polars"),
        label="Preprocessing validation steps via `pre=`."
    )
    .col_vals_gt(
        columns="a", value=5,

        # Apply transformation before validation ---
        pre=lambda df: df.with_columns(
            pl.col("a") * 2  # Double values before checking
        )
    )
    .col_vals_lt(
        columns="c", value=100,

        # Apply more complex transformation ---
        pre=lambda df: df.with_columns(
            pl.col("c").pow(2)  # Square values before checking
        )
    )
    .interrogate()
)
```

Preprocessing is particularly useful when validations need to be performed on transformed data, such
as checking calculated metrics, normalized values, or when complex business rules require
combinations of fields. This feature helps maintain clean validation code while enabling
sophisticated data quality checks on derived values.

### Segmentation

Segmentation allows you to validate data across different groups, enabling you to identify
segment-specific quality issues that might be hidden in aggregate analyses:

```{python}
(
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="polars"),
        label="Segmenting validation steps via `segments=`."
    )
    .col_vals_gt(
        columns="c", value=3,

        # Split into steps by categorical values in column 'f' ---
        segments="f"
    )
    .interrogate()
)
```

Segmentation is powerful for detecting patterns of quality issues that may exist only in specific
data subsets, such as certain time periods, categories, or geographical regions. It helps ensure
that all significant segments of your data meet quality standards, not just the data as a whole.

### Briefs

Briefs allow you to customize the descriptions and messages associated with validation steps, making
validation results more understandable to users and stakeholders. Briefs can be either automatically
generated by setting `brief=True` or defined as custom messages for more specific explanations:


```{python}
(
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="polars"),
        label="Using `brief=` for displaying brief messages."
    )
    .col_vals_gt(
        columns="a", value=0,

        # Use `True` for automatic generation of briefs ---
        brief=True
    )
    .col_exists(
        columns=["date", "date_time"],

        # Add a custom brief for this validation step ---
        brief="Verify required date columns exist for time-series analysis"
    )
    .interrogate()
)
```

Briefs make validation results more meaningful by providing context about why each check matters,
replacing generic technical descriptions with purpose-oriented explanations. They're particularly
valuable in shared reports where stakeholders from various disciplines need to understand validation
results in domain-specific terms rather than technical jargon.

## Getting More Information

Each validation step can be further customized and has additional options. See these pages for more
information:

- [Column Selection Patterns](columns.qmd): Target specific columns for validation
- [Preprocessing](preprocessing.qmd): Transform data before validation
- [Segmentation](segmentation.qmd): Validate data across different groups
- [Thresholds](thresholds.qmd): Set acceptable failure levels
- [Actions](actions.qmd): Define custom responses to validation failures
- [Briefs](briefs.qmd): Customize validation messages

## Conclusion

Validation steps are the building blocks of data validation in Pointblank. By combining steps from
different categories and leveraging common features like thresholds, actions, and preprocessing, you
can create comprehensive data quality checks tailored to your specific needs.

The next sections of this guide will dive deeper into each of these topics, providing detailed
explanations and examples.

---
title: Assertions
jupyter: python3
toc-expand: 2
html-table-processing: none
---

```{python}
#| echo: false
#| output: false
import pointblank as pb
pb.config(report_incl_footer=False)
```

In addition to validation steps that create reports, Pointblank provides **assertions**. This is a
lightweight way to confirm data quality by raising exceptions when validation conditions aren't met.
Assertions are particularly useful in:

- data processing pipelines where you need to halt execution if data doesn't meet expectations
- testing environments where you want to verify data properties programmatically
- scripts and functions where you need immediate notification of data problems

## Basic Assertion Workflow

The assertion workflow uses your familiar validation steps with assertion methods to check that
validations meet your requirements:

```{python}
import pointblank as pb
import polars as pl

# Create sample data
data = pl.DataFrame({
    "id": [1, 2, 3, 4, 5],
    "value": [10.5, 8.3, -2.1, 15.7, 7.2]
})

# Create validation plan and assert all steps pass
(
    pb.Validate(data=data)
    .col_vals_gt(columns="id", value=0, brief="IDs must be positive")
    .col_vals_gt(columns="value", value=-5, brief="Values should exceed -5")

    # Will automatically `.interrogate()` and raise an AssertionError if any validation fails ---
    .assert_passing()
)
```

Pointblank offers two types of assertions:

1. **Full Passing Assertions**: using `assert_passing()` to verify that every single test unit
passes
2. **Threshold-Based Assertions**: using `assert_below_threshold()` to verify that failure rates
stay within acceptable thresholds

## Assertion Methods

### 1. `assert_passing()`: Requires Perfect Validations

The [`assert_passing()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_passing.html)
method is the strictest form of assertion, requiring every single validation test unit to pass:

```{python}
try:
    (
        pb.Validate(data=data)
        .col_vals_gt(columns="value", value=0)
        # Direct assertion: automatically interrogates ---
        .assert_passing()
    )
except AssertionError as e:
    print("AssertionError:", str(e))
```

### 2. `assert_below_threshold()`: Tolerates Some Failures

The
[`assert_below_threshold()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_below_threshold.html)
method is more flexible as it allows some failures as long as they stay below specified threshold
levels. Pointblank uses three severity thresholds that increase in order of seriousness:

- **'warning'** (least severe): the first threshold that gets triggered when failures exceed this
level
- **'error'** (more severe): the middle threshold indicating more serious data quality issues
- **'critical'** (most severe): the highest threshold indicating critical data quality problems

```{python}
# Create a sample DataFrame
tbl = pl.DataFrame({
    "a": [4, 6, 9, 7, 12, 8, 7, 12, 10, 7],
    "b": [9, 8, 10, 5, 10, 9, 14, 6, 6, 8],

})

# Set thresholds: warning=0.2 (20%), error=0.3 (30%), critical=0.4 (40%)
validation = (
    pb.Validate(data=tbl, thresholds=(0.2, 0.3, 0.4))
    .col_vals_gt(columns="b", value=5)   # 1/10 failing (10% failure rate)
    .col_vals_lt(columns="a", value=11)  # 2/10 failing (20% failure rate)
    .col_vals_ge(columns="b", value=8)   # 3/10 failing (30% failure rate)
    .interrogate()
)

validation
```

The validation report above visually indicates threshold levels with colored circles:

- gray circles in the `W` column indicate the 'warning' threshold
- yellow circles in the `E` column indicate the 'error' threshold
- red circles in the `C` column indicate the 'critical' threshold

This won't pass the
[`assert_below_threshold()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_below_threshold.html)
assertion for the 'error' level because step 3 exceeds this threshold (30% failure rate matches the
error threshold):

```{python}
try:
    validation.assert_below_threshold(level="error")
except AssertionError as e:
    print("AssertionError:", str(e))
```

We can check against the 'error' threshold for specific steps with the `i=` parameter:

```{python}
validation.assert_below_threshold(level="error", i=[1, 2])
```

This passes because the highest threshold exceeded in steps 1 and 2 is 'warning'.

The [`assert_below_threshold()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_below_threshold.html)
method takes these parameters:

- `level=`: threshold level to check against (`"warning"`, `"error"`, or `"critical"`)
- `i=`: optional specific step number(s) to check
- `message=`: optional custom error message

This is particularly useful when:

- working with real-world data where some percentage of failures is acceptable
- implementing different severity levels for data quality rules
- gradually improving data quality with stepped thresholds

## Two Ways to Use Assertions

There are two valid approaches to using assertions in Pointblank:

### 1. Direct Assertion

You can chain assertion methods directly after your validation steps:

```{python}
try:
    (
        pb.Validate(data=data)
        .col_vals_gt(columns="id", value=0)
        .col_vals_gt(columns="value", value=-5)

        # Automatically interrogates and asserts in one step ---
        .assert_passing()
    )
except AssertionError as e:
    print("AssertionError:", str(e))
```

With the threshold-based assertion, this becomes:

```{python}
try:
    (
        pb.Validate(data=data, thresholds=(0.05, 0.10, 0.20))
        .col_vals_gt(columns="id", value=0)
        .col_vals_gt(columns="value", value=0)

        # Will fail if >5% fail in any step ---
        .assert_below_threshold(level="warning")
    )
except AssertionError as e:
    print("AssertionError:", str(e))
```

**Advantages:**

- more concise code with fewer lines
- simpler to read and understand
- less error-prone (can't forget to call
[`interrogate()`](https://posit-dev.github.io/pointblank/reference/Validate.interrogate.html))

### 2. Explicit Interrogation + Assertion

Alternatively, you can explicitly interrogate first, then assert:

```{python}
validation = (
    pb.Validate(data=data)
    .col_vals_gt(columns="id", value=0)
    .col_vals_gt(columns="value", value=-5)

    # Explicitly execute the validation ---
    .interrogate()
)

# Then assert all tests passed
try:
    validation.assert_passing()
except AssertionError as e:
    print("AssertionError:", str(e))
```

**Advantages:**

- more explicit control over the interrogation parameters
- allows inspection of results before assertion
- enables multiple assertions or operations on the same validation results

Choose the approach that best fits your workflow and preference for explicitness versus conciseness.

## Using Status Check Methods

In addition to assertion methods that raise exceptions, Pointblank provides status check methods
that return boolean values:

### 1. Using `all_passed()`

The [`all_passed()`](https://posit-dev.github.io/pointblank/reference/Validate.all_passed.html)
method will return `True` only if every single test unit in every validation step passed:

```{python}
validation = (
    pb.Validate(data=data)
    .col_vals_gt(columns="value", value=0)
    .interrogate()
)

if not validation.all_passed():
    print("Validation failed: some values are not positive")
```

### 2. Using Status Methods for Thresholds

The threshold status methods
([`warning()`](https://posit-dev.github.io/pointblank/reference/Validate.warning.html),
[`error()`](https://posit-dev.github.io/pointblank/reference/Validate.error.html), and
[`critical()`](https://posit-dev.github.io/pointblank/reference/Validate.critical.html)) correspond
directly to the three threshold levels used in
[`assert_below_threshold()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_below_threshold.html).
Each method returns information about whether validation steps exceeded that specific threshold
level.

While assertion methods raise exceptions to halt execution when thresholds are exceeded, these
status methods give you fine-grained control to implement custom logic based on different validation
quality levels.

```{python}
validation = (
    pb.Validate(data=data, thresholds=(0.05, 0.10, 0.20))
    .col_vals_gt(columns="value", value=0)  # Some values are negative
    .interrogate()
)

validation
```

The [`warning()`](https://posit-dev.github.io/pointblank/reference/Validate.warning.html) method
returns a dictionary mapping step numbers to boolean values. A `True` value means that step exceeds
the warning threshold:

```{python}
# Get dictionary of warning status for each step
warning_status = validation.warning()
print(f"Warning status: {warning_status}")  # {1: True} means step 1 exceeds warning threshold
```

You can check a specific step using the `i=` parameter, and get a single boolean with `scalar=True`:

```{python}
# Check error threshold for specific step
has_errors = validation.error(i=1, scalar=True)

if has_errors:
    print("Step 1 exceeded the error threshold")
```

Similarly, we can check if any steps exceed the 'critical' threshold:

```{python}
# Check against critical threshold
critical_status = validation.critical()
print(f"Critical status: {critical_status}")
```

These status methods are particularly useful for:

1. **Conditional logic**: taking different actions based on threshold severity
2. **Reporting**: generating summary reports about validation quality
3. **Monitoring**: tracking data quality trends over time
4. **Graceful degradation**: implementing fallback logic when quality decreases

Each method has these options:

- without parameters: returns a dictionary mapping step numbers to boolean status values
- with `i=`: check specific step(s)
- with `scalar=True`: return a single boolean instead of a dictionary (when checking a specific
step)

While assertion methods raise exceptions to halt execution when thresholds are exceeded, these
status methods give you fine-grained control to implement custom logic based on different validation
quality levels.

## Customizing Error Messages

You can provide context to failed assertions by adding descriptive briefs to your validation steps
or custom error messages:

```{python}
validation = (
    pb.Validate(data=data, thresholds=(0.2, 0.3, 0.4))
    .col_vals_gt(
        columns="value", value=0,
        brief="All values must be positive for downstream processing"
    )
    .interrogate()
)

try:
    # Custom message for threshold assertion
    validation.assert_below_threshold(
        level="warning",
        message="Data quality too low for processing!"
    )
except AssertionError as e:
    print(f"Custom handling of failure: {e}")
```

Descriptive error messages are essential in production systems where multiple team members might
need to interpret validation failures. The `brief=` parameter in validation steps creates
auto-generated messages that are both human-readable and contextual, while the `message=` parameter
in assertion methods lets you override with custom messages for specific contexts.

## Combining Assertions with Actions

When working with data pipelines, you can combine assertions with actions to handle different
failure severities:

```{python}
import logging

def log_warning(message):
    logging.warning(message)

def halt_processing(message):
    raise ValueError(f"Pipeline halted: {message}")

# Use actions for automated responses during interrogation
validation = (
    pb.Validate(data=data)
    .col_vals_gt(
        columns="id", value=0,
        thresholds=pb.Thresholds(warning=0.05, error=0.2),
        actions=pb.Actions(
            warning=log_warning,
            error=halt_processing
        )
    )
    .interrogate()  # Actions trigger here based on thresholds
)

# Then use assertions for workflow control
try:
    validation.assert_below_threshold(level="warning")
    # Continue with processing if below warning threshold
except AssertionError:
    # Handle the failure with custom recovery logic
    print("Warning threshold exceeded, using fallback process")
```

Actions and assertions can work together to provide both automated responses during validation and
workflow control after validation.

## Best Practices for Assertions

When using assertions in your data workflows, consider these best practices:

1. **Choose the right assertion type**:
   - use [`assert_passing()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_passing.html)
   for critical validations where any failure is unacceptable
   - use [`assert_below_threshold()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_below_threshold.html)
   for validations where some failure rate is acceptable

2. **Set appropriate thresholds** that match your data quality requirements:
   ```python
   # Example threshold strategy
   validation = pb.Validate(
       data=data,
       # warning at 1%, error at 5%, critical at 10%
       thresholds=pb.Thresholds(warning=0.01, error=0.05, critical=0.10)
   )
   ```

3. **Use a graduated approach** to validation severity:
   ```python
   # Critical validations: must be perfect
   validation1.assert_passing()

   # Important validations: must be below error threshold
   validation2.assert_below_threshold(level="error")

   # Monitor-only validations: check warning status
   warning_status = validation3.warning()
   ```

4. **Placement in pipelines**: place assertions at critical points where data quality is essential

5. **Error handling**: wrap assertions in try-except blocks for better error handling in production
systems

6. **Combine with reporting**: use both assertions and reporting approaches for comprehensive
quality control

## Conclusion

Pointblank's assertion methods give you flexible options for enforcing data quality requirements:

- `assert_passing()` for strict validation where every test unit must pass
- `assert_below_threshold()` for more flexible validation where some failures are tolerable
- Status methods (`warning()`, `error()`, `critical()`) for programmatic threshold checking

By using these assertion methods appropriately, you can build robust data pipelines with different
levels of quality enforcement (from strict validation of critical data properties to more lenient
checks for less critical aspects). This graduated approach to data quality helps create systems that
are both reliable and practical in real-world data environments.

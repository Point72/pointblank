---
title: Assertions
jupyter: python3
toc-expand: 2
html-table-processing: none
---

```{python}
#| echo: false
#| output: false
import pointblank as pb
pb.config(report_incl_footer=False)
```

In addition to validation steps that create reports, Pointblank provides **assertions**. This is a
lightweight way to confirm data quality by raising exceptions when validation conditions aren't met.
Assertions are particularly useful in:

- data processing pipelines where you need to halt execution if data doesn't meet expectations
- testing environments where you want to verify data properties programmatically
- scripts and functions where you need immediate notification of data problems

## Basic Assertion Workflow

The assertion workflow uses your familiar validation steps with assertion methods to check that
validations meet your requirements:

```{python}
import pointblank as pb
import polars as pl

# Create sample data
sample_data = pl.DataFrame({
    "id": [1, 2, 3, 4, 5],
    "value": [10.5, 8.3, -2.1, 15.7, 7.2]
})

# Create a validation plan and assert that all steps pass
(
    pb.Validate(data=sample_data)
    .col_vals_gt(columns="id", value=0, brief="IDs must be positive")
    .col_vals_gt(columns="value", value=-5, brief="Values should exceed -5")

    # Will automatically `interrogate()` and raise an AssertionError if any validation fails ---
    .assert_passing()
)
```

Pointblank offers two types of assertions:

1. **Full Passing Assertions**: using `assert_passing()` to verify that every single test unit
passes
2. **Threshold-Based Assertions**: using `assert_below_threshold()` to verify that failure rates
stay within acceptable thresholds

## Assertion Methods

### `assert_passing()`

The [`assert_passing()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_passing.html)
method is the strictest form of assertion, requiring every single validation test unit to pass:

```{python}
try:
    (
        pb.Validate(data=sample_data)
        .col_vals_gt(columns="value", value=0)

        # Direct assertion: automatically interrogates ---
        .assert_passing()
    )
except AssertionError as e:
    print("AssertionError:", str(e))
```

### `assert_below_threshold()`

The
[`assert_below_threshold()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_below_threshold.html)
method is more flexible as it allows some failures as long as they stay below specified threshold
levels. Pointblank uses three severity thresholds that increase in order of seriousness:

- **'warning'** (least severe): the first threshold that gets triggered when failures exceed this
level
- **'error'** (more severe): the middle threshold indicating more serious data quality issues
- **'critical'** (most severe): the highest threshold indicating critical data quality problems

```{python}
# Create a two-column DataFrame for this example
tbl_pl = pl.DataFrame({
    "a": [4, 6, 9, 7, 12, 8, 7, 12, 10, 7],
    "b": [9, 8, 10, 5, 10, 9, 14, 6, 6, 8],

})

# Set thresholds: warning=0.2 (20%), error=0.3 (30%), critical=0.4 (40%)
validation = (
    pb.Validate(data=tbl_pl, thresholds=(0.2, 0.3, 0.4))
    .col_vals_gt(columns="b", value=5)   # 1/10 failing (10% failure rate)
    .col_vals_lt(columns="a", value=11)  # 2/10 failing (20% failure rate)
    .col_vals_ge(columns="b", value=8)   # 3/10 failing (30% failure rate)
    .interrogate()
)

validation
```

The validation report above visually indicates threshold levels with colored circles:

- gray circles in the `W` column indicate the 'warning' threshold
- yellow circles in the `E` column indicate the 'error' threshold
- red circles in the `C` column indicate the 'critical' threshold

This won't pass the
[`assert_below_threshold()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_below_threshold.html)
assertion for the 'error' level because step 3 exceeds this threshold (30% failure rate matches the
error threshold):

```{python}
try:
    validation.assert_below_threshold(level="error")
except AssertionError as e:
    print("AssertionError:", str(e))
```

We can check against the 'error' threshold for specific steps with the `i=` parameter:

```{python}
validation.assert_below_threshold(level="error", i=[1, 2])
```

This passes because the highest threshold exceeded in steps 1 and 2 is 'warning'.

The [`assert_below_threshold()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_below_threshold.html)
method takes these parameters:

- `level=`: threshold level to check against (`"warning"`, `"error"`, or `"critical"`)
- `i=`: optional specific step number(s) to check
- `message=`: optional custom error message

This is particularly useful when:

- working with real-world data where some percentage of failures is acceptable
- implementing different severity levels for data quality rules
- gradually improving data quality with stepped thresholds

## Two Ways to Use Assertions

There are two valid approaches to using assertions in Pointblank:

### 1. Direct Assertion

You can chain assertion methods directly after your validation steps:

```{python}
try:
    (
        pb.Validate(data=sample_data)
        .col_vals_gt(columns="id", value=0)
        .col_vals_gt(columns="value", value=-5)

        # Automatically interrogates and asserts in one step ---
        .assert_passing()
    )
except AssertionError as e:
    print("AssertionError:", str(e))
```

With the threshold-based assertion, this becomes:

```{python}
try:
    (
        pb.Validate(data=sample_data, thresholds=(0.05, 0.10, 0.20))
        .col_vals_gt(columns="id", value=0)
        .col_vals_gt(columns="value", value=0)

        # Will fail if >5% fail in any step ---
        .assert_below_threshold(level="warning")
    )
except AssertionError as e:
    print("AssertionError:", str(e))
```

**Advantages:**

- more concise code with fewer lines
- simpler to read and understand
- less error-prone (can't forget to call
[`interrogate()`](https://posit-dev.github.io/pointblank/reference/Validate.interrogate.html))

### 2. Explicit Interrogation + Assertion

Alternatively, you can explicitly interrogate first, then assert:

```{python}
validation = (
    pb.Validate(data=sample_data)
    .col_vals_gt(columns="id", value=0)
    .col_vals_gt(columns="value", value=-5)

    # Explicitly execute the validation ---
    .interrogate()
)

# Then assert all tests passed
try:
    validation.assert_passing()
except AssertionError as e:
    print("AssertionError:", str(e))
```

**Advantages:**

- more explicit control over the interrogation parameters
- allows inspection of results before assertion
- enables multiple assertions or operations on the same validation results

Choose the approach that best fits your workflow and preference for explicitness versus conciseness.

## Using Status Check Methods

In addition to assertion methods that raise exceptions, Pointblank provides status check methods
that return boolean values:

### 1. Using `all_passed()`

The [`all_passed()`](https://posit-dev.github.io/pointblank/reference/Validate.all_passed.html)
method will return `True` only if every single test unit in every validation step passed:

```{python}
validation = (
    pb.Validate(data=sample_data)
    .col_vals_gt(columns="value", value=0)
    .interrogate()
)

if not validation.all_passed():
    print("Validation failed: some values are not positive")
```

### 2. Using Status Methods for Thresholds

The threshold status methods
([`warning()`](https://posit-dev.github.io/pointblank/reference/Validate.warning.html),
[`error()`](https://posit-dev.github.io/pointblank/reference/Validate.error.html), and
[`critical()`](https://posit-dev.github.io/pointblank/reference/Validate.critical.html)) correspond
directly to the three threshold levels used in
[`assert_below_threshold()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_below_threshold.html).
Each method returns information about whether validation steps exceeded that specific threshold
level.

While assertion methods raise exceptions to halt execution when thresholds are exceeded, these
status methods give you fine-grained control to implement custom logic based on different validation
quality levels.

```{python}
validation = (
    pb.Validate(data=sample_data, thresholds=(0.05, 0.10, 0.20))
    .col_vals_gt(columns="value", value=0)  # Some values are negative
    .interrogate()
)

validation
```

The [`warning()`](https://posit-dev.github.io/pointblank/reference/Validate.warning.html) method
returns a dictionary mapping step numbers to boolean values. A `True` value means that step exceeds
the warning threshold:

```{python}
# Get dictionary of warning status for each step
warning_status = validation.warning()
print(f"Warning status: {warning_status}")  # {1: True} means step 1 exceeds warning threshold
```

You can check a specific step using the `i=` parameter, and get a single boolean with `scalar=True`:

```{python}
# Check error threshold for specific step
has_errors = validation.error(i=1, scalar=True)

if has_errors:
    print("Step 1 exceeded the error threshold.")
```

Similarly, we can check if any steps exceed the 'critical' threshold:

```{python}
# Check against critical threshold
critical_status = validation.critical()
print(f"Critical status: {critical_status}")
```

These status methods are particularly useful for:

1. **Conditional logic**: taking different actions based on threshold severity
2. **Reporting**: generating summary reports about validation quality
3. **Monitoring**: tracking data quality trends over time
4. **Graceful degradation**: implementing fallback logic when quality decreases

Each method has these options:

- without parameters: returns a dictionary mapping step numbers to boolean status values
- with `i=`: check specific step(s)
- with `scalar=True`: return a single boolean instead of a dictionary (when checking a specific
step)

While assertion methods raise exceptions to halt execution when thresholds are exceeded, these
status methods give you fine-grained control to implement custom logic based on different validation
quality levels.

## Customizing Error Messages

You can provide context to failed assertions using two complementary approaches: validation step
briefs and custom assertion messages.

### Step 1: Use Brief Messages in Validation Steps

The `brief=` parameter in validation steps provides context about what each step is checking. When a
validation fails, this brief becomes part of the error message:

```{python}
# Create a validation with a descriptive brief
validation = (
    pb.Validate(data=sample_data, thresholds=(0.2, 0.3, 0.4))
    .col_vals_gt(
        columns="value", value=0,
        brief="All values must be positive for downstream processing."
    )
    .interrogate()
)

# Display the validation results
validation
```

As you can see from the report, one test unit failed the validation. The brief message is displayed
in the validation report, making it clear what condition wasn't met.

### Step 2: Customize Assertion Error Messages

When you need to customize the error message that appears when an assertion fails, use the
`message=` parameter:

```{python}
try:
    # Custom message for threshold assertion
    validation.assert_below_threshold(
        level="warning",
        message="Data quality too low for processing!"
    )
except AssertionError as e:
    print(f"Custom handling of failure: {e}")
```

Descriptive error messages are essential in production systems where multiple team members might
need to interpret validation failures. The `brief=` parameter in validation steps creates
human-readable context for each validation check, while the `message=` parameter in assertion
methods lets you override the error message with custom text appropriate for the specific workflow
context.

## Combining Assertions with Actions

When working with data pipelines, you can combine assertions with actions to handle different
failure severities:

```{python}
import logging
import sys

# Configure logging to show output
logging.basicConfig(
    level=logging.WARNING,
    format='%(levelname)s: %(message)s',
    stream=sys.stdout
)

# Action functions need to accept a message parameter
def log_warning(message="Data quality issue detected."):
    logging.warning(f"{message}")

def halt_processing(message="Critical data error."):
    logging.error(f"Pipeline halted: {message}")
    # In a real pipeline, this would typically raise an exception
    return False

# Create data with known failures
problem_data = pl.DataFrame({
    "id": [1, 2, 3, -4, 5],  # One negative ID
    "value": [10.5, 8.3, -2.1, 15.7, 7.2]
})

# Use actions for automated responses during interrogation
print("Running validation with actions...")
validation = (
    pb.Validate(data=problem_data, thresholds=(0.1, 0.2, 0.3))
    .col_vals_gt(
        columns="id", value=0,
        brief="IDs must be positive.",
        actions=pb.Actions(
            warning=log_warning  # Only use warning action to avoid errors
        )
    )
    .interrogate()  # Actions will trigger here based on thresholds
)

print("\nValidation report after actions were triggered:")
validation
```

The output above shows:

1. the warning action is triggered when validation runs
2. the warning message includes details about the failed validation
3. the validation continues to completion despite the warning

Now we can use assertions after the fact for workflow control:

```{python}
print("\nNow checking with assert_below_threshold...")
try:
    validation.assert_below_threshold(level="warning")
    print("This line won't execute because the assertion fails.")
except AssertionError as e:
    print(f"AssertionError caught: {e}")
    print("Implementing fallback process...")
```

This example demonstrates how actions and assertions complement each other:

- **Actions** trigger automatically during validation (internal to the validation process)
- **Assertions** check validation results afterward (external to the validation process)

You can use both approaches strategically:

- use actions for immediate responses like logging, alerting, or data quality metrics
- use assertions for workflow decisions like whether to proceed with subsequent data processing

Actions and assertions can work together to provide both automated responses during validation and
workflow control after validation.

## Best Practices for Assertions

When using assertions in your data workflows, consider these best practices:

1. **Choose the right assertion type**:
   - use [`assert_passing()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_passing.html)
   for critical validations where any failure is unacceptable
   - use [`assert_below_threshold()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_below_threshold.html)
   for validations where some failure rate is acceptable

2. **Set appropriate thresholds** that match your data quality requirements:
   ```python
   # Example threshold strategy
   validation = pb.Validate(
       data=sample_data,
       # warning at 1%, error at 5%, critical at 10%
       thresholds=pb.Thresholds(warning=0.01, error=0.05, critical=0.10)
   )
   ```

3. **Use a graduated approach** to validation severity:
   ```python
   # Critical validations: must be perfect
   validation_1.assert_passing()

   # Important validations: must be below error threshold
   validation_2.assert_below_threshold(level="error")

   # Monitor-only validations: check warning status
   warning_status = validation_3.warning()
   ```

4. **Placement in pipelines**: place assertions at critical points where data quality is essential

5. **Error handling**: wrap assertions in try-except blocks for better error handling in production
systems

6. **Combine with reporting**: use both assertions and reporting approaches for comprehensive
quality control

## Conclusion

Pointblank's assertion methods give you flexible options for enforcing data quality requirements:

- [`assert_passing()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_passing.html)
for strict validation where every test unit must pass
- [`assert_below_threshold()`](https://posit-dev.github.io/pointblank/reference/Validate.assert_below_threshold.html)
for more flexible validation where some failures are tolerable
- Status methods ([`warning()`](https://posit-dev.github.io/pointblank/reference/Validate.warning.html),
[`error()`](https://posit-dev.github.io/pointblank/reference/Validate.error.html), and
[`critical()`](https://posit-dev.github.io/pointblank/reference/Validate.critical.html)) for
programmatic threshold checking

By using these assertion methods appropriately, you can build robust data pipelines with different
levels of quality enforcement (from strict validation of critical data properties to more lenient
checks for less critical aspects). This graduated approach to data quality helps create systems that
are both reliable and practical in real-world data environments.
